[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rui-Yang Zhang",
    "section": "",
    "text": "I am currently a first year PhD student of STOR-i CDT at Lancaster University. My PhD project looks at Bayesian experiment design for ocean currents predictions, and is in collaboration with TIDE. My supervisors are Henry Moss and David Leslie at Lancaster, as well as Lachlan Astfalck and Edward Cripps at UWA.\nPreviously, I did BSc Mathematics and Statistical Science at UCL where I was mentored by Sam Livingstone and received a Royal Statistical Society Award.\nI am broadly interested in computational statistics and machine learning methodologies, and their applications in science and engineering. See here for more information.\n\n\n\nAdapted from Adrien’s codes."
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes",
    "section": "",
    "text": "Computational Statistics\n\nBasic Markov chain Monte Carlo Method\nPDMP and MCMC\nGeometric Ergodicities of Langevin and Barker Algorithms\nIntroduction to Sequential Monte Carlo\n\n\n\nMachine Learning\n\nWasserstein Gradient Flow\nIntroduction to Gaussian Processes\nIntroduction to Bayesian Optimisation\nIntroduction to Kernel Stein Discrepancy (with Lanya Yang)\nMarkov Decision Processes and How to Solve Them (code)\n\n\n\nBiostatistics\n\nMultiple Testing Problem\nResponse-Adaptive Randomisation (code1) (code2)\n\n\n\nCourse notes\n\nSpectral Theory\nProbability Theory\n\n\n\nUncategorised\n\nUnivariate Extreme Value Modelling"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "I am broadly interested in computational statistics and machine learning methodologies, and their applications in scientific disciplines. In particular, I am interested in (1) developing methodologies with strong relevance to applications, and (2) advancing our understandings of the underlying mechanisms behind existing, commonly used algorithms. Below are some of the keywords that interest me:\nCurrently, I am investigating how active learning, in combination with physics-informed Gaussian processes surrogate models, can help ocean engineers and oceanographers better understand ocean currents and inform their exploration strategies."
  },
  {
    "objectID": "research.html#publication",
    "href": "research.html#publication",
    "title": "Research",
    "section": "Publication",
    "text": "Publication\nGoogle Scholar\n\nPreprints\n\nLivingstone, S., Nüsken, N., Vasdekis, G., & Zhang, R. Y. (2024). Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift: an unadjusted Barker algorithm. [arxiv] (submitted)\n\n\n\nPublications and Workshop Papers\n\nZhang, R. Y., Moss, H. B., Astfalck, L. C., Cripps, E. J., and Leslie, D. S. (2024). BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories. NeurIPS 2024 Workshop on Bayesian Decision-making and Uncertainty. [OpenReview] [Video]"
  },
  {
    "objectID": "research.html#links",
    "href": "research.html#links",
    "title": "Resources",
    "section": "",
    "text": "BibTeX Tidy"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Lancaster CSML\nLancaster AI Reading Group\nUCL Fundamentals of Statistical Machine Learning\nGP Seminar Series\n\n\n\n\n\nProb_AI Hub\nSTOR-i\nTIDE"
  },
  {
    "objectID": "resources.html#links",
    "href": "resources.html#links",
    "title": "Resources",
    "section": "Links",
    "text": "Links\n\nLaTeX Tools\n\nBibTeX Tidy, tidy up .bib files\nLaTeX Table Generator, make a table as if you are in excel then convert to LaTeX codes\nQuiver, draw commuative diagrams using drags and pulls\n\n\n\nStatistics / ML\n\nMCMC Interactive Gallery, live demo of common MCMC algorithms on interesting targets\nGP Interactive Gallery, interactive visualisation of GP, very neat!\n\n\n\nHobbies\n\nWikiArt, online archive for art work\nXu Bing 徐冰, contemporary artist. Square Word Calligraphy, Book from the Sky, Background Story, Phoenix\nZao Wou-Ki 趙無極, painter. Le Soleil Rouge.\nLucien Freud, painter. portraits, self portraits\nFrancis Bacon, painter. Popes, Three Studies for Figures at the Base of a Crucifixion, self portraits\nEdward Hopper, painter. House by the railroad, office at night\nHaruki Murakami 村上 春樹, novelist. Hear the Wind Sing, Dance Dance Dance, The Wind-Up Bird Chronicle, After Dark, Men Without Women, What I Talk About When I Talk About Running, Novelist as a Vocation\nAng Lee 李安, director. Father Trilogy\nWoody Allen, director. Annie Hall, Match point, Midnight in Paris\nI. M. Pei 貝聿銘, architect. Pyramid, Suzhou Museum, Bank of China Tower\nXiang Biao 项飙, sociologist. Global Body Shopping, Self as Methods\nChen Jia-Ying 陈嘉映, philosopher. 哲学·科学·常识, 何为良好生活, 走出唯一真理观"
  },
  {
    "objectID": "resources.html#academic-groups-organisations",
    "href": "resources.html#academic-groups-organisations",
    "title": "Resources",
    "section": "",
    "text": "Lancaster CSML\nLancaster AI Reading Group\nUCL Fundamentals of Statistical Machine Learning\nGP Seminar Series\n\n\n\n\n\nProb_AI Hub\nSTOR-i\nTIDE"
  },
  {
    "objectID": "resources.html#academic-groups-and-organisations",
    "href": "resources.html#academic-groups-and-organisations",
    "title": "Resources",
    "section": "",
    "text": "Lancaster CSML\nLancaster AI Reading Group\nUCL Fundamentals of Statistical Machine Learning\nGP Seminar Series\n\n\n\n\n\nProb_AI Hub\nSTOR-i\nTIDE"
  },
  {
    "objectID": "enlightenment_gallery.html",
    "href": "enlightenment_gallery.html",
    "title": "Rui-Yang Zhang",
    "section": "",
    "text": "It was early 2021 when I first visited the British Museum. I was in my first year of undergraduate at UCL, and it was the middle stage of covid where lockdowns were on and off. It was a confusing time to most, and especially to me, as I was in between the discomfort of moving to a new country in a foreign continent, the uncertainties in an increasingly precarious world, and the urge for a sensible life plan.\nMy first visit was on a cloudy afternoon, rather typical weather for London during that time of the year. Few visitors were in the museum, and the ones there were subconsciously maintaining some distance from each other. The enlightenment gallery was the last stop of my visit that time, and I was exhausted, mentally and physically, from walking nonstop for two hours and seeing countless artefacts.\nThe curtains of the gallery were loosely pulled down, allowing the afternoon sun to permeate through. The long hallway - with seemingly endless shelves of books extending from the floor to the ceiling on the two sides- was bright enough to display its overwhelming wealth and dim enough to remain mysteriously noble. It was stunning. I walked towards the middle of the gallery and sat on a bench. Mesmerised by the weight of intellectual history.\nSince then, I have been back to the museum and that same enlightenment gallery numerous times, yet I could never recreate that same astonishment. Maybe I would never be in that same mental space, for better or for worse, and maybe the museum would never be that empty again, to leave me alone with the artefacts and the history."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Spatial-Temporal GP (3)\n\n\n\n\n\n\n\nGaussian Process\n\n\n\n\nA series of blog posts on spatial-temporal Gaussian processes. SDE Approach to Temporal GP Regression.\n\n\n\n\n\n\nMar 30, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nExpected Information Gain with Gaussian Process Surrogate Models\n\n\n\n\n\n\n\nActive Learning\n\n\nGaussian Process\n\n\n\n\nComputations and derivations of the expected information gain utility function of active learning when the surrogate model is a conjugate Gaussian process.\n\n\n\n\n\n\nFeb 7, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nSummary of a Bivariate Gaussian Covariance Matrix\n\n\n\n\n\n\n\nActive Learning\n\n\nGaussian Process\n\n\n\n\nVarious ways one could summarise a bivariate Gaussian’s covariance matrix.\n\n\n\n\n\n\nFeb 3, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Derivation Scribbles] 3D-Var and 4D-Var\n\n\n\n\n\n\n\nData Assimilation\n\n\nDerivation Scribbles\n\n\n\n\nDerivations for 3D-Var and 4D-Var.\n\n\n\n\n\n\nDec 3, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Derivation Scribbles] Kalman Filter and Ensemble Kalman Filter\n\n\n\n\n\n\n\nData Assimilation\n\n\nDerivation Scribbles\n\n\n\n\nDerivations for Kalman filters and Ensemble Kalman Filter.\n\n\n\n\n\n\nNov 28, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nSpatial-Temporal GP (2)\n\n\n\n\n\n\n\nGaussian Process\n\n\n\n\nA series of blog posts on spatial-temporal Gaussian processes. Exploiting the Kronecker structure of temporal GP regression with 1d space.\n\n\n\n\n\n\nOct 31, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nSpatial-Temporal GP (1)\n\n\n\n\n\n\n\nGaussian Process\n\n\n\n\nA series of blog posts on spatial-temporal Gaussian processes. Temporal GP regression with 1d space.\n\n\n\n\n\n\nOct 23, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Derivation Scribbles] Basic GP Regression Formula\n\n\n\n\n\n\n\nGaussian Process\n\n\nDerivation Scribbles\n\n\n\n\nDerivations for the Gaussian process predictive distribution. Single-output GP, observe with additive Gaussian noise.\n\n\n\n\n\n\nOct 13, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Reading Notes] Environmental sensor placement with convolutional Gaussian neural processes\n\n\n\n\n\n\n\nGaussian Process\n\n\nExperiment Design\n\n\nReading Notes\n\n\n\n\nReading Notes on Environmental sensor placement with convolutional Gaussian neural processes\n\n\n\n\n\n\nSep 30, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Reading Notes] Pre-trained Gaussian Processes for Bayesian Optimization\n\n\n\n\n\n\n\nBayesian Optimisation\n\n\nGaussian Process\n\n\nActive Learning\n\n\nExperiment Design\n\n\nReading Notes\n\n\n\n\nReading Notes on Pre-trained Gaussian Processes for Bayesian Optimization\n\n\n\n\n\n\nSep 29, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nWhy Should We Care About Gradient Flows?\n\n\n\n\n\n\n\nGradient Flow\n\n\n\n\nBlog post on gradient flows in Euclidean and Wasserstein spaces.\n\n\n\n\n\n\nSep 13, 2024\n\n\nRui-Yang Zhang, Christopher Nemeth\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-09-29-test/index.html",
    "href": "posts/2024-09-29-test/index.html",
    "title": "Reading Notes: Pre-trained Gaussian Processes for Bayesian Optimization",
    "section": "",
    "text": "Paper Link: https://www.jmlr.org/papers/volume25/23-0269/23-0269.pdf"
  },
  {
    "objectID": "posts/2024-09-29-test/index.html#motivation",
    "href": "posts/2024-09-29-test/index.html#motivation",
    "title": "Reading Notes: Pre-trained Gaussian Processes for Bayesian Optimization",
    "section": "Motivation",
    "text": "Motivation\nBO relies on surrogate models (often chosen to be GPs). A GP is specified by a kernel (and a mean sometimes), which reflects our prior knowledge about the black box function of interest. In addition, since we often do not have too many observations being queried from the black box function, the outcome will be sensitive to prior choices.\nIn this paper, the authors are motivated by BO in the context of hyperparameter tuning of large-scale ML models, where the common choices of the kernel (e.g. SE, Matern) are often unrealistic. In many settings where a GP prior is decided, a lot of domain / contextual knowledge is first gathered to make a sensible guess at how the function might look like (e.g. periodicities, spectral properties, smoothness). Such knowledge is often missing in the ML model hyperparameter tuning case.\nTo resolve this issue, the authors proposed a “transfer learning” / “information borrowing” / “pre-training” / “meta-learning” approach to turning existing final GP models from similar large-scale ML models’ hyperparameter tuning tasks into priors for the next task with a slightly different ML model. The new approach is called the HyperBO."
  },
  {
    "objectID": "posts/2024-09-29-test/index.html#methods",
    "href": "posts/2024-09-29-test/index.html#methods",
    "title": "Reading Notes: Pre-trained Gaussian Processes for Bayesian Optimization",
    "section": "Methods",
    "text": "Methods\nThe key component of HyperBO is the construction of a pre-trained prior GP. Everything else goes like the standard BO loops. We denote \\(f\\)as our current black-box function that we wish to run BO on. We have past knowledge about the functions \\(f_1, f_2, \\ldots, f_N\\) with observation datasets \\(D_1, D_2, \\ldots, D_N\\) each consists of \\(M\\) location-value pairs of the corresponding function.\nThe main assumption (Assumption 1 of paper) made in this paper, which is essential for the construction of our pre-trained prior, is that all the previously explored functions as well as our current function are i.i.d. draws from a common meta GP, i.e. \\(f_1, f_2, \\ldots, f_N, f \\sim \\mathcal{GP}(\\mu^*, k^*)\\). The other assumption (Assumption 2), which is more minor, is that the observations are all noisy with noise being additive, centred, Gaussian, and of unknown constant variance \\(\\sigma_*^2\\).\nUnder these two assumptions, we are ready to explain how the pre-trained prior is obtained. We construct a loss function \\(\\mathcal{L}\\) of the form\n\\[\n\\mathcal{L}(\\mu, k, \\sigma^2)=\\text{KL}\\left(\\mathcal{GP}_{\\sigma_*^2}(\\mu^*, k^*), \\mathcal{GP}_{\\sigma^2}(\\mu, k)\\right)\n\\]\nwhere the subscripts \\(\\sigma^2, \\sigma_*^2\\) denote the variances of the observation noise, and this is used to be optimised to obtain our estimations of the hyperparameters of our pre-trained GP. The equation above corresponds to Equation (1) in the paper with slightly adjusted notations.\nThe paper proposed two ways - KL-based and likelihood-based - to rewrite the above loss function to make it more computationally tractable."
  },
  {
    "objectID": "posts/2024-09-29-test/index.html#discussions",
    "href": "posts/2024-09-29-test/index.html#discussions",
    "title": "Reading Notes: Pre-trained Gaussian Processes for Bayesian Optimization",
    "section": "Discussions",
    "text": "Discussions\nThe approach proposed in the paper can be (at least intuitively) related to the empirical Bayes approach of inference, where the prior is obtained using some sort of MLEs of the parameters using the observations. Here, the prior GP is modelled as a sample from the meta GP \\(\\mathcal{GP}(\\mu^*, k^*)\\) with hyperparameters estimated using existing data of previous BO tasks.\nIt could also be beneficial to consider the potential link between this work and the generalised Bayes ideas (e.g. A General Framework for Updating Belief Distributions and An Optimization-centric View on Bayes’ Rule). Slightly more concretely, maybe those works could provide a different way of pre-training our GP prior given past observations.\nThe method proposed in this paper is heavily dependent on the problem setup, especially the Assumption 1 of the paper. This assumption is sensible in many contexts, such as the hyperparameter tuning examples mentioned in the paper, but it is certainly not generally applicable."
  },
  {
    "objectID": "posts/2024-09-29-pretrain-GP-BO/index.html",
    "href": "posts/2024-09-29-pretrain-GP-BO/index.html",
    "title": "[Reading Notes] Pre-trained Gaussian Processes for Bayesian Optimization",
    "section": "",
    "text": "Paper Link: https://www.jmlr.org/papers/volume25/23-0269/23-0269.pdf"
  },
  {
    "objectID": "posts/2024-09-29-pretrain-GP-BO/index.html#motivation",
    "href": "posts/2024-09-29-pretrain-GP-BO/index.html#motivation",
    "title": "[Reading Notes] Pre-trained Gaussian Processes for Bayesian Optimization",
    "section": "Motivation",
    "text": "Motivation\nBO relies on surrogate models (often chosen to be GPs). A GP is specified by a kernel (and a mean sometimes), which reflects our prior knowledge about the black box function of interest. In addition, since we often do not have too many observations being queried from the black box function, the outcome will be sensitive to prior choices.\nIn this paper, the authors are motivated by BO in the context of hyperparameter tuning of large-scale ML models, where the common choices of the kernel (e.g. SE, Matern) are often unrealistic. In many settings where a GP prior is decided, a lot of domain / contextual knowledge is first gathered to make a sensible guess at how the function might look like (e.g. periodicities, spectral properties, smoothness). Such knowledge is often missing in the ML model hyperparameter tuning case.\nTo resolve this issue, the authors proposed a “transfer learning” / “information borrowing” / “pre-training” / “meta-learning” approach to turning existing final GP models from similar large-scale ML models’ hyperparameter tuning tasks into priors for the next task with a slightly different ML model. The new approach is called the HyperBO."
  },
  {
    "objectID": "posts/2024-09-29-pretrain-GP-BO/index.html#methods",
    "href": "posts/2024-09-29-pretrain-GP-BO/index.html#methods",
    "title": "[Reading Notes] Pre-trained Gaussian Processes for Bayesian Optimization",
    "section": "Methods",
    "text": "Methods\nThe key component of HyperBO is the construction of a pre-trained prior GP. Everything else goes like the standard BO loops. We denote \\(f\\)as our current black-box function that we wish to run BO on. We have past knowledge about the functions \\(f_1, f_2, \\ldots, f_N\\) with observation datasets \\(D_1, D_2, \\ldots, D_N\\) each consists of \\(M\\) location-value pairs of the corresponding function.\nThe main assumption (Assumption 1 of paper) made in this paper, which is essential for the construction of our pre-trained prior, is that all the previously explored functions as well as our current function are i.i.d. draws from a common meta GP, i.e. \\(f_1, f_2, \\ldots, f_N, f \\sim \\mathcal{GP}(\\mu^*, k^*)\\). The other assumption (Assumption 2), which is more minor, is that the observations are all noisy with noise being additive, centred, Gaussian, and of unknown constant variance \\(\\sigma_*^2\\).\nUnder these two assumptions, we are ready to explain how the pre-trained prior is obtained. We construct a loss function \\(\\mathcal{L}\\) of the form\n\\[\n\\mathcal{L}(\\mu, k, \\sigma^2)=\\text{KL}\\left(\\mathcal{GP}_{\\sigma_*^2}(\\mu^*, k^*), \\mathcal{GP}_{\\sigma^2}(\\mu, k)\\right)\n\\]\nwhere the subscripts \\(\\sigma^2, \\sigma_*^2\\) denote the variances of the observation noise, and this is used to be optimised to obtain our estimations of the hyperparameters of our pre-trained GP. The equation above corresponds to Equation (1) in the paper with slightly adjusted notations.\nThe paper proposed two ways - KL-based and likelihood-based - to rewrite the above loss function to make it more computationally tractable.\n\n\n\nIllustration of the Pre-Train GP"
  },
  {
    "objectID": "posts/2024-09-29-pretrain-GP-BO/index.html#discussions",
    "href": "posts/2024-09-29-pretrain-GP-BO/index.html#discussions",
    "title": "[Reading Notes] Pre-trained Gaussian Processes for Bayesian Optimization",
    "section": "Discussions",
    "text": "Discussions\nThe approach proposed in the paper can be (at least intuitively) related to the empirical Bayes approach of inference, where the prior is obtained using some sort of MLEs of the parameters using the observations. Here, the prior GP is modelled as a sample from the meta GP \\(\\mathcal{GP}(\\mu^*, k^*)\\) with hyperparameters estimated using existing data of previous BO tasks.\nIt could also be beneficial to consider the potential link between this work and the generalised Bayes ideas (e.g. A General Framework for Updating Belief Distributions and An Optimization-centric View on Bayes’ Rule). Slightly more concretely, maybe those works could provide a different way of pre-training our GP prior given past observations.\nThe method proposed in this paper is heavily dependent on the problem setup, especially the Assumption 1 of the paper. This assumption is sensible in many contexts, such as the hyperparameter tuning examples mentioned in the paper, but it is certainly not generally applicable."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html",
    "href": "posts/2024-09-13-WGF/index.html",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "",
    "text": "Optimisation is a fundamental task in modern-day statistics and machine learning. A large set of problems in machine learning and statistics can be easily phrased as an optimisation problem - given some objective function \\(f\\) defined on a domain \\(\\mathcal{X}\\), we wish to find a point \\(x \\in \\mathcal{X}\\) that minimises \\(f\\) (or maximises \\(-f\\)). Sometimes, we do not even need to find the global minimum of \\(f\\), and a sufficiently close local minimum would be good too."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#gradient-flows-in-the-euclidean-space",
    "href": "posts/2024-09-13-WGF/index.html#gradient-flows-in-the-euclidean-space",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "Gradient Flows in the Euclidean Space",
    "text": "Gradient Flows in the Euclidean Space\nA common optimisation algorithm is the gradient descent. If our objective function \\(f\\) defined on the Euclidean space \\(\\mathbb{R}^d\\) is continuous and we can compute its gradient \\(\\nabla f\\), then, the gradient descent algorithm will iteratively apply the following update\n\\[\nx_{n+1} = x_n - h \\nabla f(x_n)\n\\]\nuntil we converge or reach a termination point. The parameter \\(h&gt;0\\) above is the step size of our algorithm, often referred to as a learning rate and it is a tuning parameter of the gradient descent algorithm. When we set \\(h\\) to be very small, and let it tend to zero, we would convert the above discrete-in-time algorithm into a continuous-in-time algorithm, described as\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t} x_t = -\\nabla f(x_t)\n\\]\nwhere we use \\(t\\) instead of \\(n\\) to denote the time index as we are in continuous time rather than discrete time. Notice that for the above ordinary differential equation (ODE), after an Euler discretisation (of time), will become the gradient descent algorithm. The ODE is known as the gradient flow (in Euclidean space), and we can show that various frequently used algorithms can be interpreted as different discretisations of the gradient flow. For example, an implicit Euler discretisation of the gradient flow gives us the proximal point algorithm.\nOne can certainly see the conceptual benefit of considering gradient flow for understanding discrete-in-time optimisation algorithms - we suddenly have a simple, elegant mental picture of the limiting case of these procedures. However, rather unfortunately, the gradient flow in Euclidean space could not help us that much more than that. Often in theoretical analysis of iterative algorithms, we are interested in the convergence rate of these algorithms to some target value, and in the cases where approximations happen in the algorithms, we are interested in capturing the errors induced. Because of the discretisation in time, we could not translate many of the theories about gradient flow in Euclidean space into their discrete-in-time counterparts. This is the main reason why although gradient flows are extremely natural and tempting to investigate, they have not been considered as much, until very recently."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#the-langevin-diffusion",
    "href": "posts/2024-09-13-WGF/index.html#the-langevin-diffusion",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "The Langevin Diffusion",
    "text": "The Langevin Diffusion\nA major breakthrough, at least from a theoretical perspective, happened with Jordan, Kinderlehrer & Otto’s 1998 paper The Variational Formulation of the Fokker-Planck Equation. In there, the authors made an explicit connection between the Langevin diffusion, a particular type of Stochastic Differential Equation (SDE) with very nice equilibrium properties, and a gradient flow in the space of probability distributions. The Langevin diffusion can be characterised by the SDE\n\\[\n\\mathrm{d}X_t = \\nabla \\log \\pi(X_t) \\mathrm{d}t + \\sqrt{2}\\mathrm{d}B_t\n\\]\nwhere \\(\\{B_t\\}\\) is a Brownian motion and \\(\\pi\\) is the equilibrium distribution of the process, and it could also be characterised by the Fokker-Planck equation\n\\[\n\\partial_t p_t(x) = \\text{div} \\left( p_t(x) \\nabla \\log \\frac{p_t(x)}{\\pi(x)} \\right)\n\\]\nwhere \\(p_t(x)\\) is the probability distribution of \\(X_t\\). Naively, one can think about the two characterisations of the Langevin diffusion as a state space version and a distribution space version of the same motion.\nSo, the paper of JKO1998 established that the Fokker-Planck equation of the Langevin diffusion is equivalent to a gradient flow in the Wasserstein space with the objective function being the KL divergence \\(f(\\cdot) = \\text{KL}(\\cdot \\| \\pi)\\) where\n\\[\n\\text{KL}(p\\| q) := \\int p(x) \\log[p(x) / q(x)] dx = \\mathbb{E}_{X \\sim p} [\\log ( p(X)/q(X)) ].\n\\]\nIntuitively, what this connection tells us is that the particles following a Langevin diffusion are moving - in the steepest direction - towards their equilibrium distribution.\nAs an example, let’s assume that our target distribution of interest \\(p\\) is a Gaussian \\(\\mathcal{N}(0,1)\\) and particles are represented by the distribution \\(q\\). As seen in the following movie, we can use the Wasserstein gradient flow of KL divergence to sequentially evolve \\(q\\) and minimise the KL divergence.\n (Thanks to Louis Sharrock for creating this movie)\nThis result seems neat, but what is so special about this Langevin diffusion? It turns out that the Langevin diffusion is rather fundamental in sampling algorithms for computational statistics."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#monte-carlo-sampling",
    "href": "posts/2024-09-13-WGF/index.html#monte-carlo-sampling",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "Monte Carlo Sampling",
    "text": "Monte Carlo Sampling\nIn statistics, especially in Bayesian statistics, we would often run into the problem of having a complicated probability distribution that we wish to compute expectations of, such as in the case of computing the posterior mean of a parameter of interest. If the distribution is complex and we cannot analytically evaluate our expectations of interest, then we often rely on using (independent) samples from the distribution to form an empirical approximation of the distribution. To be more precise, if we have a target probability distribution \\(\\pi\\), we will get a sequence of independent samples \\(X_1, X_2, \\ldots, X_n \\sim \\pi\\) and we have\n\\[\n\\pi(x) \\approx \\frac{1}{n} \\sum_{k=1}^n 1_{X_k}(x)\n\\]\nwhere \\(1_{X_k}(x)\\) is the indicator function that takes the value 1 when \\(x = X_k\\) and zero otherwise. This is the Monte Carlo method, and it can be shown that under weak conditions of the target distribution \\(\\pi\\), the empirical distribution converges to \\(\\pi\\) at a rate of \\(O(1/\\sqrt{n})\\) for \\(n\\) Monte Carlo samples. The only problem with the Monte Carlo method is, how do we get those samples? As alluded slightly from the Langevin diffusion, since we can set the equilibrium distribution of a Langevin diffusion to (almost) any target distribution and the process will converge to it after running for a while, we can just start the SDE at some point and run it for long enough so it hits the equilibrium, and use the trajectories afterwards as samples from the target distribution.\nImmediately, we would ask - how exactly do we simulate a continuous-in-time SDE? The simplest solution is to use the Euler-Maruyama scheme and obtain discretisations using the following iterative procedure\n\\[\nX_{(n+1)h} = X_{nh} + h \\nabla \\log \\pi(X_{nh})+\\sqrt{2h} \\xi\n\\]\nwhere \\(\\xi \\sim N(0,1)\\). This gives us the unadjusted Langevin algorithm (ULA), also known as the Langevin Monte Carlo (LMC) algorithm in the machine learning literature.\nSince this is a discretisation, it introduces some numerical errors (the precise reason for the errors will be explained in a bit) and by using ULA we will not obtain exact samples from the target distribution \\(\\pi\\). For sufficiently small \\(h\\), the error would be tolerable. We could also do smart things such as Metropolis adjustments to remove the error, and we would recover the Metropolis Adjusted Langevin Algorithm (MALA) which is a staple of the Markov chain Monte Carlo (MCMC) algorithms for computational statistics. More thorough discussions on MCMC algorithms can be found in textbooks such as Monte Carlo Statistical Methods by Robert & Casella, or the recent Scalable Monte Carlo for Bayesian Learning by Fearnhead, Nemeth, Oates & Sherlock. One could also find a more detailed theoretical study of ULA in Roberts & Tweedie’s 1996 paper Exponential Convergence of Langevin Distributions and their Discrete Approximations."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation",
    "href": "posts/2024-09-13-WGF/index.html#wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "Wasserstein Gradient Flow - a Bridge between Sampling and Optimisation",
    "text": "Wasserstein Gradient Flow - a Bridge between Sampling and Optimisation\nSo far, we have learnt that the Langevin diffusion can be viewed as a gradient flow, and the discrete-in-time version of the Langevin diffusion allows us to draw samples from a target distribution. It turns out that we can also interpret the discrete Langevin diffusion of the LMC as a discrete-in-time approximation of the corresponding gradient flow in the space of probability distributions (to be more precise, the Wasserstein space, so we would often call this type of gradient flow a Wasserstein gradient flow).\nIn the 2018 paper Sampling as Optimization in the Space of Measures by Wibisono, the author pointed out that the LMC as an Euler-Maruyama discretisation of the Langevin diffusion can be viewed as a forward-flow splitting discretisation of the Wasserstein gradient flow with the objective function being the KL divergence. The forward-flow splitting scheme is a way to discretise time by doing half a step of forward discretisation, and half a step of flow discretisation, for each full step of the iteration. The expression of the two discretisations is slightly involved to describe in the space of probability distributions, but if we translate them into the state space, it is simply\n\\[\n\\text{(forward)} \\ X_{(n+1/2)h} = X_{nh} + h \\nabla \\log \\pi(X_{nh}),\n\\] \\[\n\\text{(flow)} \\ X_{(n+1)h} = X_{(n+1/2)h} + \\sqrt{2h} \\xi\n\\] with \\(\\xi \\sim N(0,1)\\), which combines to give us the full LMC update. Another observation in Wibisono (2018) is that, if we swap the flow step with a backward discretisation step, we would be able to cancel the error of discretising the Langevin diffusion. Unfortunately, the backward step is not implementable in general. Nevertheless, this paper provides us with the very important information that there exists a hidden connection between sampling (using LMC) and optimisation (using gradient flows). A bridge between the two areas has been formally built at this point.\nTo further utilise the power of this connection, Durmus, Majewski & Miasojedow in their 2019 paper Analysis of Langevin Monte Carlo via Convex Optimization provided us with a more explicit characterisation of the error of LMC using convergence analysis of the Wasserstein gradient flow. Unlike in the case of gradient flows in Euclidean space, the theoretical studies of Wasserstein gradient flows can actually be used in the analysis of their discrete-in-time counterparts."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#what-else-can-we-do",
    "href": "posts/2024-09-13-WGF/index.html#what-else-can-we-do",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "What Else Can We Do?",
    "text": "What Else Can We Do?\nAt this point, it should be clear that the connection between sampling and optimisation established using Wasserstein gradient flows is promising and potentially very useful.\nOne immediate area of work is to interpret existing sampling algorithms as gradient flows, and use these realisations to help us better understand the properties of these algorithms. There are already some successful fruits from this branch:\n\nLiu’s 2017 paper Stein Variational Gradient Descent as Gradient Flow interpreted the Stein Variational Gradient Descent algorithm, a powerful sampling algorithm, as a type of gradient flow.\nDuncan, Nüsken & Szpruch’s 2023 paper On the Geometry of Stein Variational Gradient Descent built on the above realisation and showed several convergence results about the algorithm, as well as certain improvements based on such gradient flow analysis.\nNüsken’s 2024 paper Stein Transport for Bayesian Learning proposed a promising new algorithm Stein Transport that extends the Stein Variational Gradient Descent by tweaking the geometry of the Wassterstein gradient flow.\nChopin, Crucinio & Korba’s 2024 paper A connection between Tempering and Entropic Mirror Descent has established that tempering sequential Monte Carlo algorithms can be viewed as a type of discretisation of the gradient flow in the Fisher-Rao geometry.\n\nIn addition, a class of work that could be made possible with this new connection is those that translate algorithmic tricks from one field (say optimisation) to another (say sampling). A very nice example of this thinking is the line of work by Sharrock, Nemeth and coauthors over recent years. A lot of optimisation algorithms involve tuning parameters (also known as learning rates) that have to be manually adjusted, and different specifications of them will sometimes yield very different performance of the algorithms. To tackle the difficulties of tuning such parameters, there is a class of learning-rate-free algorithms that replace the tuning of learning rates with an automatic mechanism. With the help of the connection between sampling and optimisation made by gradient flows, recent work has managed to replace the manually-tuned learning rates of sampling algorithms with automatic, learning-rate-free ones, as shown in the papers such as Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates and Learning Rate Free Sampling in Constrained Domains.\nOverall, gradient flows and related ideas have become a promising tool for investigating theoretical properties of sampling algorithms, and have shown a considerable amount of potential to inspire designs of new sampling algorithms. There remains a vast pool of unanswered questions and possible extensions in this area. More breakthroughs are to be expected from this line of work.\nP.S. A book-length, formal introduction to the material covered above and more can be found in Statistical Optimal Transport by Chewi, Niles-Weed & Rigollet."
  },
  {
    "objectID": "posts/2024-09-30-ConvGNP/index.html",
    "href": "posts/2024-09-30-ConvGNP/index.html",
    "title": "[Reading Notes] Environmental sensor placement with convolutional Gaussian neural processes",
    "section": "",
    "text": "Paper Link: https://doi.org/10.1017/eds.2023.22\n## Motivation\nThe object of interest of this paper is the environmental / climate data, and the task of interest is to model them for various downstream tasks such as sequential experiment designs and predictions. The object of interest, climate data, has two key properties: (1) spatiotemporal non-stationarity, and (2) large data volume and high data variability. They will be further explained below. Ultimately, these two properties make the standard probabilistic model of choice - the Gaussian Process - unsuitable and a new model, the Convolutional Gaussian Neural Process (ConvGNP) is proposed in the paper as an alternative.\nClimate variables are non-stationary across time and space due to seasonality and other natural phenomena, so the probabilistic model must capture those characteristics to be sufficiently realistic. The standard Gaussian process could potentially encode them via the careful design of the kernel, which is non-trivial.\nThere is also a large volume of existing climate data, and they could be of very different formats (e.g. satellite images, weather station observations). GP is notoriously unscalable in data size, and the varied formats prevent us from a direct compilation of data since we cannot assume all the available data are numerical and measuring the same thing. This issue of data format variability also exists in other disciplines, such as ecology (e.g. citizen science and integrated population model).\nGiven these constraints of the problem, we need a model that scales better with data size and can learn non-stationarity more automatically, which motivates the introduction of ConvGNP."
  },
  {
    "objectID": "posts/2024-09-30-ConvGNP/index.html#model-setup",
    "href": "posts/2024-09-30-ConvGNP/index.html#model-setup",
    "title": "[Reading Notes] Environmental sensor placement with convolutional Gaussian neural processes",
    "section": "Model Setup",
    "text": "Model Setup\nThe model is trained as a regression with covariates. The base regression is an input (region of consideration) and output (environmental variable of interest) assisted by covariates / contexts (from other weather observations). The result of a fitted model is a map \\(\\pi\\) that takes a possible input along with its covariate values and returns a prediction of the output value.\nThe above is extremely high-level and overly simplistic. The overall map \\(\\pi\\) of ConvGNP is constructed as a Neural Net with the following structure:\n\\[\n\\text{Context Set }C \\to \\textbf{SetConv} \\longrightarrow \\textbf{U-Net} \\longrightarrow \\begin{matrix} \\textbf{Multilayer} \\\\ \\textbf{Perceptron} \\end{matrix}\\to \\begin{bmatrix} \\text{mean vector } f\\\\ \\text{covariance matrix }g \\end{bmatrix}\n\\]\nwhere the bold texts refer to the NN architecture and the standard texts refer to the inputs (the context set includes both the input location and the associated covariate values) and outputs (a mean vector and covariance matrix used for a overall multivariate Gaussian output).\nThe \\(\\textbf{SetConv}\\) layer fuses the various formats of data together on a regular grid that is enabled by interpolations so missing data and irregularly gridded data can be understood. The \\(\\textbf{U-Net}\\) produces a representation of the context set, like learning the latent variable structures. The \\(\\textbf{Multilayer Perceptron}\\) takes in the representations and outputs the mean vector and covariance matrix used by a multivariate Gaussian distribution to support a probabilistic outcome. Details about the NN architecture are omitted here.\nEssentially, ConvGNP uses an NN to ingest a large volume of data and outputs a predictive multivariate Gaussian distribution - combining the processing power and scale of NN and the uncertainty quantification of a GP.\nTo get a rough sense of the speed and the amount of data ConvGNP is capable of, here is a footnote from the paper.\n\nOur ConvGNP (with 4.16 M parameters) takes 0.88 s to process a total of 100,000 context points (21,600 temperature points and 78,400 gridded auxiliary points) and predict over 100,000 target points on a 16 GB NVIDIA A4 GPU using TensorFlow’s eager mode."
  },
  {
    "objectID": "posts/2024-09-30-ConvGNP/index.html#experiment-design",
    "href": "posts/2024-09-30-ConvGNP/index.html#experiment-design",
    "title": "[Reading Notes] Environmental sensor placement with convolutional Gaussian neural processes",
    "section": "Experiment Design",
    "text": "Experiment Design\nOne key application considered in this paper is to use a trained ConvGNP to assist an experiment design task of sensor placement where we wish to find the optimal locations to place sensors in order to maximise our knowledge about a region’s environmental variable observations. The setup considered in the paper’s experiment is that we are placing the sensors at once, instead of sequentially (in the case of sequential experiment designs and Bayesian optimisation).\nSome numerical simulations are conducted in the paper to compare the performance of experiment designs when surrogate models are ConvGNP and other GP models. ConvGNP does perform better, but it is also using A LOT more data than the other GP models (for example the contextual data are not used for the other GP models). It would be interesting to see how the performance comparison will change when we allow the other more standard GP models to incorporate more “expert knowledge” summarised from the contextual data.\nAnother thing that ConvGNP could not do - at least the version of the model introduced in the paper - is to sequentially update itself using incremental observations, which can be done by standard GPs. This will be a nice feature that enables sequential experiment designs, which are very relevant in the context of environmental data explorations."
  },
  {
    "objectID": "posts/2024-09-30-ConvGNP/index.html#discussion",
    "href": "posts/2024-09-30-ConvGNP/index.html#discussion",
    "title": "[Reading Notes] Environmental sensor placement with convolutional Gaussian neural processes",
    "section": "Discussion",
    "text": "Discussion\nOverall, the paper proposed a deep learning model ConvGNP that can be used to model and predict environmental data and admits natural uncertainty quantifications. The model is extremely suitable in cases where we have an abundance of available training data with varying formats and fidelities. The uncertainty quantifications provided by ConvGNP allow applications to some types of experiment designs, which is great. Further work that extends the model to allow sequential updates and data assimilations would further enhance its attractiveness in the sequential experiment design application."
  },
  {
    "objectID": "posts/2024-10-13-basic-GP-regression-formula/index.html",
    "href": "posts/2024-10-13-basic-GP-regression-formula/index.html",
    "title": "[Derivation Scribbles] Basic GP Regression Formula",
    "section": "",
    "text": "Gaussian Process Regression, adapted from https://docs.jaxgaussianprocesses.com/.\n\n\n\nBlock Matrix Inversion\nThe first thing we need to establish is the block matrix inversion identity. Consider an invertible matrix \\(\\Sigma\\) that can be written as\n\\[\n\\Sigma = \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}\n\\]\nwhere \\(\\Sigma_{AA}, \\Sigma_{AB}, \\Sigma_{BA}, \\Sigma_{BB}\\) are matrices of the right dimension and sufficiently non-singular. Next, we have the block matrix inversion identity stated below.\n\\[\n\\begin{split}\n\\Sigma^{-1} &= \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}^{-1} \\\\\n&= \\begin{bmatrix} (\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1}  & -(\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1} \\Sigma_{AB} \\Sigma_{BB}^{-1}\\\\ -\\Sigma_{BB}^{-1} \\Sigma_{BA}(\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1}  & (\\Sigma_{BB} - \\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB})^{-1} \\end{bmatrix}.\n\\end{split}\n\\]\n\n\nMarginal and Conditional Gaussians\nConsider a multivariate Gaussian distribution \\(x = (x_A, x_B)^T\\) where \\(x_A\\) is \\(d_A\\) dimensional, \\(x_B\\) is \\(d_B\\) dimensional, and \\(x\\) is \\(d = d_A + d_B\\) dimensional. The mean vector and covariance matrix of the multivariate Gaussian is set to be as follows:\n\\[\nx = \\begin{bmatrix} x_A \\\\ x_B \\end{bmatrix} \\sim N_d \\left( \\mu, \\Sigma\\right) = N_d \\left( \\begin{bmatrix} \\mu_A \\\\ \\mu_B \\end{bmatrix}, \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}\\right).\n\\]\nIt is easy to notice that the marginal distributions \\(x_A\\) and \\(x_B\\) can be obtained by selecting the needed entries of the above equation, i.e. \n\\[\n\\begin{split}\nx_A &\\sim N_{d_A}(\\mu_A, \\Sigma_{AA}), \\\\\nx_B &\\sim N_{d_B}(\\mu_B, \\Sigma_{BB}).\n\\end{split}\n\\]\nThe conditional distributions are a bit tricky, which we will derive below. Due to symmetry, we will derive the conditional distribution \\(x_A | x_B\\) and just state \\(x_B | x_A\\). Using \\(p(\\cdot)\\) to denote the density of a random variable, we have\n\\[\n\\begin{split}\np(x_A | x_B) &= \\frac{p(x_A, x_B)}{p(x_B)} \\\\ &\\propto \\exp\\left\\{  -\\frac{1}{2} (x - \\mu)^T\\Sigma^{-1}(x - \\mu) \\right\\}.\n\\end{split}\n\\]\nFocusing on the terms inside the second exponential, we first denote\n\\[\n\\Sigma^{-1} = \\begin{bmatrix} V_{AA} & V_{AB} \\\\ V_{BA} & V_{BB} \\end{bmatrix}\n\\]\nwhich then yield\n\\[\n\\begin{split}\n&\\quad  (x - \\mu)^T\\Sigma^{-1}(x - \\mu) \\\\\n&=  \\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix}^T \\begin{bmatrix} V_{AA} & V_{AB} \\\\ V_{BA} & V_{BB} \\end{bmatrix}\\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix}  \\\\\n&= \\begin{bmatrix} (x_A - \\mu_A)^T V_{AA} + (x_B - \\mu_B)^T V_{BA} \\\\ (x_A - \\mu_A)^T V_{AB} + (x_B - \\mu_B)^T V_{BB} \\end{bmatrix}^T\\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix} \\\\\n&= (x_A - \\mu_A)^T V_{AA} (x_A - \\mu_A) + (x_A - \\mu_A)^T V_{AB} (x_B - \\mu_B) \\\\\n&\\quad +  (x_B - \\mu_B)^T V_{BA} (x_A - \\mu_A) + (x_B - \\mu_B)^T V_{BB} (x_B - \\mu_B).\n\\end{split}\n\\]\nWe can keep terms with \\(x_A\\) and put the rest into the normalising constant. As \\(V_{AA}\\) is square and \\(V_{AB}= V_{BA}^T\\), we can simplify our above equation into\n\\[\n\\begin{split}\n&\\quad x_A^T V_{AA} x_A - 2 x_A^T V_{AA} \\mu_A + 2x_A^T V_{AB} (x_B - \\mu_B) \\\\\n&= x_A^T V_{AA} x_A - 2 x_A^T [ V_{AA} \\mu_A +V_{AB} (x_B - \\mu_B)] \\\\\n&= (x_A - \\mu')^T V_{AA}(x_A - \\mu')+ C\n\\end{split}\n\\]\nfor some constant \\(C\\) independent of \\(x_A\\) and the newly defined\n\\[\n\\mu' = \\mu_A - V_{AA}^{-1}V_{AB} (x_B - \\mu_B).\n\\]\nTherefore, using the values of \\(V_{AA}, V_{AB}\\) from the block matrix inversion formula earlier, we have\n\\[\n\\begin{split}\n\\mu' &= \\mu_A - V_{AA}^{-1}V_{AB} (x_B - \\mu_B) \\\\\n&= \\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}(x_B - \\mu_B) \\\\\nV_{AA}^{-1} &= \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA} \\\\\n\\end{split}\n\\]\nand via symmetry, we have the conditional distributions\n\\[\n\\begin{split}\nx_A | x_B &\\sim N_{d_A}(\\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}(x_B - \\mu_B), \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}), \\\\\nx_B | x_A &\\sim N_{d_B}(\\mu_B + \\Sigma_{BA}\\Sigma_{AA}^{-1}(x_A - \\mu_A), \\Sigma_{BB} - \\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB}).\n\\end{split}\n\\]\n\n\nGaussian Process Regression\nConsider we have a single-output Gaussian process \\(f \\sim \\mathcal{GP}(\\mu, k)\\) where \\(\\mu\\) is the mean function and \\(k\\) is the kernel function. The support of this GP is assumed to be \\(\\mathbb{R}^d\\). Consider we have made \\(m\\) observations of this GP \\(f\\) where the observations are made at locations \\(X \\in \\mathbb{R}^m\\) with values \\(y \\in \\mathbb{R}^m\\) and the observations are noisy with independent additive Gaussian noise of variance \\(\\sigma^2\\), i.e. \\(y = f(X) + \\xi\\) with \\(\\xi_i \\sim N(0, \\sigma^2) ~\\forall i = 1, 2, \\ldots, m\\). Denote the existing observations as \\(\\mathcal{D} = \\{ X, y \\}\\).\nUnder our modelling assumptions, we could write down the (log) likelihood of the \\(m\\) observations \\(y\\) under our GP prior \\(f \\sim \\mathcal{GP}(\\mu, k)\\). Since \\(y = f(X) + \\xi\\), we have\n\\[\ny | X \\sim N_m \\left( \\mu(X), k(X, X) + \\sigma^2 I_m \\right)\n\\] paramerised by \\(\\theta\\) (e.g. observation noise \\(\\sigma\\), lengthscale and variance of the kernel \\(k\\)) which gives us the following log likelihood\n\\[\n\\log p(y|X) = - \\frac{m}{2}\\log(2\\pi) - \\log | k(X, X) + \\sigma^2 I_m | - \\frac{1}{2} \\left( y - \\mu(X) \\right)^T ( k(X, X) + \\sigma^2 I_m)^{-1}\\left( y - \\mu(X) \\right)\n\\] that we maximise w.r.t. \\(\\theta\\) to obtain the maximum likelihood estimators of the (hyper)parameters.\nNext, conditional on these observations, we wish to know the distributions of the GP at test points \\(X_* \\in \\mathbb{R}^n\\), i.e. the conditional distribution \\(y_* = f(X_*) ~| \\mathcal{D}\\). This can be achieved by first model \\(y_*\\) and \\(y\\) jointly, then condition on \\(y\\). Using the conditional distribution formula above, we denote for simplicity the Gram matrices\n\\[\nK = k(X, X), \\qquad K_* = k(X, X_*), \\qquad K_{**}=k(X_*, X_*),\n\\]\nwhich gives us\n\\[\n\\begin{split}\ny_* ~|X_*, \\mathcal{D}, \\sigma^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= \\mu(X) + K_*^T (K + \\sigma^2 I_n)^{-1} y,\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1}K_*.\n\\end{split}\n\\]\nIn the common scenario where we assume \\(\\mu = 0\\), we further have the following GP predictive distribution\n\\[\n\\begin{split}\ny_* ~|X_*, \\mathcal{D}, \\sigma^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= K_*^T (K + \\sigma^2 I_n)^{-1} y,\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1}K_*.\n\\end{split}\n\\]"
  },
  {
    "objectID": "posts/2024-10-23-temporal-GP-1/index.html",
    "href": "posts/2024-10-23-temporal-GP-1/index.html",
    "title": "Spatial-Temporal GP (1)",
    "section": "",
    "text": "In this blog post, I will walk through how one could conduct temporal Gaussian process (GP) regression with one-dimensional space + one-dimensional time inputs and one-dimensional output. This is the first of a series of blog posts on spatial-temporal Gaussian processes.\n\nLatent Function\nThe latent function that we want our GP to learn is\n\\[\nf(x,t) = -t \\sin(2 \\pi x) + (1-t)\\cos(\\pi x)\n\\]\nwhere we will be focusing on the region of \\(x \\in [0,2]\\) and \\(t \\in [0,1]\\). We will obtain noisy observations from this function \\(f\\) at given time stamps and random locations. We will discretise time into \\(\\{0, 1/10, 2/10, \\ldots, 1\\}\\). At each of those times, we will observe \\(f\\) at 25 uniformly drawn locations (same locations across time) with additive Gaussian noise with mean zero and variance 0.01. The observations, denoted by \\(\\mathcal{D}\\) and the latent function are displayed below as a GIF.\n\n\n\nObservations\n\n\n\n\nPrior Distribution\nWe will need to define a Gaussian process to conduct the regression. Conceptually, we can treat time as just another dimension of the input without worrying much about its interpretation. So, we have a two dimensional input, one dimensional output Gaussian process. We would need to specify its mean and kernel functions. The mean function will be set to zero. The kernel will be a separable kernel, where we give an SE kernel to the space dimension and a Matérn 5/2 kernel to the time dimension. Therefore, our GP prior \\(g\\) could be written as\n\\[\ng \\sim \\mathcal{GP}({0}, {k}), \\qquad {k}(s,t) = k_s(s)\\times k_t(t)\n\\]\nwhere \\(k_s\\) is a SE kernel and \\(k_t\\) is a Matérn 5/2 kernel. A consequence of this construction of the overall kernel is that the Gram matrix \\({K}\\) can be written as a Kronecker product, i.e. \n\\[\n{K} = K_s \\otimes K_t\n\\]\nwhere \\({K}\\) is obtained using the Cartesian product \\(\\{(s_i, t_j)\\}_{i,j} = \\{s_i\\}_{i=1}^n \\times \\{t_j\\}_{j=1}^m\\) while \\(K_s, K_t\\) are obtained using \\(\\{s_i\\}_{i=1}^n\\) and \\(\\{t_j\\}_{i=1}^m\\) respectively. This matrix structure can be noticed easily from the following heat maps of the Gram matrices.\n\n\n\nGram Matrices\n\n\nBack to our GP prior \\(g\\). We can generate samples from it and they are displayed in the following GIF.\n\n\n\nPrior Samples\n\n\n\n\nPosterior Distribution\nWe have a prior and we have observations \\(\\mathcal{D}\\), now we will need to write down the likelihood and apply the Bayes rule to obtain the posterior distribution for our GP regression. The posterior distribution \\(g | \\mathcal{D}\\) is still a GP and its expression can be obtained exactly due to the conjugacy from the Gaussian noises. The GIF of mean, 2 standard deviation bound, and samples of the posterior distribution along with the latent function and the observations is presented below.\n\n\n\nPosterior Samples\n\n\n\n\nBenefits of Temporal Modelling\nThere is clear difference and benefits of directly including time into our GP model, although it increases the computational costs of regression. If we treat each GP at different time stamps as separate problems and fit a Gaussian process independently. A comparison of modelling without time and with time of the same problem is shown below.\n\n\n\nComparisons of GP Fits\n\n\nWe can see more stable performance for our temporal model, especially at \\(t=0.56\\) where the independent model overfits the data.\nAnother key benefits of having a temporal GP is that we can extrapolate our model to time stamps that we have not observed. This interpolation is not possible for the without-time model.\nA natural extension of the independent across time stamps modelling approach is to consider a multi-output Gaussian process model where we jointly model the functions at different times. It turns out that, if we assume a separable kernel for the multi-output GP, then it will be (roughly) equivalent to include time in the input space.\nFinally, the information borrowing across different time stamps help with the model’s robustness against missing data. In the following comparison plot, we sensor the left half and the right half of the observations at time \\(t = 0.33\\) and \\(t = 0.56\\) respectively. The independent version of the model fails immediately, while the performance of the temporal GP was not influenced by it.\n\n\n\nComparisons of GP Fits with Missing Data"
  },
  {
    "objectID": "posts/2024-10-31-temporal-GP-2/index.html",
    "href": "posts/2024-10-31-temporal-GP-2/index.html",
    "title": "Spatial-Temporal GP (2)",
    "section": "",
    "text": "In this blog post, I will walk through how one could exploit the Kronecker structure of the temporal Gaussian process (GP) regression with one-dimensional space + one-dimensional time inputs and one-dimensional output. This is the second of a series of blog posts on spatial-temporal Gaussian processes."
  },
  {
    "objectID": "posts/2024-10-31-temporal-GP-2/index.html#footnotes",
    "href": "posts/2024-10-31-temporal-GP-2/index.html#footnotes",
    "title": "Spatial-Temporal GP (2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrequentist MLE is equivalent to a Bayesian MAP with flat priors.↩︎\nsince we are using only a summary statistic (MAP with flat prior) for the parameters in the posterior predictive, instead of the full marginal posterior.↩︎"
  },
  {
    "objectID": "posts/2024-11-28-Kalman-filter/index.html",
    "href": "posts/2024-11-28-Kalman-filter/index.html",
    "title": "[Derivation Scribbles] Kalman Filter and Ensemble Kalman Filter",
    "section": "",
    "text": "This blog post is largely based on some notes written by Chris Sherlock."
  },
  {
    "objectID": "posts/2024-11-28-Kalman-filter/index.html#footnotes",
    "href": "posts/2024-11-28-Kalman-filter/index.html#footnotes",
    "title": "[Derivation Scribbles] Kalman Filter and Ensemble Kalman Filter",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn implicit notation throughout this blog post is that capital letters such as \\(X_t\\) represents random variables, while the lower cases like \\(x_t\\) represent the realisations of a random variable, thus is a constant.↩︎\nWe use the notation \\(1:t\\) to represent \\(1, 2, \\ldots, t\\).↩︎\nWe will slightly abuse the notation by including things such as \\(N(0,Q)\\) with properly defined random variables to ease of exposition.↩︎"
  },
  {
    "objectID": "posts/2024-12-03-3DVar-4DVar/index.html",
    "href": "posts/2024-12-03-3DVar-4DVar/index.html",
    "title": "[Derivation Scribbles] 3D-Var and 4D-Var",
    "section": "",
    "text": "This blog post follows from the previous post on the Kalman filter and ensemble Kalman filter."
  },
  {
    "objectID": "posts/2024-12-03-3DVar-4DVar/index.html#footnotes",
    "href": "posts/2024-12-03-3DVar-4DVar/index.html#footnotes",
    "title": "[Derivation Scribbles] 3D-Var and 4D-Var",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStandard results of gradient descent tells us, since our loss function (negative log density) is convex, convergence is guaranteed when we run the optimiser for long enough.↩︎"
  },
  {
    "objectID": "posts/2025-02-03-gaussian-cov/index.html",
    "href": "posts/2025-02-03-gaussian-cov/index.html",
    "title": "Summary of a Bivariate Gaussian Covariance Matrix",
    "section": "",
    "text": "For the active learning of a spatial vector field, one may impose a gird structure to the space and assign a random vector (in 2D) to each of the grid cell. The full vector field is modeled using a Gaussian process (e.g. a Helmholtz GP of Berlinghieri et al. (2023)), and under the Gaussian noise assumption, each random vector \\([u v]^T\\) of the grid cell \\((x,y)\\) is marginally a bivariate Gaussian:\n\\[\n\\begin{bmatrix} u_{(x,y)} \\\\ v_{(x,y)} \\end{bmatrix} \\sim N_2 \\left(\n\\begin{bmatrix} \\mu^u_{(x,y)} \\\\ \\mu^v_{(x,y)} \\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma^{uu}_{(x,y)} & \\Sigma^{uv}_{(x,y)} \\\\\n\\Sigma^{vu}_{(x,y)} & \\Sigma^{vv}_{(x,y)}\n\\end{bmatrix}\n\\right) = N_2 (\\mu_{(x,y)}, \\Sigma_{(x,y)}).\n\\]\nActive learning algorithms aim to choose the next design/evaluation point that yields the highest utility, where the utility is often linked to the uncertainty of the evaluation point or the full system. For example, if our surrogate model of the system has input dimension of one, i.e. each design point will be marginally an univariate Gaussian, a utility choice is simply the variance of that distribution, leading to the max-var utility. Notice that the variance of an univariate Gaussian \\(X\\) is monotonically related to its entropy\n\\[\nH(X) := \\frac{1}{2}  + \\frac{1}{2} \\log [2\\pi \\text{Var}(X)]\n\\] in the sense that for two univariate Gaussians \\(X_1, X_2\\) with variances \\(\\sigma_1^2, \\sigma_2^2\\) respectively, we have \\(\\sigma_1^2 \\le \\sigma_2^2 \\implies H(X_1) \\le H(X_2)\\) which should be obvious from the definition."
  },
  {
    "objectID": "posts/2025-02-03-gaussian-cov/index.html#bivariate-gaussian-representation",
    "href": "posts/2025-02-03-gaussian-cov/index.html#bivariate-gaussian-representation",
    "title": "Summary of a Bivariate Gaussian Covariance Matrix",
    "section": "Bivariate Gaussian Representation",
    "text": "Bivariate Gaussian Representation\nEach grid’s random vector (2D) follows a bivariate Gaussian distribution:\n\\[\n\\begin{bmatrix} U_{(x,y)} \\\\ V_{(x,y)} \\end{bmatrix} \\sim \\mathcal{N}_2 \\left(\n\\begin{bmatrix} \\mu^u_{(x,y)} \\\\ \\mu^v_{(x,y)} \\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma^{uu}_{(x,y)} & \\Sigma^{uv}_{(x,y)} \\\\\n\\Sigma^{vu}_{(x,y)} & \\Sigma^{vv}_{(x,y)}\n\\end{bmatrix}\n\\right)\n\\]\nAfter marginalizing the full Gaussian Process (GP) posterior/predictive:\n\\[\n\\mathcal{N}_2 (\\mu_{(x,y)}, \\Sigma_{(x,y)})\n\\]"
  },
  {
    "objectID": "posts/2025-02-03-gaussian-cov/index.html#max-variance-policies",
    "href": "posts/2025-02-03-gaussian-cov/index.html#max-variance-policies",
    "title": "Summary of a Bivariate Gaussian Covariance Matrix",
    "section": "Max-Variance Policies",
    "text": "Max-Variance Policies\nWe consider max-var styled policies, where we pick the evaluation point with the highest variance. In the case where we have a 2D spatial vector field to model and each vector is marginally \\(N_2\\), we try to summarize the evaluation point’s uncertainty using a function of the covariance matrix \\(\\Sigma\\).\n\nOptions for Summarizing Uncertainty:\n\nTrace of \\(\\Sigma\\)\n\\[\n\\text{tr}(\\Sigma) = \\Sigma_{11} + \\Sigma_{22}\n\\]\n\nThis captures the sum of the variances in both dimensions, but ignores the correlations.\n\nDeterminant of \\(\\Sigma\\)\n\\[\n\\det(\\Sigma) = \\Sigma_{11} \\Sigma_{22} - \\Sigma_{12} \\Sigma_{21} = \\lambda_1 \\lambda_2\n\\]\n\nThis represents the product of eigenvalues and can be interpreted as the “area” of uncertainty in the 2D space.\n\nNorm of \\(\\Sigma\\)\n\\[\n\\| \\Sigma \\|_{?}\n\\]\n\nDifferent matrix norms provide different interpretations of overall uncertainty. For example, the Frobenius norm is an element-wise norm that flattens the matrix \\(\\Sigma\\) and compute its \\(L_2\\) norm."
  },
  {
    "objectID": "posts/2025-02-03-gaussian-cov/index.html#background",
    "href": "posts/2025-02-03-gaussian-cov/index.html#background",
    "title": "Summary of a Bivariate Gaussian Covariance Matrix",
    "section": "",
    "text": "For the active learning of a spatial vector field, one may impose a gird structure to the space and assign a random vector (in 2D) to each of the grid cell. The full vector field is modeled using a Gaussian process (e.g. a Helmholtz GP of Berlinghieri et al. (2023)), and under the Gaussian noise assumption, each random vector \\([u v]^T\\) of the grid cell \\((x,y)\\) is marginally a bivariate Gaussian:\n\\[\n\\begin{bmatrix} u_{(x,y)} \\\\ v_{(x,y)} \\end{bmatrix} \\sim N_2 \\left(\n\\begin{bmatrix} \\mu^u_{(x,y)} \\\\ \\mu^v_{(x,y)} \\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma^{uu}_{(x,y)} & \\Sigma^{uv}_{(x,y)} \\\\\n\\Sigma^{vu}_{(x,y)} & \\Sigma^{vv}_{(x,y)}\n\\end{bmatrix}\n\\right) = N_2 (\\mu_{(x,y)}, \\Sigma_{(x,y)}).\n\\]\nActive learning algorithms aim to choose the next design/evaluation point that yields the highest utility, where the utility is often linked to the uncertainty of the evaluation point or the full system. For example, if our surrogate model of the system has input dimension of one, i.e. each design point will be marginally an univariate Gaussian, a utility choice is simply the variance of that distribution, leading to the max-var utility. Notice that the variance of an univariate Gaussian \\(X\\) is monotonically related to its entropy\n\\[\nH(X) := \\frac{1}{2}  + \\frac{1}{2} \\log [2\\pi \\text{Var}(X)]\n\\] in the sense that for two univariate Gaussians \\(X_1, X_2\\) with variances \\(\\sigma_1^2, \\sigma_2^2\\) respectively, we have \\(\\sigma_1^2 \\le \\sigma_2^2 \\implies H(X_1) \\le H(X_2)\\) which should be obvious from the definition."
  },
  {
    "objectID": "posts/2025-02-03-gaussian-cov/index.html#summaries-for-bivariate-gaussian-covariance",
    "href": "posts/2025-02-03-gaussian-cov/index.html#summaries-for-bivariate-gaussian-covariance",
    "title": "Summary of a Bivariate Gaussian Covariance Matrix",
    "section": "Summaries for Bivariate Gaussian Covariance",
    "text": "Summaries for Bivariate Gaussian Covariance\nWe consider max-var styled policies, where we pick the evaluation point with the highest variance. In the case where we have a 2D spatial vector field to model and each vector is marginally \\(N_2\\), we try to summarize the evaluation point’s uncertainty using a function of the covariance matrix \\(\\Sigma\\).\n\nEntropy of a Multivariate Gaussian\nFirst, we calculate the entropy of a multivariate Gaussian \\(N_D(\\mu, \\Sigma)\\). We have\n\\[\n\\begin{split}\nH(X) &= - \\mathbb{E}_{x \\sim X} \\left[ \\log p(x) \\right] \\\\\n&= - \\mathbb{E}_{x \\sim X} \\left[- \\frac{D}{2} \\log \\pi - \\frac{1}{2} \\log \\det (\\Sigma) - \\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right] \\\\\n&= \\frac{D}{2} \\log \\pi + \\frac{1}{2} \\log \\det (\\Sigma) + \\frac{1}{2} \\mathbb{E}_{x \\sim X} \\left[ (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right]\n\\end{split}\n\\] where\n\\[\n\\begin{split}\n\\mathbb{E}_{x \\sim X} \\left[ (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right] &= \\mathbb{E}_{x \\sim X} \\left[ \\text{tr} \\left( (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right) \\right] \\\\\n&= \\mathbb{E}_{x \\sim X} \\left[ \\text{tr} \\left( \\Sigma^{-T} (x - \\mu) (x - \\mu)^T \\right) \\right] \\\\\n&= \\text{tr} \\left[ \\Sigma^{-1} \\mathbb{E}_{x \\sim X} \\left[ (x - \\mu) (x - \\mu)^T \\right] \\right] \\\\\n&= \\text{tr} \\left[ \\Sigma^{-1} \\Sigma \\right] = \\text{tr} \\left[ I_D \\right] = D.\n\\end{split}\n\\]\nThus, the differential entropy is\n\\[\nH(X) = \\frac{D}{2} \\log \\pi + \\frac{D}{2} + \\frac{1}{2} \\log \\det (\\Sigma).\n\\]\n\n\nSummary Options\n\nTrace of \\(\\Sigma\\)\n\\[\n\\text{tr}(\\Sigma) = \\Sigma_{11} + \\Sigma_{22}\n\\]\n\nThis captures the sum of the variances in both dimensions, but ignores the correlations.\n\nDeterminant of \\(\\Sigma\\)\n\\[\n\\det(\\Sigma) = \\Sigma_{11} \\Sigma_{22} - \\Sigma_{12} \\Sigma_{21}\n\\]\n\nThis can be interpreted as the “area” of uncertainty in the 2D space. It is equivalent to the entropy when used for comparison. For two covariance matrices with same variances, the determinant will be smaller for the one with higher correlation.\n\nNorm of \\(\\Sigma\\)\n\\[\n\\| \\Sigma \\|_{?}\n\\]\n\nDifferent matrix norms provide different interpretations of overall uncertainty. For example, the Frobenius norm is an element-wise norm that flattens the matrix \\(\\Sigma\\) and compute its \\(L_2\\) norm.\n\n\n\n\nExample\nIn the following graph we show four covariance matrices and their covariance ellipses (horizontal cross-sections of their probability density functions), as well as the values for various summaries.\n\n\n\nAn example of the values of summaries for various covariances."
  },
  {
    "objectID": "posts/2025-02-03-EIG-GP/index.html",
    "href": "posts/2025-02-03-EIG-GP/index.html",
    "title": "Expected Information Gain with Gaussian Process Surrogate Models",
    "section": "",
    "text": "Introducing Expected Information Gain\nIn Bayesian experiment design (Rainforth et al. 2024), a commonly used utility function is the information gain, where we are comparing the entropy of the distributions before and after observing an addition point. Assuming that our existing data set is denoted by \\(\\mathcal{D}\\) and the posterior distribution is \\(p(\\cdot | \\mathcal{D})\\). If we make an observation at \\(x\\) and observe \\(y\\), our new data set will become \\(\\mathcal{D}^+ := \\mathcal{D} \\cup \\{(x, y)\\}\\). This will then correspond to a new posterior \\(p(\\cdot | \\mathcal{D}^+)\\).\nGiven those, the information gain (IG) is given by:\n\\[\nIG(x) = H(p(\\cdot | \\mathcal{D})) - H(p(\\cdot | \\mathcal{D}^+)).\n\\]\nConsider our distribution is a Gaussian process (GP) with mean zero and kernel \\(k\\), and the posterior is the posterior predictive distribution of this GP on some finite set of test points \\(x_*\\) with size \\(m\\). We also assume the current data set \\(\\mathcal{D} := \\{(x_i, y_i)\\}_{i=1}^{n}\\) is of size \\(n\\) and the observations with additive, centered, independent Gaussian noise of variance \\(\\sigma^2\\).\nWe will also use the following notations to denote the various Gram matrices using kernel \\(k\\)\n\n\\(K = k(X,X)\\), size \\(n \\times n\\).\n\\(K_* = k(X, x_*)\\), size \\(n \\times m\\).\n\\(K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\nThe posterior is therefore (see here for a detailed derivation)\n\\[\n\\begin{split}\np(y^* | x^*, \\mathcal{D}, \\sigma^2) &\\sim \\mathcal{N}(\\mu_{y^*|\\mathcal{D}}, \\Sigma_{y^*|\\mathcal{D}}) \\\\\n&\\mu_{y^*|\\mathcal{D}} = K_*^T (K + \\sigma^2 I_n)^{-1} y\\\\\n&\\Sigma_{y^*|\\mathcal{D}} = K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*.\n\\end{split}\n\\]\nAfter adding a new observation at \\(x\\), we will have an updated dataset \\(\\mathcal{D}^+\\) with \\(X^+ = X \\cup \\{x\\}\\) and have an updated posterior using the following Gram matrices\n\n\\(K^+ = k(X^+,X^+)\\), size \\((n+1) \\times (n+1)\\).\n\\(K_*^+ = k(X^+, x_*)\\), size \\((n+1) \\times m\\).\n\\(K_{**}^+ = K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\nSo, the updated posterior’s covariance matrix is\n\\[\n\\Sigma_{y^*|\\mathcal{D}^+} = K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\n\\]\nThus, the information gain can be written as\n\\[\nIG(x) = H(p(\\cdot | \\mathcal{D})) - H(p(\\cdot | \\mathcal{D}^+))\n\\] where using the definition of the entropy of multivariate Gaussian yields\n\\[\n\\begin{split}\nIG(x) &= \\frac{1}{2} \\log \\det \\Sigma_{y^*|\\mathcal{D}} - \\frac{1}{2} \\log \\det \\Sigma_{y^*|\\mathcal{D}^+} \\\\\n&= \\frac{1}{2} \\log \\det \\Big( K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_* \\Big) - \\frac{1}{2} \\log \\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big)\n\\end{split}\n\\] Since \\(IG(x)\\) is independent of \\(y | x\\), the acquisition function expected information gain (EIG) is therefore\n\\[\nEIG(x) = \\mathbb{E}_{y}[IG(x)] = IG(x)\n\\] Furthermore, we can remove several terms when we do \\(\\arg\\max_x\\) for the acquisition function optimisation, and get\n\\[\nEIG(x) = - \\log \\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big).\n\\]\nIn the current setup, the information gain is tractable due to nice properties of multivariate Gaussians and GP regression conjugacies. Albeit tractable, the immediate formulation of the expected information gain has undesirable computational costs which we will elaborate below. After a preliminary attempt to reformulate EIG in order to reduce the computation cost, we will present a different perspective of EIG using mutual information, which enables an EIG formulation with low computational costs.\n\n\nEIG Reformualtion - a first attempt\nWe will consider the naive computation of the above \\(EIG(x)\\) expression. One should note that in the active learning settings, we would often be in the scenarios where \\(m &gt;&gt; n\\). An improved approach of computing the same quantity is presented below, leveraging the matrix determinant lemma.\n\nNaive Implementation\nWe wish to compute\n\\[\nEIG(x) = - \\log\\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big).\n\\]\n\n\n\n\n\n\n\n\nOrder\nExpression\nCost\n\n\n\n\n1\n\\((K^+ + \\delta^2 I_{n+1})^{-1}\\)\n\\(O((n+1)^3)\\)\n\n\n2\n\\((K^+ + \\delta^2 I_{n+1})^{-1} K_*^+\\)\n\\(O((n+1)^2 m)\\)\n\n\n3\n\\(K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1}K_*^+\\)\n\\(O(m (n+1)^2)\\)\n\n\n4\n\\(K^+ - K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1} K_*^+\\)\n\\(O(m^2)\\)\n\n\n5\n\\(-\\log\\det\\big(K_{**}^+ - K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1} K_*^+ \\big)\\)\n\\(O({\\color{red}m^3})\\)\n\n\n\nSo the cost is\n\\[\nO((n+1)^3 + (n+1)^2 m + m^2 (n+1) + m^2 + {\\color{red}m^3}).\n\\] We will need to compute the above quantity \\(m\\) times for comparison \\(\\arg\\max_x\\), thus the full costs is\n\\[\nO((n+1)^3m + (n+1)^2 m^2 + m^3 (n+1) + m^3 + {\\color{red}m^4}).\n\\]\n\n\nNontrivial Implementation\nWe use the matrix determinant identity:\n\\[\n\\det(A + UWV^T) = \\det(A) \\det(W) \\det(W + V^T A^{-1} U)\n\\]\nwhere here\n\n\\(A = K_{**}^+\\)\n\\(U = -K_*^{+T}\\)\n\\(W = (K^+ + \\sigma^2 I_{n+1})^{-1}\\)\n\\(V = K_*^+\\)\n\nThus, we wish to compute\n\\[\nEIG(x) = -\\log \\left[ \\det(K_{**}^+) \\cdot 1/ \\det(K^+ + \\sigma^2 I_{n+1}) \\cdot \\det \\big(K^+ + \\sigma^2 I_{n+1}- K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big) \\right]\n\\]\nSince \\(K_{**}\\) is positive semi-definite, its determinant is always non-negative so we can ignore it in comparisons.\n\n\n\n\n\n\n\n\nOrder\nExpression\nCost\n\n\n\n\n1\n\\(K_{**}^+ + \\sigma^2 I_{n+1}\\)\n\\(O((n+1)^2)\\)\n\n\n2\n\\(\\det(K^+ + \\sigma^2 I_{n+1})\\)\n\\(O((n+1)^3)\\)\n\n\n3\n\\((K_{**}^+)^{-1}\\)\n\\(O(m^3)\\), reusable\n\n\n4\n\\((K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O(m^2(n+1))\\)\n\n\n5\n\\(K_*^+ (K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O(m(n+1)^2)\\)\n\n\n6\n\\((K^+ + \\sigma^2 I_{n+1}) - K_*^+ (K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O((n+1)^2)\\)\n\n\n7\n\\(\\det\\big( (K^+ + \\sigma^2 I_{n+1}) - K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big)\\)\n\\(O((n+1)^3)\\)\n\n\n8\n\\(\\log \\left[ 1/ \\det(K^+ + \\sigma^2 I_{n+1}) \\cdot \\det \\big(K^+ + \\sigma^2 I_{n+1}- K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big) \\right]\\)\n\\(O(1)\\)\n\n\n\nSo the cost is \\[\nO((n+1)^2 + (n+1)^3 + {\\color{blue}m^3} + m^2(n+1) + m(n+1)^2).\n\\]\nWe will need to compute the above quantity \\(m\\) times for comparison \\(\\arg\\max_x\\), thus the full costs is\n\\[\nO((n+1)^2m + (n+1)^3m + {\\color{blue}m^3} + m^3(n+1) + m^2(n+1)^2).\n\\]\n\n\n\nEIG Reformualtion - a second attempt\n\nEIG over \\(n\\) Observations\nInstead of the one-step EIG update (the difference in entropies between the posteriors with and without an additional observation), below we derive the EIG of the entirety of \\(n\\) observations. This quantity could be used as the objective for non-myopic policies, such as the case of deep adaptive designs (Foster et al. 2021).\nConsider we have the prior (a GP with kernel \\(k\\)) \\(p(\\cdot)\\) and we have \\(n\\) observations \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n =: \\{ (\\boldsymbol{x}, \\boldsymbol{y})\\}\\) which yields the posterior \\(p(\\cdot | \\mathcal{D})\\), the information gain quantity of interest would be\n\\[\nIG(\\boldsymbol{x}) = H(p(\\cdot)) - H(p(\\cdot | \\mathcal{D})) = MI(p(\\cdot); \\boldsymbol{y})\n\\] where the last equality follows from the definition of mutual information.\nAgain, if we consider those GPs on a fixed, finite set of test points \\(x_*\\) like before, we would be able to show the following:\n\\[\n\\begin{split}\nIG(\\boldsymbol{x}) &= H(p(\\cdot)) - H(p(\\cdot | \\mathcal{D})) \\\\\n&= \\frac{1}{2} \\log \\det K_{**} - \\frac{1}{2} \\log \\det \\left[ K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*\\right] \\\\\n&= - \\frac{1}{2} \\log \\det \\left[K_{**}^{-1}( K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*)\\right] \\\\\n&= - \\frac{1}{2} \\log \\det \\left[I_m - K_{**}^{-1} K_*^T (K + \\sigma^2 I_n)^{-1} K_*)\\right] \\\\\n\\end{split}\n\\] where, as before, we use the shorthand notations\n\n\\(K = k(X,X)\\), size \\(n \\times n\\).\n\\(K_* = k(X, x_*)\\), size \\(n \\times m\\).\n\\(K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\n\n\nLow Cost EIG Formulation\nThe above computation is at least cubic in \\(m\\) due to the determinant operation. In fact, using the symmetric property of the mutual information, we could obtain a much better expression.\nNote that \\[MI(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X).\\] We first denote the prior as \\(f\\), the observations \\(\\boldsymbol{y_A}\\) at locations \\(\\boldsymbol{x_A}\\) with observational noise \\(\\boldsymbol{\\varepsilon}\\) so \\(\\boldsymbol{y_A} = f(\\boldsymbol{x_A}) + \\boldsymbol{\\varepsilon}\\).\nThe information gain from prior to posterior after observing \\(\\boldsymbol{y_A}\\) can be written as the mutual information\n\\[\nIG(\\boldsymbol{x_A}) = H(f) - H(f |\\boldsymbol{y_A}) = MI(f; \\boldsymbol{y_A}) = H(\\boldsymbol{y_A}) - H(\\boldsymbol{y_A} | f).\n\\]\nNotice that since \\(y_A = f(\\boldsymbol{x_A}) + \\boldsymbol{\\varepsilon}\\), it is a multivariate with covariance \\(K(\\boldsymbol{x_A}, \\boldsymbol{x_A}) + \\sigma^2 I\\). In addition, \\(\\boldsymbol{y_A} | f\\) has covariance being just \\(\\sigma^2 I\\). Therefore, we have\n\\[\n\\begin{split}\nIG(\\boldsymbol{x_A}) &= H(\\boldsymbol{y_A}) - H(\\boldsymbol{y_A} | f) \\\\\n&= \\log \\det (K(\\boldsymbol{x_A}, \\boldsymbol{x_A}) + \\sigma^2 I) - \\log \\det (\\sigma^2 I) \\\\\n&= \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_A}, \\boldsymbol{x_A}))\n\\end{split}\n\\]\nwhich is the expression used in Section 2.2 of Srinivas et al. (2010), and is computationally cheap.\nUsing the same concept, we can rewrite the EIG of posteriors between \\(\\mathcal{D} = \\{ (\\boldsymbol{x_A}, \\boldsymbol{y_A}) \\}\\) and \\(\\mathcal{D}^+ = \\{ (\\boldsymbol{x_B}, \\boldsymbol{y_B}) \\}\\) (i.e. subject to one more observation). We have the information gain\n\\[\n\\begin{split}\nIG(x) &= H(f | \\boldsymbol{y_A}) - H(f | \\boldsymbol{y_B}) \\\\\n&= - H(f) + H(f | \\boldsymbol{y_A}) + H(f) - H(f | \\boldsymbol{y_B}) \\\\\n&= - [H(f) - H(f | \\boldsymbol{y_A})] + [H(f) - H(f | \\boldsymbol{y_B})] \\\\\n&= - [IG(\\boldsymbol{x_A}) ] + [IG(\\boldsymbol{x_B})] \\\\\n&= - \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_A}, \\boldsymbol{x_A})) + \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_B}, \\boldsymbol{x_B})).\n\\end{split}\n\\]\nNotice that the first term is the same when comparing across different \\(x\\), thus can be omitted. This formulation’s cost is therefore\n\\[\nO((n+1)^2 + (n+1)^3)\n\\]\nfor one-time computation and the overall cost for comparison \\(\\arg\\max_x\\) is\n\\[\nO(m(n+1)^2 + m(n+1)^3).\n\\]\nOne should note that similar rewriting of entropy-related objectives using the symmetry of mutual information also exist in the Bayesian optimization literature with the entropy search and the predictive entropy search (e.g. Hernández-Lobato, Hoffman, and Ghahramani (2014)).\n\n\n\n\n\n\nReferences\n\nFoster, Adam, Desi R Ivanova, Ilyas Malik, and Tom Rainforth. 2021. “Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design.” In International Conference on Machine Learning, 3384–95. PMLR.\n\n\nHernández-Lobato, José Miguel, Matthew W Hoffman, and Zoubin Ghahramani. 2014. “Predictive Entropy Search for Efficient Global Optimization of Black-Box Functions.” Advances in Neural Information Processing Systems 27.\n\n\nRainforth, Tom, Adam Foster, Desi R Ivanova, and Freddie Bickford Smith. 2024. “Modern Bayesian Experimental Design.” Statistical Science 39 (1): 100–114.\n\n\nSrinivas, Niranjan, Andreas Krause, Sham Kakade, and Matthias Seeger. 2010. “Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design.” In Proceedings of the 27th International Conference on Machine Learning, 1015–22. Omnipress."
  },
  {
    "objectID": "posts/2025-02-03-EIG-GP/index.html#expected-information-gain",
    "href": "posts/2025-02-03-EIG-GP/index.html#expected-information-gain",
    "title": "Expected Information Gain with Gaussian Process Surrogate Models",
    "section": "",
    "text": "In Bayesian experiment design (Rainforth et al. 2024), a commonly used utility function is the information gain, where we are comparing the entropy of the distributions before and after observing an addition point. Assuming that our existing dataset is denoted by \\(\\mathcal{D}\\) and the posterior distribution is \\(p(\\cdot | \\mathcal{D})\\). If we make an observation at \\(x\\) and observe \\(y\\), our new dataset will become \\(\\mathcal{D}^+ := \\mathcal{D} \\cup \\{(x, y)\\}\\). This will then correspond to a new posterior \\(p(\\cdot | \\mathcal{D}^+)\\).\nGiven those, the information gain (IG) is given by:\n\\[\nIG(x) = H(p(\\cdot | \\mathcal{D})) - H(p(\\cdot | \\mathcal{D}^+)).\n\\]"
  },
  {
    "objectID": "posts/2025-02-03-EIG-GP/index.html#gaussian-process-and-its-entropy",
    "href": "posts/2025-02-03-EIG-GP/index.html#gaussian-process-and-its-entropy",
    "title": "Expected Information Gain with Gaussian Process Surrogate Models",
    "section": "Gaussian Process and Its Entropy",
    "text": "Gaussian Process and Its Entropy\nConsider our distribution is a Gaussian process (GP) with mean zero and kernel \\(k\\), and the posterior is the posterior predictive distribution of this GP on some finite set of test points \\(x_*\\) with size \\(m\\). We also assume the current dataset \\(\\mathcal{D} := \\{(x_i, y_i)\\}_{i=1}^{n}\\) is of size \\(n\\) and the observations with additive, centered, independent Gaussian noise of variance \\(\\sigma^2\\).\nWe will also use the following notations to denote the various Gram matrices using kernel \\(k\\): - \\(K = k(X,X)\\), size \\(n \\times n\\). - \\(K_* = k(X, x_*)\\), size \\(n \\times m\\). - \\(K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\nThe posterior is therefore\n\\[\n\\begin{split}\np(y^* | x^*, \\mathcal{D}, \\sigma^2) &\\sim \\mathcal{N}(\\mu_{y^*|\\mathcal{D}}, \\Sigma_{y^*|\\mathcal{D}}) \\\\\n&\\mu_{y^*|\\mathcal{D}} = K_*^T (K + \\sigma^2 I_n)^{-1} y\\\\\n&\\Sigma_{y^*|\\mathcal{D}} = K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*.\n\\end{split}\n\\]\n\n\nUpdated Posterior\nAfter adding a new observation, the updated posterior is:\n\\[\n\\Sigma_{y^*|\\mathcal{D}^+} =\n\\begin{bmatrix}\nK_{**} & K_*^T (K + \\delta^2 I_n)^{-1} K_* \\\\\n- K_*^T (K + \\delta^2 I_n)^{-1} K_* & K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_*\n\\end{bmatrix}\n\\]\nwhere:\n\nTest points: \\(M\\)\nObserved points: \\(N\\)\n\n\n\n\nInformation Gain Calculation\n\\[\nIG(x) = H(p(\\cdot | \\mathcal{D})) - H(p(\\cdot | \\mathcal{D}^+))\n\\]\nUsing the determinant of the covariance matrices:\n\\[\nIG(x) = \\frac{1}{2} \\log \\det \\Sigma_{y^*|\\mathcal{D}} - \\frac{1}{2} \\log \\det \\Sigma_{y^*|\\mathcal{D}^+}\n\\]\n\\[\n= \\frac{1}{2} \\log \\det \\Big( K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_* \\Big)\n\\]\nSince \\(IG(x)\\) is independent of \\(y | x^*\\), we compute expected IG:\n\\[\nEIG(x) = E_{y_{x^*}}[IG(x)] = IG(x)\n\\]\nSo, we maximize:\n\\[\nEIG(x) = \\det \\Big( K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_* \\Big)\n\\]\nwith:\n\n\\(K_{**}\\) (size \\(m \\times m\\))\n\\(K_*\\) (size \\((n+1) \\times m\\))\n\n\n\n\nCompute Order Analysis\n\n\\((K + \\delta^2 I_n)^{-1}\\) \\(\\quad O((n+1)^3)\\)\n\\((K + \\delta^2 I_n)^{-1} K_*\\) \\(\\quad O((n+1)^2 m)\\)\n\\(K_*^T (K + \\delta^2 I_n)^{-1}\\) \\(\\quad O(m (n+1)^2)\\)\n\\(K_*^T (K + \\delta^2 I_n)^{-1} K_*\\) \\(\\quad O(m^2)\\)\n\\(\\det(K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_*)\\) \\(\\quad O(m^3)\\)\n\nOverall cost:\n\\[\nO((n+1)^3 + (n+1)^2 m + m^2 (n+1) + m^3)\n\\]\n\n\n\nMatrix Determinant Lemma\nWe use the determinant identity:\n\\[\n\\det(A + UWV^T) = \\det(A) \\det(W) \\det(W + V^T A^{-1} U)\n\\]\nwhere:\n\n\\(A = K_{**}\\)\n\\(U = -K_*^T\\)\n\\(W = (K + \\delta^2 I_n)^{-1}\\)\n\\(V = K_*\\)\n\nThus,\n\\[\nEIG(x) = \\det(K_{**}) \\det(K + \\delta^2 I_n)^{-1} \\det(K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_*)\n\\]\nwhich simplifies to:\n\\[\n\\det(K_{**}) / \\det(K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_*)\n\\]\nSince \\(K_{**}\\) is positive semi-definite, its determinant is always non-negative so we can ignore it in comparisons.\n\n\n\nCompute Order (Using Determinant Lemma)\n\n\\(K_{**} + \\delta^2 I_m\\) \\(\\quad O((n+1)^2)\\)\n\\(\\det(K + \\delta^2 I_n)\\) \\(\\quad O((n+1)^3)\\)\n\\((K_{**})^{-1}\\) \\(\\quad O(m^3)\\)\n$K_{**}^{-1} K_*^T $ \\(\\quad O(m^2 (n+1))\\)\n\\(K_{**}^{-1} K_*^T (K_{**}^{-1} K_*^T)^T\\) \\(\\quad O(m (n+1)^2)\\)\n\\((K + \\delta^2 I_n)^{-1} - (K_{**}^{-1} K_*^T (K + \\delta^2 I_n)^{-1} K_* )\\) \\(\\quad O((n+1)^2)\\)\n\\(\\det \\Big( (K_{**} + \\delta^2 I_m) - (K_*^T K_{**}^{-1} K_*^T) \\Big)\\) $O((n+1)^3) $\nFinal determinant computation \\(\\quad O(1)\\)\n\nFinal overall cost:\n\\[\nO((n+1)^3 + (n+1)^2 + M (n+1)^2 + m^2 (n+1) + m^3)\n\\]"
  },
  {
    "objectID": "posts/2025-03-30-temporal-GP-3/index.html",
    "href": "posts/2025-03-30-temporal-GP-3/index.html",
    "title": "Spatial-Temporal GP (3)",
    "section": "",
    "text": "In this blog post, I will describe how one could formulate an one-dimensional temporal Matérn Gaussian process as a stochastic differential equation. This dynamic formulation of a Gaussian process allows one to do regression with linear computational cost.\nThe detailed mathematical derivations are omitted in the blog post, but can be found here. The Python implementation codes can be found here. A large portion of the post is based on Solin (2016) and Sarkka, Solin, and Hartikainen (2013).\n\n\n\nGaussan Process Regression via the SDE Approach\n\n\n\nBasic Gaussian Process Regression\nConsider an one-dimensional, scalar output Gaussian process (GP) \\(f \\sim \\mathcal{GP}(0, k)\\) with zero mean and kernel \\(k\\). This GP \\(f\\) is defined on input space \\(\\mathbb{R}\\) and its output space is \\(\\mathbb{R}\\). To help with the subsequent exposition, it is beneficial to view the input space as a timeline, and the GP models an univariate time series.\n\n\n\nA Draw from a Matérn 3/2 GP.\n\n\nWhen one make observations \\(\\boldsymbol{y} \\in \\mathbb{R}^{n_\\text{obs}}\\) at observation times \\(\\boldsymbol{x} \\in \\mathbb{R}^{n_\\text{obs}}\\), we assume the observations are noisy and follow\n\\[\ny_i = f(x_i) + \\varepsilon_i, \\qquad \\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} N(0, \\sigma_{\\text{obs}}^2), \\qquad \\forall i = 1, 2, \\ldots, n_\\text{obs}\n\\]\nwhich allow conjugacy in regression. We denote the observed data as \\(\\mathcal{D} = \\{\\boldsymbol{x}, \\boldsymbol{y}\\}\\). Following GP regression formula, we have the predictive distribution at new test points \\(X_*\\) as\n\\[\n\\begin{split}\ny_* ~|X_*, \\mathcal{D}, \\sigma_\\text{obs}^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= K_*^T (K + \\sigma_\\text{obs}^2 I_{n_\\text{obs}})^{-1} y,\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma_\\text{obs}^2 I_{n_\\text{obs}})^{-1}K_*.\n\\end{split}\n\\]\nwhere \\(K(\\cdot,\\cdot)\\) is the Gram matrix using the kernel \\(k\\). The computation of predictive distribution is \\(O(n_\\text{obs}^3)\\) using the above formula, since there exists an inversion of \\(n_\\text{obs} \\times n_\\text{obs}\\) matrix.\n\n\nStationary Kernels and Spectral Densities\nA GP is a stationary stochastic process if its kernel \\(k\\) is a stationary kernel, in the sense that the kernel between two points \\(x\\) and \\(x'\\) can be determined solely by their distance, i.e. \n\\[\nk(x, x') = k(r), \\qquad r = \\| x - x' \\|.\n\\]\nTwo commonly used stationary kernels are the radial basis function (RBF) kernel, also known as the squared exponential (SE) kernel, with variance \\(\\sigma^2\\) and lengthscale \\(l\\)\n\\[\nk_\\text{RBF}(x, x') = \\sigma^2 \\exp \\left[ -\\frac{\\| x - x'\\|}{2l^2} \\right]\n\\] and the Matérn kernel with variance \\(\\sigma^2\\), lengthscale \\(l\\), and smoothness \\(\\nu\\)\n\\[\nk_\\text{Matérn} (x,x') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2\\nu} \\frac{\\| x - x'\\|}{l} \\right)^\\nu K_\\nu\\left( \\sqrt{2\\nu} \\frac{\\| x - x'\\|}{l} \\right)\n\\] where \\(\\Gamma\\) is the Gamma function and \\(K_\\nu\\) is the modified Bessel function of the second kind. With Matérn kernels, it is common to consider smoothness parameter \\(\\nu\\) to be half-integers (i.e. \\(\\nu = p + 1/2\\) for \\(p \\in \\mathbb{Z}\\)). In such cases, we have a simpler expression for the kernel, which is given by\n\\[\nk_\\text{Matérn} (x,x') = \\sigma^2 \\exp \\left( - \\sqrt{2p + 1} \\frac{\\|x - x'\\|}{l} \\right) \\frac{p!}{(2p)!} \\sum_{i = 0}^p \\frac{(p+i)!}{i! (p-i)!} \\left( \\frac{2 \\sqrt{2p + 1} \\| x - x'\\|}{l} \\right)^{p - i}.\n\\]\nFor \\(\\nu = 1/2\\) (thus \\(p = 0\\)), we have\n\\[\nk_{\\text{Matérn}-1/2} (x,x') = \\sigma^2 \\exp \\left( - \\frac{\\| x - x'\\|}{l}\\right).\n\\]\nFor \\(\\nu = 3/2\\) (thus \\(p = 1\\)), we have\n\\[\nk_{\\text{Matérn}-3/2} (x,x') = \\sigma^2 \\left( 1 + \\frac{\\sqrt{3}\\|x - x'\\|}{l} \\right) \\exp \\left( - \\frac{\\sqrt{3}\\| x - x'\\|}{l}\\right).\n\\]\nIt can also be shown that \\(k_{\\text{Matérn}-\\nu} \\to k_\\text{SE}\\) as \\(\\nu \\to \\infty\\).\n\n\n\nKernel Function Comparison\n\n\nThe stationarity of these kernels allow us to assess their spectrum using Fourier transform. After standard Fourier transform computations, one can find the following spectral densities\n\\[\n\\begin{split}\nS_\\text{SE}(\\omega) &= 2 \\pi l^2 \\exp(-2\\pi^2 l^2 \\omega^2) \\\\\nS_\\text{Matérn}(\\omega) &= \\frac{\\Gamma(\\nu + 1/2) (2\\nu)^\\nu}{\\sqrt{\\pi} \\Gamma(\\nu) l^{2\\nu}} \\frac{1}{(\\omega^2 + 2\\nu / l^2)^{\\nu + 1/2}} \\\\\nS_{\\text{Matérn}-1/2}(\\omega) &= \\frac{1}{\\pi l} \\frac{1}{\\omega^2 + 1/l^2} \\\\\nS_{\\text{Matérn}-3/2}(\\omega) &= \\frac{2 \\sqrt{3}^3}{\\pi l^3} \\frac{1}{(\\omega^2 + 3/l^2)^2}.\n\\end{split}\n\\]\n\n\n\nSpectral Density Comparison\n\n\nThe SDE formulation we will be presenting below would only allow reformulation of stationary GPs. In particular, we will focus on the Matérn GPs as they are both flexible and commonly used model classes.\n\n\nSDE Formulation\nFirst of all, a Gaussian process is closed under linear operators, i.e. for a linear operator \\(\\mathcal{L}\\) and a Gaussian process \\(f\\), we know that \\(\\mathcal{L} f\\) is still a Gaussian process (Särkkä 2011). Since addition, scalar multiplication, and (partial) differentiation are all linear operators, the solution \\(f\\) of the following equation would be a Gaussian process\n\\[\na_0 f(t) + a_1 \\frac{df(t)}{dt} + a_2 \\frac{d^2 f(t)}{dt^2} + \\cdots + a_m \\frac{d^m f(t)}{dt^m} = w(t)\n\\]\nwhere \\(w(t)\\) is a white noise process with spectral density \\(\\Sigma\\) and is a Gaussian process.\nConsider the random vector \\(\\boldsymbol{f} = [f, f^{(1)}, f^{(2)}, \\ldots, f^{(m)}]^T\\) and the random process \\(\\boldsymbol{w} = [w_1, w_2, \\ldots, w_{m-1}, w]^T\\). We can recover the solution \\(f\\) via \\(f = \\boldsymbol{H} \\boldsymbol{f}\\) where \\(\\boldsymbol{H} = [1, 0, \\ldots, 0]\\) and the white noise process \\(w\\) via \\(w =\\boldsymbol{L} \\boldsymbol{w}\\) where \\(\\boldsymbol{L} = [0, \\ldots, 0, 1]\\). After rearrangements, we can convert the above equation into the following SDE\n\\[\n\\frac{d}{dt} \\boldsymbol{f}(t) = \\boldsymbol{F} \\boldsymbol{f}(t) + \\boldsymbol{L} \\boldsymbol{w}(t)\n\\]\nwhere\n\\[\n\\boldsymbol{F} = \\begin{bmatrix}\n        0 & 1 & 0 & 0 & \\cdots & 0 \\\\\n        0 & 0 & 1 & 0 &\\cdots & 0 \\\\\n        \\vdots & & \\ddots & \\ddots & & \\vdots \\\\\n        0 & &&&  1 & 0 \\\\\n        -a_0 & &\\cdots&\\cdots&  & -a_m \\\\\n    \\end{bmatrix}.\n\\]\nNotice that the above SDE can be solved exactly using integrating factor and Itô lemma, which gives us\n\\[\n\\boxed{\\begin{split}\n\\boldsymbol{f}(t) | \\boldsymbol{f}(t') &\\sim \\boldsymbol{N} \\left(A_t , Q_t \\right) \\\\\nA_t &= \\exp[\\boldsymbol{F}(t-t')]  \\boldsymbol{f}(t') \\\\\nQ_t &= \\int_{t'}^t \\exp[\\boldsymbol{F}(t - s)] \\boldsymbol{L} \\Sigma L^T \\exp[\\boldsymbol{F}^T (t - s)] ds.\n\\end{split}}\n\\]\nFinally, one should find the correct specifications of \\(\\boldsymbol{F}\\) and \\(\\Sigma\\) such that the solution GP of the SDE is the GP of interest. For example, with\n\\[\nF = \\begin{bmatrix} 0 & 1 \\\\ -\\lambda^2 & -2\\lambda \\end{bmatrix}, \\quad\nL = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad\nH = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\qquad \\Sigma  = 4\\lambda^3 \\sigma^2, \\qquad P_\\infty = \\begin{bmatrix}\n    \\sigma^2 & 0 \\\\ 0 & \\lambda^2 \\sigma^2\n\\end{bmatrix}\n\\]\nthe solution \\(f(t) = {H} \\boldsymbol{f}(t)\\) of SDE\n\\[\n\\frac{d}{dt} \\boldsymbol{f}(t) = \\boldsymbol{F} \\boldsymbol{f}(t) + {L} \\cdot \\boldsymbol{w}(t).\n\\]\nis a zero-mean GP with Matérn 3/2 kernel.\n\n\nRegression as Kalman Smoothing\nAssume we have made observations \\(\\boldsymbol{y} \\in \\mathbb{R}^{n_\\text{obs}}\\) at observation times \\(\\boldsymbol{x} \\in \\mathbb{R}^{n_\\text{obs}}\\), we assume the observations are noisy and follow\n\\[\ny_i = f(x_i) + \\varepsilon_i, \\qquad \\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} N(0, \\sigma_{\\text{obs}}^2), \\qquad \\forall i = 1, 2, \\ldots, n_\\text{obs}.\n\\]\nWe can construct the following system\n\\[\n\\begin{cases}\n\\frac{d}{dt} \\boldsymbol{f}(t) &= \\boldsymbol{F} \\boldsymbol{f}(t) + {L} \\cdot \\boldsymbol{w}(t) \\\\\n{y}_i &= \\boldsymbol{H} \\boldsymbol{f}({x}_i) + \\varepsilon_i \\qquad \\forall i = 1, 2, \\ldots, n_\\text{obs}\n\\end{cases}\n\\]\nwhich is a state-space model. The regression task is to find the distribution of \\(\\boldsymbol{f} | \\boldsymbol{y}\\), which is equivalent to applying the Kalman smoothing to the above state-space model.\nWe further assume that the observations are made at regular time intervals with gap \\(\\Delta\\). This makes the state-space model into:\n\\[\n\\begin{aligned}\nf_{k+1} &= \\Phi\\,f_k + e_k,\\quad e_k \\sim \\mathcal{N}(0, Q), \\\\\ny_k &= H\\,f_k + \\epsilon_k,\\quad \\epsilon_k \\sim \\mathcal{N}(0, \\sigma_\\text{obs}^2).\n\\end{aligned}\n\\]\nfor \\(k = 1, 2, \\ldots, n_\\text{obs}\\) with\n\\[\n\\Phi = \\exp[\\boldsymbol{F}\\Delta], \\qquad Q = P_\\infty - \\Phi P_\\infty \\Phi^T.\n\\]\nWe are ready to present the Kalman filter and RTS smoother.\n\nKalman Filter\nThe Kalman filter proceeds in two main steps - propagation and assimilation.\n\nPropagation Step\nPredict the state and covariance at time \\(k+1\\) given the filtered estimates at time \\(k\\): \\[\n\\begin{aligned}\n\\hat{f}_{k+1|k} &= \\Phi\\,\\hat{f}_{k|k}, \\\\\nP_{k+1|k} &= \\Phi\\,P_{k|k}\\,\\Phi^\\top + Q.\n\\end{aligned}\n\\]\n\n\nAssimilation Step\nWhen an observation \\(y_{k+1}\\) is available, update the prediction as follows:\n\nInnovation: \\[\n\\nu_{k+1} = y_{k+1} - H\\,\\hat{f}_{k+1|k}.\n\\]\nInnovation covariance: \\[\nS_{k+1} = H\\,P_{k+1|k}\\,H^\\top + \\sigma_\\text{obs}^2.\n\\]\nKalman gain: \\[\nK_{k+1} = \\frac{P_{k+1|k}\\,H^\\top}{S_{k+1}}.\n\\]\nUpdated state estimate: \\[\n\\hat{f}_{k+1|k+1} = \\hat{f}_{k+1|k} + K_{k+1}\\,\\nu_{k+1}.\n\\]\nUpdated covariance: \\[\nP_{k+1|k+1} = P_{k+1|k} - K_{k+1}\\,H\\,P_{k+1|k}.\n\\]\n\nIf no observation is available at a given time step, then the predicted state and covariance are carried forward:\n\\[\n\\hat{f}_{k+1|k+1} = \\hat{f}_{k+1|k}, \\quad P_{k+1|k+1} = P_{k+1|k}.\n\\]\nAdditionally, the log-likelihood contribution from the \\((k+1)\\)-th observation is computed as:\n\\[\n\\log p(y_{k+1} \\mid \\text{past}) = -\\frac{1}{2}\\left[\\log(2\\pi) + \\log(S_{k+1}) + \\frac{\\nu_{k+1}^2}{S_{k+1}}\\right].\n\\]\n\n\n\nRTS Smoother\nAfter running the forward Kalman filter, the Rauch–Tung–Striebel (RTS) smoother refines the state estimates by incorporating future observations. For \\(k = n_\\text{obs}-1, n_\\text{obs}-2, \\dots, 1\\):\n\nSmoothing gain: \\[\nC_k = P_{k|k}\\,\\Phi^\\top\\,(P_{k+1|k})^{-1}.\n\\]\nSmoothed state: \\[\n\\hat{f}_{k|n_\\text{obs}} = \\hat{f}_{k|k} + C_k\\left(\\hat{f}_{k+1|n_\\text{obs}} - \\hat{f}_{k+1|k}\\right).\n\\]\nSmoothed covariance: \\[\nP_{k|n_\\text{obs}} = P_{k|k} + C_k\\left(P_{k+1|n_\\text{obs}} - P_{k+1|k}\\right)C_k^\\top.\n\\]\n\n\n\n\nGaussan Process Regression via the SDE Approach\n\n\n\n\n\nComparison and Implementation Details\nThe computational costs of GP regression via the vanilla approach is cubic, i.e. \\(O(n_\\text{obs}^3)\\), whereas the SDE approach is linear, i.e. \\(O(n_\\text{obs})\\).\n\n\n\nComputational Cost Comparison of Gaussian Process Regression - Conjugacy v.s. Kalman\n\n\nBoth approaches are in fact equivalent, so the computational gain of the SDE approach has no hidden costs.\n\n\n\nGaussian Process Regression Comparison - COnjugacy v.s. Kalman\n\n\nFinally, one could do likelihood training with the SDE approach which gives maximum likelihood estimates of the hyperparameters of the prior distribution (thus we are doing empirical Bayes instead of standard Bayes).\nSome remarks on implementation. The plots above are all using a more granular time grid than the observation time grid, as can be observed from the smooth posterior mean. This means, we are filtering at times where there are no observations (so only propagate, not assimilate) and then correct them in the filtering step. This will bump up the computational costs (linearly).\nIn practice, if prediction at future time is the only downstream task of GP regression, then one could simply do filtering till the last observation time and not do any smoothing. This would drastically reduce the computational cost as we are doing updating at observation times (rather than the more granular regression time grid) and can skip the smoothing.\n\n\n\n\n\nReferences\n\nSarkka, Simo, Arno Solin, and Jouni Hartikainen. 2013. “Spatiotemporal Learning via Infinite-Dimensional Bayesian Filtering and Smoothing: A Look at Gaussian Process Regression Through Kalman Filtering.” IEEE Signal Processing Magazine 30 (4): 51–61.\n\n\nSärkkä, Simo. 2011. “Linear Operators and Stochastic Partial Differential Equations in Gaussian Process Regression.” In Artificial Neural Networks and Machine Learning–ICANN 2011: 21st International Conference on Artificial Neural Networks, Espoo, Finland, June 14-17, 2011, Proceedings, Part II 21, 151–58. Springer.\n\n\nSolin, Arno. 2016. “Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression.”"
  },
  {
    "objectID": "posts/2025-02-07-EIG-GP/index.html",
    "href": "posts/2025-02-07-EIG-GP/index.html",
    "title": "Expected Information Gain with Gaussian Process Surrogate Models",
    "section": "",
    "text": "Dennis Lindley; Lindley (1956)\n\n\n\nIntroducing Expected Information Gain\nIn Bayesian experiment design (Rainforth et al. 2024), a commonly used utility function is the information gain, where we are comparing the entropy of the distributions before and after observing an addition point. Assuming that our existing data set is denoted by \\(\\mathcal{D}\\) and the posterior distribution is \\(p(\\cdot | \\mathcal{D})\\). If we make an observation at \\(x\\) and observe \\(y\\), our new data set will become \\(\\mathcal{D}^+ := \\mathcal{D} \\cup \\{(x, y)\\}\\). This will then correspond to a new posterior \\(p(\\cdot | \\mathcal{D}^+)\\).\nGiven those, the information gain (IG) is given by:\n\\[\nIG(x) = H(p(\\cdot | \\mathcal{D})) - H(p(\\cdot | \\mathcal{D}^+)).\n\\]\nConsider our distribution is a Gaussian process (GP) with mean zero and kernel \\(k\\), and the posterior is the posterior predictive distribution of this GP on some finite set of test points \\(x_*\\) with size \\(m\\). We also assume the current data set \\(\\mathcal{D} := \\{(x_i, y_i)\\}_{i=1}^{n}\\) is of size \\(n\\) and the observations with additive, centered, independent Gaussian noise of variance \\(\\sigma^2\\).\nWe will also use the following notations to denote the various Gram matrices using kernel \\(k\\)\n\n\\(K = k(X,X)\\), size \\(n \\times n\\).\n\\(K_* = k(X, x_*)\\), size \\(n \\times m\\).\n\\(K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\nThe posterior is therefore (see here for a detailed derivation)\n\\[\n\\begin{split}\np(y^* | x^*, \\mathcal{D}, \\sigma^2) &\\sim \\mathcal{N}(\\mu_{y^*|\\mathcal{D}}, \\Sigma_{y^*|\\mathcal{D}}) \\\\\n&\\mu_{y^*|\\mathcal{D}} = K_*^T (K + \\sigma^2 I_n)^{-1} y\\\\\n&\\Sigma_{y^*|\\mathcal{D}} = K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*.\n\\end{split}\n\\]\nAfter adding a new observation at \\(x\\), we will have an updated dataset \\(\\mathcal{D}^+\\) with \\(X^+ = X \\cup \\{x\\}\\) and have an updated posterior using the following Gram matrices\n\n\\(K^+ = k(X^+,X^+)\\), size \\((n+1) \\times (n+1)\\).\n\\(K_*^+ = k(X^+, x_*)\\), size \\((n+1) \\times m\\).\n\\(K_{**}^+ = K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\nSo, the updated posterior’s covariance matrix is\n\\[\n\\Sigma_{y^*|\\mathcal{D}^+} = K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\n\\]\nThus, the information gain can be written as\n\\[\nIG(x) = H(p(\\cdot | \\mathcal{D})) - H(p(\\cdot | \\mathcal{D}^+))\n\\] where using the definition of the entropy of multivariate Gaussian yields\n\\[\n\\begin{split}\nIG(x) &= \\frac{1}{2} \\log \\det \\Sigma_{y^*|\\mathcal{D}} - \\frac{1}{2} \\log \\det \\Sigma_{y^*|\\mathcal{D}^+} \\\\\n&= \\frac{1}{2} \\log \\det \\Big( K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_* \\Big) - \\frac{1}{2} \\log \\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big)\n\\end{split}\n\\] Since \\(IG(x)\\) is independent of \\(y | x\\), the acquisition function expected information gain (EIG) is therefore\n\\[\nEIG(x) = \\mathbb{E}_{y}[IG(x)] = IG(x)\n\\] Furthermore, we can remove several terms when we do \\(\\arg\\max_x\\) for the acquisition function optimisation, and get\n\\[\nEIG(x) = - \\log \\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big).\n\\]\nIn the current setup, the information gain is tractable due to nice properties of multivariate Gaussians and GP regression conjugacies. Albeit tractable, the immediate formulation of the expected information gain has undesirable computational costs which we will elaborate below. After a preliminary attempt to reformulate EIG in order to reduce the computation cost, we will present a different perspective of EIG using mutual information, which enables an EIG formulation with low computational costs.\n\n\nEIG Reformualtion - a first attempt\nWe will consider the naive computation of the above \\(EIG(x)\\) expression. One should note that in the active learning settings, we would often be in the scenarios where \\(m &gt;&gt; n\\). An improved approach of computing the same quantity is presented below, leveraging the matrix determinant lemma.\n\nNaive Implementation\nWe wish to compute\n\\[\nEIG(x) = - \\log\\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big).\n\\]\n\n\n\n\n\n\n\n\nOrder\nExpression\nCost\n\n\n\n\n1\n\\((K^+ + \\delta^2 I_{n+1})^{-1}\\)\n\\(O((n+1)^3)\\)\n\n\n2\n\\((K^+ + \\delta^2 I_{n+1})^{-1} K_*^+\\)\n\\(O((n+1)^2 m)\\)\n\n\n3\n\\(K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1}K_*^+\\)\n\\(O(m (n+1)^2)\\)\n\n\n4\n\\(K^+ - K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1} K_*^+\\)\n\\(O(m^2)\\)\n\n\n5\n\\(-\\log\\det\\big(K_{**}^+ - K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1} K_*^+ \\big)\\)\n\\(O({\\color{red}m^3})\\)\n\n\n\nSo the cost is\n\\[\nO((n+1)^3 + (n+1)^2 m + m^2 (n+1) + m^2 + {\\color{red}m^3}).\n\\] We will need to compute the above quantity \\(m\\) times for comparison \\(\\arg\\max_x\\), thus the full costs is\n\\[\nO((n+1)^3m + (n+1)^2 m^2 + m^3 (n+1) + m^3 + {\\color{red}m^4}).\n\\]\n\n\nNontrivial Implementation\nWe use the matrix determinant identity:\n\\[\n\\det(A + UWV^T) = \\det(A) \\det(W) \\det(W + V^T A^{-1} U)\n\\]\nwhere here\n\n\\(A = K_{**}^+\\)\n\\(U = -K_*^{+T}\\)\n\\(W = (K^+ + \\sigma^2 I_{n+1})^{-1}\\)\n\\(V = K_*^+\\)\n\nThus, we wish to compute\n\\[\nEIG(x) = -\\log \\left[ \\det(K_{**}^+) \\cdot 1/ \\det(K^+ + \\sigma^2 I_{n+1}) \\cdot \\det \\big(K^+ + \\sigma^2 I_{n+1}- K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big) \\right]\n\\]\nSince \\(K_{**}\\) is positive semi-definite, its determinant is always non-negative so we can ignore it in comparisons.\n\n\n\n\n\n\n\n\nOrder\nExpression\nCost\n\n\n\n\n1\n\\(K_{**}^+ + \\sigma^2 I_{n+1}\\)\n\\(O((n+1)^2)\\)\n\n\n2\n\\(\\det(K^+ + \\sigma^2 I_{n+1})\\)\n\\(O((n+1)^3)\\)\n\n\n3\n\\((K_{**}^+)^{-1}\\)\n\\(O(m^3)\\), reusable\n\n\n4\n\\((K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O(m^2(n+1))\\)\n\n\n5\n\\(K_*^+ (K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O(m(n+1)^2)\\)\n\n\n6\n\\((K^+ + \\sigma^2 I_{n+1}) - K_*^+ (K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O((n+1)^2)\\)\n\n\n7\n\\(\\det\\big( (K^+ + \\sigma^2 I_{n+1}) - K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big)\\)\n\\(O((n+1)^3)\\)\n\n\n8\n\\(\\log \\left[ 1/ \\det(K^+ + \\sigma^2 I_{n+1}) \\cdot \\det \\big(K^+ + \\sigma^2 I_{n+1}- K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big) \\right]\\)\n\\(O(1)\\)\n\n\n\nSo the cost is \\[\nO((n+1)^2 + (n+1)^3 + {\\color{blue}m^3} + m^2(n+1) + m(n+1)^2).\n\\]\nWe will need to compute the above quantity \\(m\\) times for comparison \\(\\arg\\max_x\\), thus the full costs is\n\\[\nO((n+1)^2m + (n+1)^3m + {\\color{blue}m^3} + m^3(n+1) + m^2(n+1)^2).\n\\]\n\n\n\nEIG Reformualtion - a second attempt\n\nEIG over \\(n\\) Observations\nInstead of the one-step EIG update (the difference in entropies between the posteriors with and without an additional observation), below we derive the EIG of the entirety of \\(n\\) observations. This quantity could be used as the objective for non-myopic policies, such as the case of deep adaptive designs (Foster et al. 2021).\nConsider we have the prior (a GP with kernel \\(k\\)) \\(p(\\cdot)\\) and we have \\(n\\) observations \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n =: \\{ (\\boldsymbol{x}, \\boldsymbol{y})\\}\\) which yields the posterior \\(p(\\cdot | \\mathcal{D})\\), the information gain quantity of interest would be\n\\[\nIG(\\boldsymbol{x}) = H(p(\\cdot)) - H(p(\\cdot | \\mathcal{D})) = MI(p(\\cdot); \\boldsymbol{y})\n\\] where the last equality follows from the definition of mutual information.\nAgain, if we consider those GPs on a fixed, finite set of test points \\(x_*\\) like before, we would be able to show the following:\n\\[\n\\begin{split}\nIG(\\boldsymbol{x}) &= H(p(\\cdot)) - H(p(\\cdot | \\mathcal{D})) \\\\\n&= \\frac{1}{2} \\log \\det K_{**} - \\frac{1}{2} \\log \\det \\left[ K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*\\right] \\\\\n&= - \\frac{1}{2} \\log \\det \\left[K_{**}^{-1}( K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*)\\right] \\\\\n&= - \\frac{1}{2} \\log \\det \\left[I_m - K_{**}^{-1} K_*^T (K + \\sigma^2 I_n)^{-1} K_*)\\right] \\\\\n\\end{split}\n\\] where, as before, we use the shorthand notations\n\n\\(K = k(X,X)\\), size \\(n \\times n\\).\n\\(K_* = k(X, x_*)\\), size \\(n \\times m\\).\n\\(K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\n\n\nLow Cost EIG Formulation\nThe above computation is at least cubic in \\(m\\) due to the determinant operation. In fact, using the symmetric property of the mutual information, we could obtain a much better expression.\nNote that \\[MI(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X).\\] We first denote the prior as \\(f\\), the observations \\(\\boldsymbol{y_A}\\) at locations \\(\\boldsymbol{x_A}\\) with observational noise \\(\\boldsymbol{\\varepsilon}\\) so \\(\\boldsymbol{y_A} = f(\\boldsymbol{x_A}) + \\boldsymbol{\\varepsilon}\\).\nThe information gain from prior to posterior after observing \\(\\boldsymbol{y_A}\\) can be written as the mutual information\n\\[\nIG(\\boldsymbol{x_A}) = H(f) - H(f |\\boldsymbol{y_A}) = MI(f; \\boldsymbol{y_A}) = H(\\boldsymbol{y_A}) - H(\\boldsymbol{y_A} | f).\n\\]\nNotice that since \\(y_A = f(\\boldsymbol{x_A}) + \\boldsymbol{\\varepsilon}\\), it is a multivariate with covariance \\(K(\\boldsymbol{x_A}, \\boldsymbol{x_A}) + \\sigma^2 I\\). In addition, \\(\\boldsymbol{y_A} | f\\) has covariance being just \\(\\sigma^2 I\\). Therefore, we have\n\\[\n\\begin{split}\nIG(\\boldsymbol{x_A}) &= H(\\boldsymbol{y_A}) - H(\\boldsymbol{y_A} | f) \\\\\n&= \\log \\det (K(\\boldsymbol{x_A}, \\boldsymbol{x_A}) + \\sigma^2 I) - \\log \\det (\\sigma^2 I) \\\\\n&= \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_A}, \\boldsymbol{x_A}))\n\\end{split}\n\\]\nwhich is the expression used in Section 2.2 of Srinivas et al. (2010), and is computationally cheap.\nUsing the same concept, we can rewrite the EIG of posteriors between \\(\\mathcal{D} = \\{ (\\boldsymbol{x_A}, \\boldsymbol{y_A}) \\}\\) and \\(\\mathcal{D}^+ = \\{ (\\boldsymbol{x_B}, \\boldsymbol{y_B}) \\}\\) (i.e. subject to one more observation). We have the information gain\n\\[\n\\begin{split}\nIG(x) &= H(f | \\boldsymbol{y_A}) - H(f | \\boldsymbol{y_B}) \\\\\n&= - H(f) + H(f | \\boldsymbol{y_A}) + H(f) - H(f | \\boldsymbol{y_B}) \\\\\n&= - [H(f) - H(f | \\boldsymbol{y_A})] + [H(f) - H(f | \\boldsymbol{y_B})] \\\\\n&= - [IG(\\boldsymbol{x_A}) ] + [IG(\\boldsymbol{x_B})] \\\\\n&= - \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_A}, \\boldsymbol{x_A})) + \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_B}, \\boldsymbol{x_B})).\n\\end{split}\n\\]\nNotice that the first term is the same when comparing across different \\(x\\), thus can be omitted. This formulation’s cost is therefore\n\\[\nO((n+1)^2 + (n+1)^3)\n\\]\nfor one-time computation and the overall cost for comparison \\(\\arg\\max_x\\) is\n\\[\nO(m(n+1)^2 + m(n+1)^3).\n\\]\nOne should note that similar rewriting of entropy-related objectives using the symmetry of mutual information also exist in the Bayesian optimization literature with the entropy search and the predictive entropy search (e.g. Hernández-Lobato, Hoffman, and Ghahramani (2014)).\n\n\n\n\n\n\nReferences\n\nFoster, Adam, Desi R Ivanova, Ilyas Malik, and Tom Rainforth. 2021. “Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design.” In International Conference on Machine Learning, 3384–95. PMLR.\n\n\nHernández-Lobato, José Miguel, Matthew W Hoffman, and Zoubin Ghahramani. 2014. “Predictive Entropy Search for Efficient Global Optimization of Black-Box Functions.” Advances in Neural Information Processing Systems 27.\n\n\nLindley, Dennis V. 1956. “On a Measure of the Information Provided by an Experiment.” The Annals of Mathematical Statistics 27 (4): 986–1005.\n\n\nRainforth, Tom, Adam Foster, Desi R Ivanova, and Freddie Bickford Smith. 2024. “Modern Bayesian Experimental Design.” Statistical Science 39 (1): 100–114.\n\n\nSrinivas, Niranjan, Andreas Krause, Sham Kakade, and Matthias Seeger. 2010. “Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design.” In Proceedings of the 27th International Conference on Machine Learning, 1015–22. Omnipress."
  }
]