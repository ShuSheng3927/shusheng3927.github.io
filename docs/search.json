[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Lancaster CSML\nLancaster AI Reading Group\nUCL Fundamentals of Statistical Machine Learning\nGP Seminar Series\n\n\n\n\n\nProb_AI Hub\nSTOR-i\nTIDE"
  },
  {
    "objectID": "resources.html#academic-groups-and-organisations",
    "href": "resources.html#academic-groups-and-organisations",
    "title": "Resources",
    "section": "",
    "text": "Lancaster CSML\nLancaster AI Reading Group\nUCL Fundamentals of Statistical Machine Learning\nGP Seminar Series\n\n\n\n\n\nProb_AI Hub\nSTOR-i\nTIDE"
  },
  {
    "objectID": "resources.html#links",
    "href": "resources.html#links",
    "title": "Resources",
    "section": "Links",
    "text": "Links\n\nLaTeX Tools\n\nBibTeX Tidy, tidy up .bib files\nLaTeX Table Generator, make a table as if you are in excel then convert to LaTeX codes\nQuiver, draw commuative diagrams using drags and pulls\n\n\n\nStatistics / ML\n\nMCMC Interactive Gallery, live demo of common MCMC algorithms on interesting targets\nGP Interactive Gallery, interactive visualisation of GP, very neat!\n\n\n\nHobbies\n\nWikiArt, online archive for art work\nXu Bing 徐冰, contemporary artist. Square Word Calligraphy, Book from the Sky, Background Story, Phoenix\nZao Wou-Ki 趙無極, painter. Le Soleil Rouge.\nLucien Freud, painter. portraits, self portraits\nFrancis Bacon, painter. Popes, Three Studies for Figures at the Base of a Crucifixion, self portraits\nEdward Hopper, painter. House by the railroad, office at night\nHaruki Murakami 村上 春樹, novelist. Hear the Wind Sing, Dance Dance Dance, The Wind-Up Bird Chronicle, After Dark, Men Without Women, What I Talk About When I Talk About Running, Novelist as a Vocation\nAng Lee 李安, director. Father Trilogy\nI. M. Pei 貝聿銘, architect. Pyramid, Suzhou Museum, Bank of China Tower\nXiang Biao 项飙, sociologist. Global Body Shopping, Self as Methods\nChen Jia-Ying 陈嘉映, philosopher. 哲学·科学·常识, 何为良好生活, 走出唯一真理观"
  },
  {
    "objectID": "posts/2025-07-30-brownian-path-upscale/index.html",
    "href": "posts/2025-07-30-brownian-path-upscale/index.html",
    "title": "Upscale a Brownian Motion Path",
    "section": "",
    "text": "Consider a fixed Brownian motion path \\(\\{W_t\\}_{t \\ge 0}\\) that we have made observations \\(\\{W_n\\}_n\\) at times \\(\\{t_n\\}_n\\), we wish to upscale the observed path to more timestamps \\(\\{s_m\\}_m\\) while making sure that it is the same Brownian motion path that we are simulating from.\nBy definition, a Brownian motion has Gaussian increments \\(W_{t+u} - W_t \\sim N(0, u)\\) and the increments are independent of past values \\(W_s\\) for \\(s &lt; t\\). It is straightforward also to view a Brownian motion as a Gaussian process (GP) with mean zero and kernel \\(k_\\text{BM}(W_s, W_t) = \\min\\{s, t\\}\\).\n\n\n\nSampled Brownian Motion Paths\n\n\nLeveraging this GP interpretation, the task of “upscaling” or “extrapolating” given observations of a Brownian path \\(\\{W_n\\}_n\\) to additional timestamps \\(\\{s_m\\}_m\\) would be equivalent to finding the posterior predictive distribution at the additional timestamps \\(\\{s_m\\}_m\\) where we condition on exact observations \\(\\{W_n\\}_n\\). In particular, to find \\(\\{W_{s_m}\\}_m\\), we can simply find the posterior predictive, then sample from it.\n\n\n\nUpscale Brownian Motion Paths\n\n\nTo be fair, this is quite inefficient and I am not sure if there would be any use case for this particular way of upscaling Brownian paths. My original motivation was to simulate coupled Brownian paths for multilevel Monte Carlo layers, but for in such cases we could simply sample a Brownian path with the two timestamps aggregated i.e. \\(\\{t_n\\}_n \\cup \\{s_m\\}_m\\) and pick out the needed portions – the order won’t matter there. Doing this GP posterior predictive approach is much more computationally costly for obvious GP reasons, but oh well, thought this at least feels cool.\nThe Python code is here."
  },
  {
    "objectID": "posts/2025-07-01-bayesian-data-assimilation/index.html",
    "href": "posts/2025-07-01-bayesian-data-assimilation/index.html",
    "title": "Toy Bayesian Data Assimilation",
    "section": "",
    "text": "We do Bayesian data assimilation for a deterministic ODE model using its noisy observations. The Python codes used are here."
  },
  {
    "objectID": "posts/2025-07-01-bayesian-data-assimilation/index.html#model-and-observations",
    "href": "posts/2025-07-01-bayesian-data-assimilation/index.html#model-and-observations",
    "title": "Toy Bayesian Data Assimilation",
    "section": "Model and Observations",
    "text": "Model and Observations\nThe ODE of interest is defined as:\n\\[\n\\frac{dx}{dt} = \\exp(-l t) \\cos(\\alpha t)\n\\]\nwhere \\(l\\) and \\(\\alpha\\) are unknown parameters to be inferred. The system is numerically integrated using the Euler method with an initial condition \\(x(0) = x_0\\) over the time interval \\([0, 5]\\) with time step \\(\\Delta t = 0.01\\).\nA synthetic trajectory is generated using the true parameter values: \\(\\alpha = 5, l = 0.5, x_0 = 1.0\\). Noisy observations are sampled at 100 randomly chosen time points \\(\\{t_i\\}_{i=1}^{100}\\) following:\n\\[\ny_i = x(t_i) + \\epsilon_i, \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nwith observation noise standard deviation \\(\\sigma = 0.05\\). The dynamics and observations are shown in Figure below.\n\n\n\nDynamics of \\(dx/dt = \\exp(-l t) \\cos(\\alpha t)\\) with noisy observations."
  },
  {
    "objectID": "posts/2025-07-01-bayesian-data-assimilation/index.html#bayesian-inference-setup",
    "href": "posts/2025-07-01-bayesian-data-assimilation/index.html#bayesian-inference-setup",
    "title": "Toy Bayesian Data Assimilation",
    "section": "Bayesian Inference Setup",
    "text": "Bayesian Inference Setup\nWe assume the observation noise standard deviation is known, and wish to conduct Bayesian inference on the ODE hyperparameters and initial condition \\(\\theta = (\\alpha, l, x_0)\\). We set a flat prior for all the parameters. The (unnormalised) posterior distribution of the parameters of interest \\(\\theta\\) is thus given by the likelihood.\nSince we assume the dynamics are deterministic, all the stochasticities are captured in the observation noise. Here, the log-likelihood function is constructed by simulating the system forward (via Euler’s method with stepsize $0.01$) using candidate parameters and comparing the predicted states to observations, thus:\n\\[\n\\log L(\\theta) = \\log p(y \\mid \\theta) = \\sum_{i=1}^{N} \\log \\phi\\left( \\frac{y_i - x_\\theta(t_i)}{\\sigma}\\right)\n\\]\nwhere \\(x_\\theta(t_i)\\) denotes the simulated trajectory’s value at time \\(t_i\\) under parameters \\(\\theta\\) and \\(\\phi\\) is the density function of a standard normal distribution."
  },
  {
    "objectID": "posts/2025-07-01-bayesian-data-assimilation/index.html#mcmc-results",
    "href": "posts/2025-07-01-bayesian-data-assimilation/index.html#mcmc-results",
    "title": "Toy Bayesian Data Assimilation",
    "section": "MCMC Results",
    "text": "MCMC Results\nWe run MCMC on the constructed posterior using Python’s BlackJax package (Cabezas et al. 2024). I first consider a random walk Metropolis. After trying to hand-tune the stepsize (for each parameter) and getting frustrated, I used an adaptive version of RWM with Robbins-Monro adaptation (Andrieu and Thoms 2008) targeting the optimal acceptance rate (around 0.3, given the low posterior dimension (Roberts and Rosenthal 2001)).\n\n\n\nRWM with Robbins-Monro adaptation. (Top) Trace plots for sampled parameters with true values shown as red dashed lines. (Bottom) Histogram for the samples with true values shown as red dashed lines.\n\n\nI initialise the chain at \\((\\alpha, l, x_0) = (4,1,0.5)\\) so they are moderately close to the truth, yet some good MCMC movement is needed. For the chain, I first run 20,000 steps with adaptation as burn-in and warm-up, then use the learned stepsize to run another 20,000 steps without adaptation to obtain samples from the posterior. The resulting trajectories (burn-in removed) are shown in Figure above. The assimilated dynamics with 90% credible interval are shown below, and the result looks great. The assimilated dynamic trajectories are simulated using 1000 sets of parameter values, bootstrapped from the joint posterior samples, which are then averaged and quantiled to produce the mean and credible interval.\n\n\n\nAssimilated dynamics with 90% credible interval.\n\n\n\nNUTS\nI initially tried NUTS but it did not work. Somehow, I tried it again, with the help of Thomas Newman, and it worked! For 1000 steps of warmup and 1000 further steps, we have the following results, which is visually indistinguishable from that of the RWM – seems like it works correctly too.\n\n\n\nNUTS. (Top) Trace plots for sampled parameters with true values shown as red dashed lines. (Bottom) Histogram for the samples with true values shown as red dashed lines."
  },
  {
    "objectID": "posts/2025-03-30-temporal-GP-3/index.html",
    "href": "posts/2025-03-30-temporal-GP-3/index.html",
    "title": "Spatial-Temporal GP (3)",
    "section": "",
    "text": "In this blog post, I will describe how one could formulate an one-dimensional temporal Matérn Gaussian process as a stochastic differential equation. This dynamic formulation of a Gaussian process allows one to do regression with linear computational cost.\nThe detailed mathematical derivations are omitted in the blog post, but can be found here. The Python implementation codes can be found here. A large portion of the post is based on Solin (2016) and Sarkka, Solin, and Hartikainen (2013).\n\n\n\nGaussan Process Regression via the SDE Approach\n\n\n\nBasic Gaussian Process Regression\nConsider an one-dimensional, scalar output Gaussian process (GP) \\(f \\sim \\mathcal{GP}(0, k)\\) with zero mean and kernel \\(k\\). This GP \\(f\\) is defined on input space \\(\\mathbb{R}\\) and its output space is \\(\\mathbb{R}\\). To help with the subsequent exposition, it is beneficial to view the input space as a timeline, and the GP models an univariate time series.\n\n\n\nA Draw from a Matérn 3/2 GP.\n\n\nWhen one make observations \\(\\boldsymbol{y} \\in \\mathbb{R}^{n_\\text{obs}}\\) at observation times \\(\\boldsymbol{x} \\in \\mathbb{R}^{n_\\text{obs}}\\), we assume the observations are noisy and follow\n\\[\ny_i = f(x_i) + \\varepsilon_i, \\qquad \\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} N(0, \\sigma_{\\text{obs}}^2), \\qquad \\forall i = 1, 2, \\ldots, n_\\text{obs}\n\\]\nwhich allow conjugacy in regression. We denote the observed data as \\(\\mathcal{D} = \\{\\boldsymbol{x}, \\boldsymbol{y}\\}\\). Following GP regression formula, we have the predictive distribution at new test points \\(X_*\\) as\n\\[\n\\begin{split}\ny_* ~|X_*, \\mathcal{D}, \\sigma_\\text{obs}^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= K_*^T (K + \\sigma_\\text{obs}^2 I_{n_\\text{obs}})^{-1} y,\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma_\\text{obs}^2 I_{n_\\text{obs}})^{-1}K_*.\n\\end{split}\n\\]\nwhere \\(K(\\cdot,\\cdot)\\) is the Gram matrix using the kernel \\(k\\). The computation of predictive distribution is \\(O(n_\\text{obs}^3)\\) using the above formula, since there exists an inversion of \\(n_\\text{obs} \\times n_\\text{obs}\\) matrix.\n\n\nStationary Kernels and Spectral Densities\nA GP is a stationary stochastic process if its kernel \\(k\\) is a stationary kernel, in the sense that the kernel between two points \\(x\\) and \\(x'\\) can be determined solely by their distance, i.e. \n\\[\nk(x, x') = k(r), \\qquad r = \\| x - x' \\|.\n\\]\nTwo commonly used stationary kernels are the radial basis function (RBF) kernel, also known as the squared exponential (SE) kernel, with variance \\(\\sigma^2\\) and lengthscale \\(l\\)\n\\[\nk_\\text{RBF}(x, x') = \\sigma^2 \\exp \\left[ -\\frac{\\| x - x'\\|}{2l^2} \\right]\n\\] and the Matérn kernel with variance \\(\\sigma^2\\), lengthscale \\(l\\), and smoothness \\(\\nu\\)\n\\[\nk_\\text{Matérn} (x,x') = \\sigma^2 \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2\\nu} \\frac{\\| x - x'\\|}{l} \\right)^\\nu K_\\nu\\left( \\sqrt{2\\nu} \\frac{\\| x - x'\\|}{l} \\right)\n\\] where \\(\\Gamma\\) is the Gamma function and \\(K_\\nu\\) is the modified Bessel function of the second kind. With Matérn kernels, it is common to consider smoothness parameter \\(\\nu\\) to be half-integers (i.e. \\(\\nu = p + 1/2\\) for \\(p \\in \\mathbb{Z}\\)). In such cases, we have a simpler expression for the kernel, which is given by\n\\[\nk_\\text{Matérn} (x,x') = \\sigma^2 \\exp \\left( - \\sqrt{2p + 1} \\frac{\\|x - x'\\|}{l} \\right) \\frac{p!}{(2p)!} \\sum_{i = 0}^p \\frac{(p+i)!}{i! (p-i)!} \\left( \\frac{2 \\sqrt{2p + 1} \\| x - x'\\|}{l} \\right)^{p - i}.\n\\]\nFor \\(\\nu = 1/2\\) (thus \\(p = 0\\)), we have\n\\[\nk_{\\text{Matérn}-1/2} (x,x') = \\sigma^2 \\exp \\left( - \\frac{\\| x - x'\\|}{l}\\right).\n\\]\nFor \\(\\nu = 3/2\\) (thus \\(p = 1\\)), we have\n\\[\nk_{\\text{Matérn}-3/2} (x,x') = \\sigma^2 \\left( 1 + \\frac{\\sqrt{3}\\|x - x'\\|}{l} \\right) \\exp \\left( - \\frac{\\sqrt{3}\\| x - x'\\|}{l}\\right).\n\\]\nIt can also be shown that \\(k_{\\text{Matérn}-\\nu} \\to k_\\text{SE}\\) as \\(\\nu \\to \\infty\\).\n\n\n\nKernel Function Comparison\n\n\nThe stationarity of these kernels allow us to assess their spectrum using Fourier transform. After standard Fourier transform computations, one can find the following spectral densities\n\\[\n\\begin{split}\nS_\\text{SE}(\\omega) &= 2 \\pi l^2 \\exp(-2\\pi^2 l^2 \\omega^2) \\\\\nS_\\text{Matérn}(\\omega) &= \\frac{\\Gamma(\\nu + 1/2) (2\\nu)^\\nu}{\\sqrt{\\pi} \\Gamma(\\nu) l^{2\\nu}} \\frac{1}{(\\omega^2 + 2\\nu / l^2)^{\\nu + 1/2}} \\\\\nS_{\\text{Matérn}-1/2}(\\omega) &= \\frac{1}{\\pi l} \\frac{1}{\\omega^2 + 1/l^2} \\\\\nS_{\\text{Matérn}-3/2}(\\omega) &= \\frac{2 \\sqrt{3}^3}{\\pi l^3} \\frac{1}{(\\omega^2 + 3/l^2)^2}.\n\\end{split}\n\\]\n\n\n\nSpectral Density Comparison\n\n\nThe SDE formulation we will be presenting below would only allow reformulation of stationary GPs. In particular, we will focus on the Matérn GPs as they are both flexible and commonly used model classes.\n\n\nSDE Formulation\nFirst of all, a Gaussian process is closed under linear operators, i.e. for a linear operator \\(\\mathcal{L}\\) and a Gaussian process \\(f\\), we know that \\(\\mathcal{L} f\\) is still a Gaussian process (Särkkä 2011). Since addition, scalar multiplication, and (partial) differentiation are all linear operators, the solution \\(f\\) of the following equation would be a Gaussian process\n\\[\na_0 f(t) + a_1 \\frac{df(t)}{dt} + a_2 \\frac{d^2 f(t)}{dt^2} + \\cdots + a_m \\frac{d^m f(t)}{dt^m} = w(t)\n\\]\nwhere \\(w(t)\\) is a white noise process with spectral density \\(\\Sigma\\) and is a Gaussian process.\nConsider the random vector \\(\\boldsymbol{f} = [f, f^{(1)}, f^{(2)}, \\ldots, f^{(m)}]^T\\) and the random process \\(\\boldsymbol{w} = [w_1, w_2, \\ldots, w_{m-1}, w]^T\\). We can recover the solution \\(f\\) via \\(f = \\boldsymbol{H} \\boldsymbol{f}\\) where \\(\\boldsymbol{H} = [1, 0, \\ldots, 0]\\) and the white noise process \\(w\\) via \\(w =\\boldsymbol{L} \\boldsymbol{w}\\) where \\(\\boldsymbol{L} = [0, \\ldots, 0, 1]\\). After rearrangements, we can convert the above equation into the following SDE\n\\[\n\\frac{d}{dt} \\boldsymbol{f}(t) = \\boldsymbol{F} \\boldsymbol{f}(t) + \\boldsymbol{L} \\boldsymbol{w}(t)\n\\]\nwhere\n\\[\n\\boldsymbol{F} = \\begin{bmatrix}\n        0 & 1 & 0 & 0 & \\cdots & 0 \\\\\n        0 & 0 & 1 & 0 &\\cdots & 0 \\\\\n        \\vdots & & \\ddots & \\ddots & & \\vdots \\\\\n        0 & &&&  1 & 0 \\\\\n        -a_0 & &\\cdots&\\cdots&  & -a_m \\\\\n    \\end{bmatrix}.\n\\]\nNotice that the above SDE can be solved exactly using integrating factor and Itô lemma, which gives us\n\\[\n\\boxed{\\begin{split}\n\\boldsymbol{f}(t) | \\boldsymbol{f}(t') &\\sim \\boldsymbol{N} \\left(A_t , Q_t \\right) \\\\\nA_t &= \\exp[\\boldsymbol{F}(t-t')]  \\boldsymbol{f}(t') \\\\\nQ_t &= \\int_{t'}^t \\exp[\\boldsymbol{F}(t - s)] \\boldsymbol{L} \\Sigma L^T \\exp[\\boldsymbol{F}^T (t - s)] ds.\n\\end{split}}\n\\]\nFinally, one should find the correct specifications of \\(\\boldsymbol{F}\\) and \\(\\Sigma\\) such that the solution GP of the SDE is the GP of interest. For example, with\n\\[\nF = \\begin{bmatrix} 0 & 1 \\\\ -\\lambda^2 & -2\\lambda \\end{bmatrix}, \\quad\nL = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}, \\quad\nH = \\begin{bmatrix} 1 & 0 \\end{bmatrix}, \\qquad \\Sigma  = 4\\lambda^3 \\sigma^2, \\qquad P_\\infty = \\begin{bmatrix}\n    \\sigma^2 & 0 \\\\ 0 & \\lambda^2 \\sigma^2\n\\end{bmatrix}\n\\]\nthe solution \\(f(t) = {H} \\boldsymbol{f}(t)\\) of SDE\n\\[\n\\frac{d}{dt} \\boldsymbol{f}(t) = \\boldsymbol{F} \\boldsymbol{f}(t) + {L} \\cdot \\boldsymbol{w}(t).\n\\]\nis a zero-mean GP with Matérn 3/2 kernel.\n\n\nRegression as Kalman Smoothing\nAssume we have made observations \\(\\boldsymbol{y} \\in \\mathbb{R}^{n_\\text{obs}}\\) at observation times \\(\\boldsymbol{x} \\in \\mathbb{R}^{n_\\text{obs}}\\), we assume the observations are noisy and follow\n\\[\ny_i = f(x_i) + \\varepsilon_i, \\qquad \\varepsilon_i \\stackrel{\\text{i.i.d.}}{\\sim} N(0, \\sigma_{\\text{obs}}^2), \\qquad \\forall i = 1, 2, \\ldots, n_\\text{obs}.\n\\]\nWe can construct the following system\n\\[\n\\begin{cases}\n\\frac{d}{dt} \\boldsymbol{f}(t) &= \\boldsymbol{F} \\boldsymbol{f}(t) + {L} \\cdot \\boldsymbol{w}(t) \\\\\n{y}_i &= \\boldsymbol{H} \\boldsymbol{f}({x}_i) + \\varepsilon_i \\qquad \\forall i = 1, 2, \\ldots, n_\\text{obs}\n\\end{cases}\n\\]\nwhich is a state-space model. The regression task is to find the distribution of \\(\\boldsymbol{f} | \\boldsymbol{y}\\), which is equivalent to applying the Kalman smoothing to the above state-space model.\nWe further assume that the observations are made at regular time intervals with gap \\(\\Delta\\). This makes the state-space model into:\n\\[\n\\begin{aligned}\nf_{k+1} &= \\Phi\\,f_k + e_k,\\quad e_k \\sim \\mathcal{N}(0, Q), \\\\\ny_k &= H\\,f_k + \\epsilon_k,\\quad \\epsilon_k \\sim \\mathcal{N}(0, \\sigma_\\text{obs}^2).\n\\end{aligned}\n\\]\nfor \\(k = 1, 2, \\ldots, n_\\text{obs}\\) with\n\\[\n\\Phi = \\exp[\\boldsymbol{F}\\Delta], \\qquad Q = P_\\infty - \\Phi P_\\infty \\Phi^T.\n\\]\nWe are ready to present the Kalman filter and RTS smoother.\n\nKalman Filter\nThe Kalman filter proceeds in two main steps - propagation and assimilation.\n\nPropagation Step\nPredict the state and covariance at time \\(k+1\\) given the filtered estimates at time \\(k\\): \\[\n\\begin{aligned}\n\\hat{f}_{k+1|k} &= \\Phi\\,\\hat{f}_{k|k}, \\\\\nP_{k+1|k} &= \\Phi\\,P_{k|k}\\,\\Phi^\\top + Q.\n\\end{aligned}\n\\]\n\n\nAssimilation Step\nWhen an observation \\(y_{k+1}\\) is available, update the prediction as follows:\n\nInnovation: \\[\n\\nu_{k+1} = y_{k+1} - H\\,\\hat{f}_{k+1|k}.\n\\]\nInnovation covariance: \\[\nS_{k+1} = H\\,P_{k+1|k}\\,H^\\top + \\sigma_\\text{obs}^2.\n\\]\nKalman gain: \\[\nK_{k+1} = \\frac{P_{k+1|k}\\,H^\\top}{S_{k+1}}.\n\\]\nUpdated state estimate: \\[\n\\hat{f}_{k+1|k+1} = \\hat{f}_{k+1|k} + K_{k+1}\\,\\nu_{k+1}.\n\\]\nUpdated covariance: \\[\nP_{k+1|k+1} = P_{k+1|k} - K_{k+1}\\,H\\,P_{k+1|k}.\n\\]\n\nIf no observation is available at a given time step, then the predicted state and covariance are carried forward:\n\\[\n\\hat{f}_{k+1|k+1} = \\hat{f}_{k+1|k}, \\quad P_{k+1|k+1} = P_{k+1|k}.\n\\]\nAdditionally, the log-likelihood contribution from the \\((k+1)\\)-th observation is computed as:\n\\[\n\\log p(y_{k+1} \\mid \\text{past}) = -\\frac{1}{2}\\left[\\log(2\\pi) + \\log(S_{k+1}) + \\frac{\\nu_{k+1}^2}{S_{k+1}}\\right].\n\\]\n\n\n\nRTS Smoother\nAfter running the forward Kalman filter, the Rauch–Tung–Striebel (RTS) smoother refines the state estimates by incorporating future observations. For \\(k = n_\\text{obs}-1, n_\\text{obs}-2, \\dots, 1\\):\n\nSmoothing gain: \\[\nC_k = P_{k|k}\\,\\Phi^\\top\\,(P_{k+1|k})^{-1}.\n\\]\nSmoothed state: \\[\n\\hat{f}_{k|n_\\text{obs}} = \\hat{f}_{k|k} + C_k\\left(\\hat{f}_{k+1|n_\\text{obs}} - \\hat{f}_{k+1|k}\\right).\n\\]\nSmoothed covariance: \\[\nP_{k|n_\\text{obs}} = P_{k|k} + C_k\\left(P_{k+1|n_\\text{obs}} - P_{k+1|k}\\right)C_k^\\top.\n\\]\n\n\n\n\nGaussan Process Regression via the SDE Approach\n\n\n\n\n\nComparison and Implementation Details\nThe computational costs of GP regression via the vanilla approach is cubic, i.e. \\(O(n_\\text{obs}^3)\\), whereas the SDE approach is linear, i.e. \\(O(n_\\text{obs})\\).\n\n\n\nComputational Cost Comparison of Gaussian Process Regression - Conjugacy v.s. Kalman\n\n\nBoth approaches are in fact equivalent, so the computational gain of the SDE approach has no hidden costs.\n\n\n\nGaussian Process Regression Comparison - COnjugacy v.s. Kalman\n\n\nFinally, one could do likelihood training with the SDE approach which gives maximum likelihood estimates of the hyperparameters of the prior distribution (thus we are doing empirical Bayes instead of standard Bayes).\nSome remarks on implementation. The plots above are all using a more granular time grid than the observation time grid, as can be observed from the smooth posterior mean. This means, we are filtering at times where there are no observations (so only propagate, not assimilate) and then correct them in the filtering step. This will bump up the computational costs (linearly).\nIn practice, if prediction at future time is the only downstream task of GP regression, then one could simply do filtering till the last observation time and not do any smoothing. This would drastically reduce the computational cost as we are doing updating at observation times (rather than the more granular regression time grid) and can skip the smoothing.\n\n\n\n\n\nReferences\n\nSarkka, Simo, Arno Solin, and Jouni Hartikainen. 2013. “Spatiotemporal Learning via Infinite-Dimensional Bayesian Filtering and Smoothing: A Look at Gaussian Process Regression Through Kalman Filtering.” IEEE Signal Processing Magazine 30 (4): 51–61.\n\n\nSärkkä, Simo. 2011. “Linear Operators and Stochastic Partial Differential Equations in Gaussian Process Regression.” In Artificial Neural Networks and Machine Learning–ICANN 2011: 21st International Conference on Artificial Neural Networks, Espoo, Finland, June 14-17, 2011, Proceedings, Part II 21, 151–58. Springer.\n\n\nSolin, Arno. 2016. “Stochastic Differential Equation Methods for Spatio-Temporal Gaussian Process Regression.”"
  },
  {
    "objectID": "posts/2025-02-03-gaussian-cov/index.html",
    "href": "posts/2025-02-03-gaussian-cov/index.html",
    "title": "Summary of a Bivariate Gaussian Covariance Matrix",
    "section": "",
    "text": "For the active learning of a spatial vector field, one may impose a gird structure to the space and assign a random vector (in 2D) to each of the grid cell. The full vector field is modeled using a Gaussian process (e.g. a Helmholtz GP of Berlinghieri et al. (2023)), and under the Gaussian noise assumption, each random vector \\([u v]^T\\) of the grid cell \\((x,y)\\) is marginally a bivariate Gaussian:\n\\[\n\\begin{bmatrix} u_{(x,y)} \\\\ v_{(x,y)} \\end{bmatrix} \\sim N_2 \\left(\n\\begin{bmatrix} \\mu^u_{(x,y)} \\\\ \\mu^v_{(x,y)} \\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma^{uu}_{(x,y)} & \\Sigma^{uv}_{(x,y)} \\\\\n\\Sigma^{vu}_{(x,y)} & \\Sigma^{vv}_{(x,y)}\n\\end{bmatrix}\n\\right) = N_2 (\\mu_{(x,y)}, \\Sigma_{(x,y)}).\n\\]\nActive learning algorithms aim to choose the next design/evaluation point that yields the highest utility, where the utility is often linked to the uncertainty of the evaluation point or the full system. For example, if our surrogate model of the system has input dimension of one, i.e. each design point will be marginally an univariate Gaussian, a utility choice is simply the variance of that distribution, leading to the max-var utility. Notice that the variance of an univariate Gaussian \\(X\\) is monotonically related to its entropy\n\\[\nH(X) := \\frac{1}{2}  + \\frac{1}{2} \\log [2\\pi \\text{Var}(X)]\n\\] in the sense that for two univariate Gaussians \\(X_1, X_2\\) with variances \\(\\sigma_1^2, \\sigma_2^2\\) respectively, we have \\(\\sigma_1^2 \\le \\sigma_2^2 \\implies H(X_1) \\le H(X_2)\\) which should be obvious from the definition."
  },
  {
    "objectID": "posts/2025-02-03-gaussian-cov/index.html#background",
    "href": "posts/2025-02-03-gaussian-cov/index.html#background",
    "title": "Summary of a Bivariate Gaussian Covariance Matrix",
    "section": "",
    "text": "For the active learning of a spatial vector field, one may impose a gird structure to the space and assign a random vector (in 2D) to each of the grid cell. The full vector field is modeled using a Gaussian process (e.g. a Helmholtz GP of Berlinghieri et al. (2023)), and under the Gaussian noise assumption, each random vector \\([u v]^T\\) of the grid cell \\((x,y)\\) is marginally a bivariate Gaussian:\n\\[\n\\begin{bmatrix} u_{(x,y)} \\\\ v_{(x,y)} \\end{bmatrix} \\sim N_2 \\left(\n\\begin{bmatrix} \\mu^u_{(x,y)} \\\\ \\mu^v_{(x,y)} \\end{bmatrix},\n\\begin{bmatrix}\n\\Sigma^{uu}_{(x,y)} & \\Sigma^{uv}_{(x,y)} \\\\\n\\Sigma^{vu}_{(x,y)} & \\Sigma^{vv}_{(x,y)}\n\\end{bmatrix}\n\\right) = N_2 (\\mu_{(x,y)}, \\Sigma_{(x,y)}).\n\\]\nActive learning algorithms aim to choose the next design/evaluation point that yields the highest utility, where the utility is often linked to the uncertainty of the evaluation point or the full system. For example, if our surrogate model of the system has input dimension of one, i.e. each design point will be marginally an univariate Gaussian, a utility choice is simply the variance of that distribution, leading to the max-var utility. Notice that the variance of an univariate Gaussian \\(X\\) is monotonically related to its entropy\n\\[\nH(X) := \\frac{1}{2}  + \\frac{1}{2} \\log [2\\pi \\text{Var}(X)]\n\\] in the sense that for two univariate Gaussians \\(X_1, X_2\\) with variances \\(\\sigma_1^2, \\sigma_2^2\\) respectively, we have \\(\\sigma_1^2 \\le \\sigma_2^2 \\implies H(X_1) \\le H(X_2)\\) which should be obvious from the definition."
  },
  {
    "objectID": "posts/2025-02-03-gaussian-cov/index.html#summaries-for-bivariate-gaussian-covariance",
    "href": "posts/2025-02-03-gaussian-cov/index.html#summaries-for-bivariate-gaussian-covariance",
    "title": "Summary of a Bivariate Gaussian Covariance Matrix",
    "section": "Summaries for Bivariate Gaussian Covariance",
    "text": "Summaries for Bivariate Gaussian Covariance\nWe consider max-var styled policies, where we pick the evaluation point with the highest variance. In the case where we have a 2D spatial vector field to model and each vector is marginally \\(N_2\\), we try to summarize the evaluation point’s uncertainty using a function of the covariance matrix \\(\\Sigma\\).\n\nEntropy of a Multivariate Gaussian\nFirst, we calculate the entropy of a multivariate Gaussian \\(N_D(\\mu, \\Sigma)\\). We have\n\\[\n\\begin{split}\nH(X) &= - \\mathbb{E}_{x \\sim X} \\left[ \\log p(x) \\right] \\\\\n&= - \\mathbb{E}_{x \\sim X} \\left[- \\frac{D}{2} \\log \\pi - \\frac{1}{2} \\log \\det (\\Sigma) - \\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right] \\\\\n&= \\frac{D}{2} \\log \\pi + \\frac{1}{2} \\log \\det (\\Sigma) + \\frac{1}{2} \\mathbb{E}_{x \\sim X} \\left[ (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right]\n\\end{split}\n\\] where\n\\[\n\\begin{split}\n\\mathbb{E}_{x \\sim X} \\left[ (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right] &= \\mathbb{E}_{x \\sim X} \\left[ \\text{tr} \\left( (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right) \\right] \\\\\n&= \\mathbb{E}_{x \\sim X} \\left[ \\text{tr} \\left( \\Sigma^{-T} (x - \\mu) (x - \\mu)^T \\right) \\right] \\\\\n&= \\text{tr} \\left[ \\Sigma^{-1} \\mathbb{E}_{x \\sim X} \\left[ (x - \\mu) (x - \\mu)^T \\right] \\right] \\\\\n&= \\text{tr} \\left[ \\Sigma^{-1} \\Sigma \\right] = \\text{tr} \\left[ I_D \\right] = D.\n\\end{split}\n\\]\nThus, the differential entropy is\n\\[\nH(X) = \\frac{D}{2} \\log \\pi + \\frac{D}{2} + \\frac{1}{2} \\log \\det (\\Sigma).\n\\]\n\n\nSummary Options\n\nTrace of \\(\\Sigma\\)\n\\[\n\\text{tr}(\\Sigma) = \\Sigma_{11} + \\Sigma_{22}\n\\]\n\nThis captures the sum of the variances in both dimensions, but ignores the correlations.\n\nDeterminant of \\(\\Sigma\\)\n\\[\n\\det(\\Sigma) = \\Sigma_{11} \\Sigma_{22} - \\Sigma_{12} \\Sigma_{21}\n\\]\n\nThis can be interpreted as the “area” of uncertainty in the 2D space. It is equivalent to the entropy when used for comparison. For two covariance matrices with same variances, the determinant will be smaller for the one with higher correlation.\n\nNorm of \\(\\Sigma\\)\n\\[\n\\| \\Sigma \\|_{?}\n\\]\n\nDifferent matrix norms provide different interpretations of overall uncertainty. For example, the Frobenius norm is an element-wise norm that flattens the matrix \\(\\Sigma\\) and compute its \\(L_2\\) norm.\n\n\n\n\nExample\nIn the following graph we show four covariance matrices and their covariance ellipses (horizontal cross-sections of their probability density functions), as well as the values for various summaries.\n\n\n\nAn example of the values of summaries for various covariances."
  },
  {
    "objectID": "posts/2024-11-28-Kalman-filter/index.html",
    "href": "posts/2024-11-28-Kalman-filter/index.html",
    "title": "[Derivation Scribbles] Kalman Filter and Ensemble Kalman Filter",
    "section": "",
    "text": "This blog post is largely based on some notes written by Chris Sherlock."
  },
  {
    "objectID": "posts/2024-11-28-Kalman-filter/index.html#footnotes",
    "href": "posts/2024-11-28-Kalman-filter/index.html#footnotes",
    "title": "[Derivation Scribbles] Kalman Filter and Ensemble Kalman Filter",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAn implicit notation throughout this blog post is that capital letters such as \\(X_t\\) represents random variables, while the lower cases like \\(x_t\\) represent the realisations of a random variable, thus is a constant.↩︎\nWe use the notation \\(1:t\\) to represent \\(1, 2, \\ldots, t\\).↩︎\nWe will slightly abuse the notation by including things such as \\(N(0,Q)\\) with properly defined random variables to ease of exposition.↩︎"
  },
  {
    "objectID": "posts/2024-10-23-temporal-GP-1/index.html",
    "href": "posts/2024-10-23-temporal-GP-1/index.html",
    "title": "Spatial-Temporal GP (1)",
    "section": "",
    "text": "In this blog post, I will walk through how one could conduct temporal Gaussian process (GP) regression with one-dimensional space + one-dimensional time inputs and one-dimensional output. This is the first of a series of blog posts on spatial-temporal Gaussian processes.\n\nLatent Function\nThe latent function that we want our GP to learn is\n\\[\nf(x,t) = -t \\sin(2 \\pi x) + (1-t)\\cos(\\pi x)\n\\]\nwhere we will be focusing on the region of \\(x \\in [0,2]\\) and \\(t \\in [0,1]\\). We will obtain noisy observations from this function \\(f\\) at given time stamps and random locations. We will discretise time into \\(\\{0, 1/10, 2/10, \\ldots, 1\\}\\). At each of those times, we will observe \\(f\\) at 25 uniformly drawn locations (same locations across time) with additive Gaussian noise with mean zero and variance 0.01. The observations, denoted by \\(\\mathcal{D}\\) and the latent function are displayed below as a GIF.\n\n\n\nObservations\n\n\n\n\nPrior Distribution\nWe will need to define a Gaussian process to conduct the regression. Conceptually, we can treat time as just another dimension of the input without worrying much about its interpretation. So, we have a two dimensional input, one dimensional output Gaussian process. We would need to specify its mean and kernel functions. The mean function will be set to zero. The kernel will be a separable kernel, where we give an SE kernel to the space dimension and a Matérn 5/2 kernel to the time dimension. Therefore, our GP prior \\(g\\) could be written as\n\\[\ng \\sim \\mathcal{GP}({0}, {k}), \\qquad {k}(s,t) = k_s(s)\\times k_t(t)\n\\]\nwhere \\(k_s\\) is a SE kernel and \\(k_t\\) is a Matérn 5/2 kernel. A consequence of this construction of the overall kernel is that the Gram matrix \\({K}\\) can be written as a Kronecker product, i.e. \n\\[\n{K} = K_s \\otimes K_t\n\\]\nwhere \\({K}\\) is obtained using the Cartesian product \\(\\{(s_i, t_j)\\}_{i,j} = \\{s_i\\}_{i=1}^n \\times \\{t_j\\}_{j=1}^m\\) while \\(K_s, K_t\\) are obtained using \\(\\{s_i\\}_{i=1}^n\\) and \\(\\{t_j\\}_{i=1}^m\\) respectively. This matrix structure can be noticed easily from the following heat maps of the Gram matrices.\n\n\n\nGram Matrices\n\n\nBack to our GP prior \\(g\\). We can generate samples from it and they are displayed in the following GIF.\n\n\n\nPrior Samples\n\n\n\n\nPosterior Distribution\nWe have a prior and we have observations \\(\\mathcal{D}\\), now we will need to write down the likelihood and apply the Bayes rule to obtain the posterior distribution for our GP regression. The posterior distribution \\(g | \\mathcal{D}\\) is still a GP and its expression can be obtained exactly due to the conjugacy from the Gaussian noises. The GIF of mean, 2 standard deviation bound, and samples of the posterior distribution along with the latent function and the observations is presented below.\n\n\n\nPosterior Samples\n\n\n\n\nBenefits of Temporal Modelling\nThere is clear difference and benefits of directly including time into our GP model, although it increases the computational costs of regression. If we treat each GP at different time stamps as separate problems and fit a Gaussian process independently. A comparison of modelling without time and with time of the same problem is shown below.\n\n\n\nComparisons of GP Fits\n\n\nWe can see more stable performance for our temporal model, especially at \\(t=0.56\\) where the independent model overfits the data.\nAnother key benefits of having a temporal GP is that we can extrapolate our model to time stamps that we have not observed. This interpolation is not possible for the without-time model.\nA natural extension of the independent across time stamps modelling approach is to consider a multi-output Gaussian process model where we jointly model the functions at different times. It turns out that, if we assume a separable kernel for the multi-output GP, then it will be (roughly) equivalent to include time in the input space.\nFinally, the information borrowing across different time stamps help with the model’s robustness against missing data. In the following comparison plot, we sensor the left half and the right half of the observations at time \\(t = 0.33\\) and \\(t = 0.56\\) respectively. The independent version of the model fails immediately, while the performance of the temporal GP was not influenced by it.\n\n\n\nComparisons of GP Fits with Missing Data"
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html",
    "href": "posts/2024-09-13-WGF/index.html",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "",
    "text": "Optimisation is a fundamental task in modern-day statistics and machine learning. A large set of problems in machine learning and statistics can be easily phrased as an optimisation problem - given some objective function \\(f\\) defined on a domain \\(\\mathcal{X}\\), we wish to find a point \\(x \\in \\mathcal{X}\\) that minimises \\(f\\) (or maximises \\(-f\\)). Sometimes, we do not even need to find the global minimum of \\(f\\), and a sufficiently close local minimum would be good too."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#gradient-flows-in-the-euclidean-space",
    "href": "posts/2024-09-13-WGF/index.html#gradient-flows-in-the-euclidean-space",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "Gradient Flows in the Euclidean Space",
    "text": "Gradient Flows in the Euclidean Space\nA common optimisation algorithm is the gradient descent. If our objective function \\(f\\) defined on the Euclidean space \\(\\mathbb{R}^d\\) is continuous and we can compute its gradient \\(\\nabla f\\), then, the gradient descent algorithm will iteratively apply the following update\n\\[\nx_{n+1} = x_n - h \\nabla f(x_n)\n\\]\nuntil we converge or reach a termination point. The parameter \\(h&gt;0\\) above is the step size of our algorithm, often referred to as a learning rate and it is a tuning parameter of the gradient descent algorithm. When we set \\(h\\) to be very small, and let it tend to zero, we would convert the above discrete-in-time algorithm into a continuous-in-time algorithm, described as\n\\[\n\\frac{\\mathrm{d}}{\\mathrm{d}t} x_t = -\\nabla f(x_t)\n\\]\nwhere we use \\(t\\) instead of \\(n\\) to denote the time index as we are in continuous time rather than discrete time. Notice that for the above ordinary differential equation (ODE), after an Euler discretisation (of time), will become the gradient descent algorithm. The ODE is known as the gradient flow (in Euclidean space), and we can show that various frequently used algorithms can be interpreted as different discretisations of the gradient flow. For example, an implicit Euler discretisation of the gradient flow gives us the proximal point algorithm.\nOne can certainly see the conceptual benefit of considering gradient flow for understanding discrete-in-time optimisation algorithms - we suddenly have a simple, elegant mental picture of the limiting case of these procedures. However, rather unfortunately, the gradient flow in Euclidean space could not help us that much more than that. Often in theoretical analysis of iterative algorithms, we are interested in the convergence rate of these algorithms to some target value, and in the cases where approximations happen in the algorithms, we are interested in capturing the errors induced. Because of the discretisation in time, we could not translate many of the theories about gradient flow in Euclidean space into their discrete-in-time counterparts. This is the main reason why although gradient flows are extremely natural and tempting to investigate, they have not been considered as much, until very recently."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#the-langevin-diffusion",
    "href": "posts/2024-09-13-WGF/index.html#the-langevin-diffusion",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "The Langevin Diffusion",
    "text": "The Langevin Diffusion\nA major breakthrough, at least from a theoretical perspective, happened with Jordan, Kinderlehrer & Otto’s 1998 paper The Variational Formulation of the Fokker-Planck Equation. In there, the authors made an explicit connection between the Langevin diffusion, a particular type of Stochastic Differential Equation (SDE) with very nice equilibrium properties, and a gradient flow in the space of probability distributions. The Langevin diffusion can be characterised by the SDE\n\\[\n\\mathrm{d}X_t = \\nabla \\log \\pi(X_t) \\mathrm{d}t + \\sqrt{2}\\mathrm{d}B_t\n\\]\nwhere \\(\\{B_t\\}\\) is a Brownian motion and \\(\\pi\\) is the equilibrium distribution of the process, and it could also be characterised by the Fokker-Planck equation\n\\[\n\\partial_t p_t(x) = \\text{div} \\left( p_t(x) \\nabla \\log \\frac{p_t(x)}{\\pi(x)} \\right)\n\\]\nwhere \\(p_t(x)\\) is the probability distribution of \\(X_t\\). Naively, one can think about the two characterisations of the Langevin diffusion as a state space version and a distribution space version of the same motion.\nSo, the paper of JKO1998 established that the Fokker-Planck equation of the Langevin diffusion is equivalent to a gradient flow in the Wasserstein space with the objective function being the KL divergence \\(f(\\cdot) = \\text{KL}(\\cdot \\| \\pi)\\) where\n\\[\n\\text{KL}(p\\| q) := \\int p(x) \\log[p(x) / q(x)] dx = \\mathbb{E}_{X \\sim p} [\\log ( p(X)/q(X)) ].\n\\]\nIntuitively, what this connection tells us is that the particles following a Langevin diffusion are moving - in the steepest direction - towards their equilibrium distribution.\nAs an example, let’s assume that our target distribution of interest \\(p\\) is a Gaussian \\(\\mathcal{N}(0,1)\\) and particles are represented by the distribution \\(q\\). As seen in the following movie, we can use the Wasserstein gradient flow of KL divergence to sequentially evolve \\(q\\) and minimise the KL divergence.\n (Thanks to Louis Sharrock for creating this movie)\nThis result seems neat, but what is so special about this Langevin diffusion? It turns out that the Langevin diffusion is rather fundamental in sampling algorithms for computational statistics."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#monte-carlo-sampling",
    "href": "posts/2024-09-13-WGF/index.html#monte-carlo-sampling",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "Monte Carlo Sampling",
    "text": "Monte Carlo Sampling\nIn statistics, especially in Bayesian statistics, we would often run into the problem of having a complicated probability distribution that we wish to compute expectations of, such as in the case of computing the posterior mean of a parameter of interest. If the distribution is complex and we cannot analytically evaluate our expectations of interest, then we often rely on using (independent) samples from the distribution to form an empirical approximation of the distribution. To be more precise, if we have a target probability distribution \\(\\pi\\), we will get a sequence of independent samples \\(X_1, X_2, \\ldots, X_n \\sim \\pi\\) and we have\n\\[\n\\pi(x) \\approx \\frac{1}{n} \\sum_{k=1}^n 1_{X_k}(x)\n\\]\nwhere \\(1_{X_k}(x)\\) is the indicator function that takes the value 1 when \\(x = X_k\\) and zero otherwise. This is the Monte Carlo method, and it can be shown that under weak conditions of the target distribution \\(\\pi\\), the empirical distribution converges to \\(\\pi\\) at a rate of \\(O(1/\\sqrt{n})\\) for \\(n\\) Monte Carlo samples. The only problem with the Monte Carlo method is, how do we get those samples? As alluded slightly from the Langevin diffusion, since we can set the equilibrium distribution of a Langevin diffusion to (almost) any target distribution and the process will converge to it after running for a while, we can just start the SDE at some point and run it for long enough so it hits the equilibrium, and use the trajectories afterwards as samples from the target distribution.\nImmediately, we would ask - how exactly do we simulate a continuous-in-time SDE? The simplest solution is to use the Euler-Maruyama scheme and obtain discretisations using the following iterative procedure\n\\[\nX_{(n+1)h} = X_{nh} + h \\nabla \\log \\pi(X_{nh})+\\sqrt{2h} \\xi\n\\]\nwhere \\(\\xi \\sim N(0,1)\\). This gives us the unadjusted Langevin algorithm (ULA), also known as the Langevin Monte Carlo (LMC) algorithm in the machine learning literature.\nSince this is a discretisation, it introduces some numerical errors (the precise reason for the errors will be explained in a bit) and by using ULA we will not obtain exact samples from the target distribution \\(\\pi\\). For sufficiently small \\(h\\), the error would be tolerable. We could also do smart things such as Metropolis adjustments to remove the error, and we would recover the Metropolis Adjusted Langevin Algorithm (MALA) which is a staple of the Markov chain Monte Carlo (MCMC) algorithms for computational statistics. More thorough discussions on MCMC algorithms can be found in textbooks such as Monte Carlo Statistical Methods by Robert & Casella, or the recent Scalable Monte Carlo for Bayesian Learning by Fearnhead, Nemeth, Oates & Sherlock. One could also find a more detailed theoretical study of ULA in Roberts & Tweedie’s 1996 paper Exponential Convergence of Langevin Distributions and their Discrete Approximations."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation",
    "href": "posts/2024-09-13-WGF/index.html#wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "Wasserstein Gradient Flow - a Bridge between Sampling and Optimisation",
    "text": "Wasserstein Gradient Flow - a Bridge between Sampling and Optimisation\nSo far, we have learnt that the Langevin diffusion can be viewed as a gradient flow, and the discrete-in-time version of the Langevin diffusion allows us to draw samples from a target distribution. It turns out that we can also interpret the discrete Langevin diffusion of the LMC as a discrete-in-time approximation of the corresponding gradient flow in the space of probability distributions (to be more precise, the Wasserstein space, so we would often call this type of gradient flow a Wasserstein gradient flow).\nIn the 2018 paper Sampling as Optimization in the Space of Measures by Wibisono, the author pointed out that the LMC as an Euler-Maruyama discretisation of the Langevin diffusion can be viewed as a forward-flow splitting discretisation of the Wasserstein gradient flow with the objective function being the KL divergence. The forward-flow splitting scheme is a way to discretise time by doing half a step of forward discretisation, and half a step of flow discretisation, for each full step of the iteration. The expression of the two discretisations is slightly involved to describe in the space of probability distributions, but if we translate them into the state space, it is simply\n\\[\n\\text{(forward)} \\ X_{(n+1/2)h} = X_{nh} + h \\nabla \\log \\pi(X_{nh}),\n\\] \\[\n\\text{(flow)} \\ X_{(n+1)h} = X_{(n+1/2)h} + \\sqrt{2h} \\xi\n\\] with \\(\\xi \\sim N(0,1)\\), which combines to give us the full LMC update. Another observation in Wibisono (2018) is that, if we swap the flow step with a backward discretisation step, we would be able to cancel the error of discretising the Langevin diffusion. Unfortunately, the backward step is not implementable in general. Nevertheless, this paper provides us with the very important information that there exists a hidden connection between sampling (using LMC) and optimisation (using gradient flows). A bridge between the two areas has been formally built at this point.\nTo further utilise the power of this connection, Durmus, Majewski & Miasojedow in their 2019 paper Analysis of Langevin Monte Carlo via Convex Optimization provided us with a more explicit characterisation of the error of LMC using convergence analysis of the Wasserstein gradient flow. Unlike in the case of gradient flows in Euclidean space, the theoretical studies of Wasserstein gradient flows can actually be used in the analysis of their discrete-in-time counterparts."
  },
  {
    "objectID": "posts/2024-09-13-WGF/index.html#what-else-can-we-do",
    "href": "posts/2024-09-13-WGF/index.html#what-else-can-we-do",
    "title": "Why Should We Care About Gradient Flows?",
    "section": "What Else Can We Do?",
    "text": "What Else Can We Do?\nAt this point, it should be clear that the connection between sampling and optimisation established using Wasserstein gradient flows is promising and potentially very useful.\nOne immediate area of work is to interpret existing sampling algorithms as gradient flows, and use these realisations to help us better understand the properties of these algorithms. There are already some successful fruits from this branch:\n\nLiu’s 2017 paper Stein Variational Gradient Descent as Gradient Flow interpreted the Stein Variational Gradient Descent algorithm, a powerful sampling algorithm, as a type of gradient flow.\nDuncan, Nüsken & Szpruch’s 2023 paper On the Geometry of Stein Variational Gradient Descent built on the above realisation and showed several convergence results about the algorithm, as well as certain improvements based on such gradient flow analysis.\nNüsken’s 2024 paper Stein Transport for Bayesian Learning proposed a promising new algorithm Stein Transport that extends the Stein Variational Gradient Descent by tweaking the geometry of the Wassterstein gradient flow.\nChopin, Crucinio & Korba’s 2024 paper A connection between Tempering and Entropic Mirror Descent has established that tempering sequential Monte Carlo algorithms can be viewed as a type of discretisation of the gradient flow in the Fisher-Rao geometry.\n\nIn addition, a class of work that could be made possible with this new connection is those that translate algorithmic tricks from one field (say optimisation) to another (say sampling). A very nice example of this thinking is the line of work by Sharrock, Nemeth and coauthors over recent years. A lot of optimisation algorithms involve tuning parameters (also known as learning rates) that have to be manually adjusted, and different specifications of them will sometimes yield very different performance of the algorithms. To tackle the difficulties of tuning such parameters, there is a class of learning-rate-free algorithms that replace the tuning of learning rates with an automatic mechanism. With the help of the connection between sampling and optimisation made by gradient flows, recent work has managed to replace the manually-tuned learning rates of sampling algorithms with automatic, learning-rate-free ones, as shown in the papers such as Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates and Learning Rate Free Sampling in Constrained Domains.\nOverall, gradient flows and related ideas have become a promising tool for investigating theoretical properties of sampling algorithms, and have shown a considerable amount of potential to inspire designs of new sampling algorithms. There remains a vast pool of unanswered questions and possible extensions in this area. More breakthroughs are to be expected from this line of work.\nP.S. A book-length, formal introduction to the material covered above and more can be found in Statistical Optimal Transport by Chewi, Niles-Weed & Rigollet."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rui-Yang Zhang",
    "section": "",
    "text": "I am currently a second year Computational Statistics and Machine Learning PhD student of STOR-i CDT at Lancaster University and am supported by ARC TIDE Hub in Australia. My supervisors are Henry Moss and David Leslie at Lancaster, as well as Lachlan Astfalck and Edward Cripps at UWA.\nI am broadly interested in the decision aspect of Bayesian computational statistics and machine learning. In particular, my research focuses on developing sequential Bayesian designs – such as active learning and Bayesian optimization – for a wide range of application areas including probabilistic numerics and ocean engineering. I am also exploring what post-Bayesian ideas could offer for more robust, efficient decision-making.\nPreviously, I did BSc Mathematics and Statistical Science at UCL where I was mentored by Sam Livingstone and received a Royal Statistical Society Award.\n\nNews\n\n[Feb 26] Presented BALLAST at AI Across Scale workshop in Newton Institute, Cambridge. The recording of my talk can be found here.\n[Jan 26] Presented BALLAST at the STOR-i Annual Conference in Lancaster and the Irish CRT Winter Symposium in Dublin.\n[Dec 25] Our paper Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift has been accepted in the IMA Journal of Numerical Analysis! This is joint work with Yuga Iguchi, Sam Livingstone, Nik Nüsken, and Giorgos Vasdekis.\n\n\n\nOlder News\n\n\n[Oct 25] Presented VaSE at Workshop on Kernels, Flows, and Sampling in Newcastle. Had a great time!\n[Oct 25] Presented BALLAST at Physics Enhancing Machine Learning workshop in Institute of Physics, London.\n[Sep 25] Passed the baton of organising the LAI reading group to Cass. What a year!\n[Aug 25] Supervised two wonderful STOR-i Summer Interns: Ben on probabilistic eddy identifications and Harriett (co-supervised with Kajal) on sea surface temperature modelling.\n[Jul 25] Awarded the PGR Citizenship Prize with Andreas by the School of Mathematical Sciences for setting up the LAI reading group.\n[Jun 25] Conducted a week-long research visit at UCL with Yuga, Sam, and Giorgos; sponsored by the Heilbronn Institute.\n[Mar 25] Did several outreach talks to sixth form students in Lancashire (Lancaster, Blackpool, and Southport) on decision and game theories – giving back to the community. Supported by STOR-i, especially Wendy and Kate!\n[Feb 25] Presented an earlier version of BALLAST at the Statistics PhD Seminar in UCL. Talk recording can be found here.\n[Dec 24] Attended APTS training in Oxford. Beautiful place!\n[Nov 24] Visited UWA in Perth, Australia for a month; sponsored by ARC TIDE Hub. Nice sunny break from the damp English winter.\n[Sep 24] Co-organised the BayesAI workshop; sponsored by STOR-i and ProbAI.\n\n\n\n\n\nAdapted from Adrien’s codes."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "[Derivation Scribbles] Fourier Transform and FFT\n\n\n\n\n\n\n\nDerivation Scribbles\n\n\n\n\nDescribes the Fourier transform, mentions the sampling theorem that connects discrete and continuous time Fourier transform, and presents the fast Fourier transform that efficiently applies the discrete Fourier transform to a uniformly sampled sequence.\n\n\n\n\n\n\nOct 12, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nApproximate GP Posterior Sampling via the Matheron Rule\n\n\n\n\n\n\n\nGaussian Process\n\n\n\n\nThe Matheron rule of conditional multivariate normal distribution offers a new way to approximately sample from a posterior Gaussian process.\n\n\n\n\n\n\nOct 9, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nMonte Carlo, Antithetic, and Quasi Monte Carlo\n\n\n\n\n\n\n\nSampling\n\n\n\n\nBasics of Monte Carlo, Antithetic, Quasi Monte Carlo, Randomised Quasi Monte Carlo.\n\n\n\n\n\n\nAug 11, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nUpscale a Brownian Motion Path\n\n\n\n\n\n\n\nGaussian Process\n\n\nSampling\n\n\n\n\nFor a Brownian motion path observed at fixed locations, upscale the resolution using conditioning using the fact that a Brownian motion is a Gaussian process.\n\n\n\n\n\n\nJul 30, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Reading Notes] Deep Learning is Not So Mysterious or Different\n\n\n\n\n\n\n\nDeep Learning\n\n\nReading Notes\n\n\n\n\nNotes on reading Andrew Gordon Wilson’s ICML 2025 paper Deep Learning is Not So Mysterious or Different.\n\n\n\n\n\n\nJul 11, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nToy Bayesian Data Assimilation\n\n\n\n\n\n\n\nData Assimilation\n\n\n\n\nBayesian data assimilation for a toy problem.\n\n\n\n\n\n\nJul 1, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nIdentifiability Issues of Gaussian Processes\n\n\n\n\n\n\n\nGaussian Process\n\n\n\n\nDescribe the consistency and identifiability of Matérn GPs.\n\n\n\n\n\n\nJun 18, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nSpatial-Temporal GP (3)\n\n\n\n\n\n\n\nGaussian Process\n\n\n\n\nA series of blog posts on spatial-temporal Gaussian processes. SDE Approach to Temporal GP Regression.\n\n\n\n\n\n\nMar 30, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nExpected Information Gain with Gaussian Process Surrogate Models\n\n\n\n\n\n\n\nActive Learning\n\n\nGaussian Process\n\n\n\n\nComputations and derivations of the expected information gain utility function of active learning when the surrogate model is a conjugate Gaussian process.\n\n\n\n\n\n\nFeb 7, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nSummary of a Bivariate Gaussian Covariance Matrix\n\n\n\n\n\n\n\nActive Learning\n\n\nGaussian Process\n\n\n\n\nVarious ways one could summarise a bivariate Gaussian’s covariance matrix.\n\n\n\n\n\n\nFeb 3, 2025\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Derivation Scribbles] 3D-Var and 4D-Var\n\n\n\n\n\n\n\nData Assimilation\n\n\nDerivation Scribbles\n\n\n\n\nDerivations for 3D-Var and 4D-Var.\n\n\n\n\n\n\nDec 3, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Derivation Scribbles] Kalman Filter and Ensemble Kalman Filter\n\n\n\n\n\n\n\nData Assimilation\n\n\nDerivation Scribbles\n\n\n\n\nDerivations for Kalman filters and Ensemble Kalman Filter.\n\n\n\n\n\n\nNov 28, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nSpatial-Temporal GP (2)\n\n\n\n\n\n\n\nGaussian Process\n\n\n\n\nA series of blog posts on spatial-temporal Gaussian processes. Exploiting the Kronecker structure of temporal GP regression with 1d space.\n\n\n\n\n\n\nOct 31, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nSpatial-Temporal GP (1)\n\n\n\n\n\n\n\nGaussian Process\n\n\n\n\nA series of blog posts on spatial-temporal Gaussian processes. Temporal GP regression with 1d space.\n\n\n\n\n\n\nOct 23, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\n[Derivation Scribbles] Basic GP Regression Formula\n\n\n\n\n\n\n\nGaussian Process\n\n\nDerivation Scribbles\n\n\n\n\nDerivations for the Gaussian process predictive distribution. Single-output GP, observe with additive Gaussian noise.\n\n\n\n\n\n\nOct 13, 2024\n\n\nRui-Yang Zhang\n\n\n\n\n\n\n  \n\n\n\n\nWhy Should We Care About Gradient Flows?\n\n\n\n\n\n\n\nGradient Flow\n\n\n\n\nBlog post on gradient flows in Euclidean and Wasserstein spaces.\n\n\n\n\n\n\nSep 13, 2024\n\n\nRui-Yang Zhang, Christopher Nemeth\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "Notes & Slides",
    "section": "",
    "text": "Basic Markov chain Monte Carlo Method\nPDMP and MCMC\nGeometric Ergodicities of Langevin and Barker Algorithms\nIntroduction to Sequential Monte Carlo\n\n\n\n\n\nWasserstein Gradient Flow\nIntroduction to Gaussian Processes\nIntroduction to Bayesian Optimisation\nIntroduction to Kernel Stein Discrepancy (with Lanya Yang)\nMarkov Decision Processes and How to Solve Them (code)\n\n\n\n\n\nDirichlet Laplacian Eigenfunction Problem\n\n\n\n\n\nMultiple Testing Problem\nResponse-Adaptive Randomisation (code1) (code2)\n\n\n\n\n\nSpectral Theory\nProbability Theory\n\n\n\n\n\nUnivariate Extreme Value Modelling"
  },
  {
    "objectID": "notes.html#notes",
    "href": "notes.html#notes",
    "title": "Notes & Slides",
    "section": "",
    "text": "Basic Markov chain Monte Carlo Method\nPDMP and MCMC\nGeometric Ergodicities of Langevin and Barker Algorithms\nIntroduction to Sequential Monte Carlo\n\n\n\n\n\nWasserstein Gradient Flow\nIntroduction to Gaussian Processes\nIntroduction to Bayesian Optimisation\nIntroduction to Kernel Stein Discrepancy (with Lanya Yang)\nMarkov Decision Processes and How to Solve Them (code)\n\n\n\n\n\nDirichlet Laplacian Eigenfunction Problem\n\n\n\n\n\nMultiple Testing Problem\nResponse-Adaptive Randomisation (code1) (code2)\n\n\n\n\n\nSpectral Theory\nProbability Theory\n\n\n\n\n\nUnivariate Extreme Value Modelling"
  },
  {
    "objectID": "notes.html#slides",
    "href": "notes.html#slides",
    "title": "Notes & Slides",
    "section": "Slides",
    "text": "Slides\n\nLLM for Research\nDouble Descent"
  },
  {
    "objectID": "posts/2024-10-13-basic-GP-regression-formula/index.html",
    "href": "posts/2024-10-13-basic-GP-regression-formula/index.html",
    "title": "[Derivation Scribbles] Basic GP Regression Formula",
    "section": "",
    "text": "Gaussian Process Regression, adapted from https://docs.jaxgaussianprocesses.com/.\n\n\n\nBlock Matrix Inversion\nThe first thing we need to establish is the block matrix inversion identity. Consider an invertible matrix \\(\\Sigma\\) that can be written as\n\\[\n\\Sigma = \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}\n\\]\nwhere \\(\\Sigma_{AA}, \\Sigma_{AB}, \\Sigma_{BA}, \\Sigma_{BB}\\) are matrices of the right dimension and sufficiently non-singular. Next, we have the block matrix inversion identity stated below.\n\\[\n\\begin{split}\n\\Sigma^{-1} &= \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}^{-1} \\\\\n&= \\begin{bmatrix} (\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1}  & -(\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1} \\Sigma_{AB} \\Sigma_{BB}^{-1}\\\\ -\\Sigma_{BB}^{-1} \\Sigma_{BA}(\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1}  & (\\Sigma_{BB} - \\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB})^{-1} \\end{bmatrix}.\n\\end{split}\n\\]\n\n\nMarginal and Conditional Gaussians\nConsider a multivariate Gaussian distribution \\(x = (x_A, x_B)^T\\) where \\(x_A\\) is \\(d_A\\) dimensional, \\(x_B\\) is \\(d_B\\) dimensional, and \\(x\\) is \\(d = d_A + d_B\\) dimensional. The mean vector and covariance matrix of the multivariate Gaussian is set to be as follows:\n\\[\nx = \\begin{bmatrix} x_A \\\\ x_B \\end{bmatrix} \\sim N_d \\left( \\mu, \\Sigma\\right) = N_d \\left( \\begin{bmatrix} \\mu_A \\\\ \\mu_B \\end{bmatrix}, \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}\\right).\n\\]\nIt is easy to notice that the marginal distributions \\(x_A\\) and \\(x_B\\) can be obtained by selecting the needed entries of the above equation, i.e.\n\\[\n\\begin{split}\nx_A &\\sim N_{d_A}(\\mu_A, \\Sigma_{AA}), \\\\\nx_B &\\sim N_{d_B}(\\mu_B, \\Sigma_{BB}).\n\\end{split}\n\\]\nThe conditional distributions are a bit tricky, which we will derive below. Due to symmetry, we will derive the conditional distribution \\(x_A | x_B\\) and just state \\(x_B | x_A\\). Using \\(p(\\cdot)\\) to denote the density of a random variable, we have\n\\[\n\\begin{split}\np(x_A | x_B) &= \\frac{p(x_A, x_B)}{p(x_B)} \\\\ &\\propto \\exp\\left\\{  -\\frac{1}{2} (x - \\mu)^T\\Sigma^{-1}(x - \\mu) \\right\\}.\n\\end{split}\n\\]\nFocusing on the terms inside the second exponential, we first denote\n\\[\n\\Sigma^{-1} = \\begin{bmatrix} V_{AA} & V_{AB} \\\\ V_{BA} & V_{BB} \\end{bmatrix}\n\\]\nwhich then yield\n\\[\n\\begin{split}\n&\\quad  (x - \\mu)^T\\Sigma^{-1}(x - \\mu) \\\\\n&=  \\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix}^T \\begin{bmatrix} V_{AA} & V_{AB} \\\\ V_{BA} & V_{BB} \\end{bmatrix}\\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix}  \\\\\n&= \\begin{bmatrix} (x_A - \\mu_A)^T V_{AA} + (x_B - \\mu_B)^T V_{BA} \\\\ (x_A - \\mu_A)^T V_{AB} + (x_B - \\mu_B)^T V_{BB} \\end{bmatrix}^T\\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix} \\\\\n&= (x_A - \\mu_A)^T V_{AA} (x_A - \\mu_A) + (x_A - \\mu_A)^T V_{AB} (x_B - \\mu_B) \\\\\n&\\quad +  (x_B - \\mu_B)^T V_{BA} (x_A - \\mu_A) + (x_B - \\mu_B)^T V_{BB} (x_B - \\mu_B).\n\\end{split}\n\\]\nWe can keep terms with \\(x_A\\) and put the rest into the normalising constant. As \\(V_{AA}\\) is square and \\(V_{AB}= V_{BA}^T\\), we can simplify our above equation into\n\\[\n\\begin{split}\n&\\quad x_A^T V_{AA} x_A - 2 x_A^T V_{AA} \\mu_A + 2x_A^T V_{AB} (x_B - \\mu_B) \\\\\n&= x_A^T V_{AA} x_A - 2 x_A^T [ V_{AA} \\mu_A +V_{AB} (x_B - \\mu_B)] \\\\\n&= (x_A - \\mu')^T V_{AA}(x_A - \\mu')+ C\n\\end{split}\n\\]\nfor some constant \\(C\\) independent of \\(x_A\\) and the newly defined\n\\[\n\\mu' = \\mu_A - V_{AA}^{-1}V_{AB} (x_B - \\mu_B).\n\\]\nTherefore, using the values of \\(V_{AA}, V_{AB}\\) from the block matrix inversion formula earlier, we have\n\\[\n\\begin{split}\n\\mu' &= \\mu_A - V_{AA}^{-1}V_{AB} (x_B - \\mu_B) \\\\\n&= \\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}(x_B - \\mu_B) \\\\\nV_{AA}^{-1} &= \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA} \\\\\n\\end{split}\n\\]\nand via symmetry, we have the conditional distributions\n\\[\n\\begin{split}\nx_A | x_B &\\sim N_{d_A}(\\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}(x_B - \\mu_B), \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}), \\\\\nx_B | x_A &\\sim N_{d_B}(\\mu_B + \\Sigma_{BA}\\Sigma_{AA}^{-1}(x_A - \\mu_A), \\Sigma_{BB} - \\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB}).\n\\end{split}\n\\]\n\n\nGaussian Process Regression\nConsider we have a single-output Gaussian process \\(f \\sim \\mathcal{GP}(\\mu, k)\\) where \\(\\mu\\) is the mean function and \\(k\\) is the kernel function. The support of this GP is assumed to be \\(\\mathbb{R}^d\\). Consider we have made \\(n\\) observations of this GP \\(f\\) where the observations are made at locations \\(X \\in \\mathbb{R}^n\\) with values \\(y \\in \\mathbb{R}^n\\) and the observations are noisy with independent additive Gaussian noise of variance \\(\\sigma^2\\), i.e. \\(y = f(X) + \\xi\\) with \\(\\xi_i \\sim N(0, \\sigma^2) ~\\forall i = 1, 2, \\ldots, n\\). Denote the existing observations as \\(\\mathcal{D} = \\{ X, y \\}\\).\nUnder our modelling assumptions, we could write down the (log) likelihood of the \\(m\\) observations \\(y\\) under our GP prior \\(f \\sim \\mathcal{GP}(\\mu, k)\\). Since \\(y = f(X) + \\xi\\), we have\n\\[\ny | X \\sim N_n \\left( \\mu(X), k(X, X) + \\sigma^2 I_n \\right)\n\\]\nparamerised by \\(\\theta\\) (e.g. observation noise \\(\\sigma\\), lengthscale and variance of the kernel \\(k\\)) which gives us the following log likelihood\n\\[\n\\log p(y|X) = - \\frac{n}{2}\\log(2\\pi) - \\log | k(X, X) + \\sigma^2 I_n | - \\frac{1}{2} \\left( y - \\mu(X) \\right)^T ( k(X, X) + \\sigma^2 I_n)^{-1}\\left( y - \\mu(X) \\right)\n\\]\nthat we maximise w.r.t. \\(\\theta\\) to obtain the maximum likelihood estimators of the (hyper)parameters.\nNext, conditional on these observations, we wish to know the distributions of the GP at test points \\(X_* \\in \\mathbb{R}^m\\), i.e. the conditional distribution \\(y_* = f(X_*) ~| \\mathcal{D}\\). This can be achieved by first model \\(y_*\\) and \\(y\\) jointly, then condition on \\(y\\). Using the conditional distribution formula above, we denote for simplicity the Gram matrices\n\\[\nK = k(X, X) \\in \\mathbb{R}^{n \\times n}, \\qquad K_* = k(X, X_*) \\in \\mathbb{R}^{n \\times m}, \\qquad K_{**}=k(X_*, X_*) \\in \\mathbb{R}^{m \\times m},\n\\]\nwhich gives us\n\\[\n\\begin{split}\ny_* ~|X_*, \\mathcal{D}, \\sigma^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= \\mu(X_*) + K_*^T (K + \\sigma^2 I_n)^{-1} (y - \\mu(X)),\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1}K_*.\n\\end{split}\n\\]\nIn the common scenario where we assume \\(\\mu = 0\\), we further have the following GP predictive distribution\n\\[\n\\begin{split}\ny_* ~|X_*, \\mathcal{D}, \\sigma^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= K_*^T (K + \\sigma^2 I_n)^{-1} y,\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1}K_*.\n\\end{split}\n\\]\n\n\nComputational Costs\nWe first state the basic computational costs of matrix operations. See here and the links within for more details.\n\n\n\n\n\n\n\n\n\nOperations\nDimensions\nCost\nPotential Tricks\n\n\n\n\nMatrix Inversion\n\\(\\mathbb{R}^{n \\times n}\\)\n\\(O(n^3)\\)\nWoodbury matrix identity\n\n\nMatrix Determinant\n\\(\\mathbb{R}^{n \\times n}\\)\n\\(O(n^3)\\)\nAlmost by-product of matrix inversion when using LU decomposition\n\n\nMatrix Multiplication\n\\(\\mathbb{R}^{n \\times m}, \\mathbb{R}^{m \\times p}\\)\n\\(O(nmp)\\)\n/\n\n\nMatrix Square Root (Cholesky)\n\\(\\mathbb{R}^{n \\times n}\\)\n\\(O(n^3)\\)\n/\n\n\n\nUsing those, we can realise the following computational costs of GP operations.\n\n\n\n\n\n\n\n\nOperations\nSize\nCost\n\n\n\n\nLikelihood Evaluation\n\\(n\\) observations\n\\(O(n^3)\\)\n\n\nPosterior Mean\n\\(n\\) observations, \\(m\\) test points\n\\(O(n^3 + n^2 m)\\)\n\n\nPosterior Covariance\n\\(n\\) observations, \\(m\\) test points\n\\(O(n^3 + n^2 m + n m^2)\\)\n\n\nPosterior Sampling\n\\(n\\) observations, \\(m\\) test points, \\(j\\) samples\n\\(O(n^3 + n^2 m + n m^2 + m^2j)\\)"
  },
  {
    "objectID": "posts/2024-10-31-temporal-GP-2/index.html",
    "href": "posts/2024-10-31-temporal-GP-2/index.html",
    "title": "Spatial-Temporal GP (2)",
    "section": "",
    "text": "In this blog post, I will walk through how one could exploit the Kronecker structure of the temporal Gaussian process (GP) regression with one-dimensional space + one-dimensional time inputs and one-dimensional output. This is the second of a series of blog posts on spatial-temporal Gaussian processes."
  },
  {
    "objectID": "posts/2024-10-31-temporal-GP-2/index.html#footnotes",
    "href": "posts/2024-10-31-temporal-GP-2/index.html#footnotes",
    "title": "Spatial-Temporal GP (2)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrequentist MLE is equivalent to a Bayesian MAP with flat priors.↩︎\nsince we are using only a summary statistic (MAP with flat prior) for the parameters in the posterior predictive, instead of the full marginal posterior.↩︎"
  },
  {
    "objectID": "posts/2024-12-03-3DVar-4DVar/index.html",
    "href": "posts/2024-12-03-3DVar-4DVar/index.html",
    "title": "[Derivation Scribbles] 3D-Var and 4D-Var",
    "section": "",
    "text": "This blog post follows from the previous post on the Kalman filter and ensemble Kalman filter."
  },
  {
    "objectID": "posts/2024-12-03-3DVar-4DVar/index.html#footnotes",
    "href": "posts/2024-12-03-3DVar-4DVar/index.html#footnotes",
    "title": "[Derivation Scribbles] 3D-Var and 4D-Var",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStandard results of gradient descent tells us, since our loss function (negative log density) is convex, convergence is guaranteed when we run the optimiser for long enough.↩︎"
  },
  {
    "objectID": "posts/2025-02-07-EIG-GP/index.html",
    "href": "posts/2025-02-07-EIG-GP/index.html",
    "title": "Expected Information Gain with Gaussian Process Surrogate Models",
    "section": "",
    "text": "Dennis Lindley; Lindley (1956)\n\n\n\nIntroducing Expected Information Gain\nIn Bayesian experiment design (Rainforth et al. 2024), a commonly used utility function is the information gain, where we are comparing the entropy of the distributions before and after observing an addition point. Assuming that our existing data set is denoted by \\(\\mathcal{D}\\) and the posterior distribution is \\(p(\\cdot | \\mathcal{D})\\). If we make an observation at \\(x\\) and observe \\(y\\), our new data set will become \\(\\mathcal{D}^+ := \\mathcal{D} \\cup \\{(x, y)\\}\\). This will then correspond to a new posterior \\(p(\\cdot | \\mathcal{D}^+)\\).\nGiven those, the information gain (IG) is given by:\n\\[\nIG(x) = H(p(\\cdot | \\mathcal{D})) - H(p(\\cdot | \\mathcal{D}^+)).\n\\]\nConsider our distribution is a Gaussian process (GP) with mean zero and kernel \\(k\\), and the posterior is the posterior predictive distribution of this GP on some finite set of test points \\(x_*\\) with size \\(m\\). We also assume the current data set \\(\\mathcal{D} := \\{(x_i, y_i)\\}_{i=1}^{n}\\) is of size \\(n\\) and the observations with additive, centered, independent Gaussian noise of variance \\(\\sigma^2\\).\nWe will also use the following notations to denote the various Gram matrices using kernel \\(k\\)\n\n\\(K = k(X,X)\\), size \\(n \\times n\\).\n\\(K_* = k(X, x_*)\\), size \\(n \\times m\\).\n\\(K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\nThe posterior is therefore (see here for a detailed derivation)\n\\[\n\\begin{split}\np(y^* | x^*, \\mathcal{D}, \\sigma^2) &\\sim \\mathcal{N}(\\mu_{y^*|\\mathcal{D}}, \\Sigma_{y^*|\\mathcal{D}}) \\\\\n&\\mu_{y^*|\\mathcal{D}} = K_*^T (K + \\sigma^2 I_n)^{-1} y\\\\\n&\\Sigma_{y^*|\\mathcal{D}} = K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*.\n\\end{split}\n\\]\nAfter adding a new observation at \\(x\\), we will have an updated dataset \\(\\mathcal{D}^+\\) with \\(X^+ = X \\cup \\{x\\}\\) and have an updated posterior using the following Gram matrices\n\n\\(K^+ = k(X^+,X^+)\\), size \\((n+1) \\times (n+1)\\).\n\\(K_*^+ = k(X^+, x_*)\\), size \\((n+1) \\times m\\).\n\\(K_{**}^+ = K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\nSo, the updated posterior’s covariance matrix is\n\\[\n\\Sigma_{y^*|\\mathcal{D}^+} = K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\n\\]\nThus, the information gain can be written as\n\\[\nIG(x) = H(p(\\cdot | \\mathcal{D})) - H(p(\\cdot | \\mathcal{D}^+))\n\\] where using the definition of the entropy of multivariate Gaussian yields\n\\[\n\\begin{split}\nIG(x) &= \\frac{1}{2} \\log \\det \\Sigma_{y^*|\\mathcal{D}} - \\frac{1}{2} \\log \\det \\Sigma_{y^*|\\mathcal{D}^+} \\\\\n&= \\frac{1}{2} \\log \\det \\Big( K_{**} - K_*^T (K + \\delta^2 I_n)^{-1} K_* \\Big) - \\frac{1}{2} \\log \\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big)\n\\end{split}\n\\] Since \\(IG(x)\\) is independent of \\(y | x\\), the acquisition function expected information gain (EIG) is therefore\n\\[\nEIG(x) = \\mathbb{E}_{y}[IG(x)] = IG(x)\n\\] Furthermore, we can remove several terms when we do \\(\\arg\\max_x\\) for the acquisition function optimisation, and get\n\\[\nEIG(x) = - \\log \\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big).\n\\]\nIn the current setup, the information gain is tractable due to nice properties of multivariate Gaussians and GP regression conjugacies. Albeit tractable, the immediate formulation of the expected information gain has undesirable computational costs which we will elaborate below. After a preliminary attempt to reformulate EIG in order to reduce the computation cost, we will present a different perspective of EIG using mutual information, which enables an EIG formulation with low computational costs.\n\n\nEIG Reformualtion - a first attempt\nWe will consider the naive computation of the above \\(EIG(x)\\) expression. One should note that in the active learning settings, we would often be in the scenarios where \\(m &gt;&gt; n\\). An improved approach of computing the same quantity is presented below, leveraging the matrix determinant lemma.\n\nNaive Implementation\nWe wish to compute\n\\[\nEIG(x) = - \\log\\det \\Big( K_{**}^+ - K_*^{+T} (K^+ + \\sigma^2 I_{n+1})^{-1} K_*^+\\Big).\n\\]\n\n\n\n\n\n\n\n\nOrder\nExpression\nCost\n\n\n\n\n1\n\\((K^+ + \\delta^2 I_{n+1})^{-1}\\)\n\\(O((n+1)^3)\\)\n\n\n2\n\\((K^+ + \\delta^2 I_{n+1})^{-1} K_*^+\\)\n\\(O((n+1)^2 m)\\)\n\n\n3\n\\(K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1}K_*^+\\)\n\\(O(m (n+1)^2)\\)\n\n\n4\n\\(K^+ - K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1} K_*^+\\)\n\\(O(m^2)\\)\n\n\n5\n\\(-\\log\\det\\big(K_{**}^+ - K_*^{+T} (K^+ + \\delta^2 I_{n+1})^{-1} K_*^+ \\big)\\)\n\\(O({\\color{red}m^3})\\)\n\n\n\nSo the cost is\n\\[\nO((n+1)^3 + (n+1)^2 m + m^2 (n+1) + m^2 + {\\color{red}m^3}).\n\\] We will need to compute the above quantity \\(m\\) times for comparison \\(\\arg\\max_x\\), thus the full costs is\n\\[\nO((n+1)^3m + (n+1)^2 m^2 + m^3 (n+1) + m^3 + {\\color{red}m^4}).\n\\]\n\n\nNontrivial Implementation\nWe use the matrix determinant identity:\n\\[\n\\det(A + UWV^T) = \\det(A) \\det(W) \\det(W + V^T A^{-1} U)\n\\]\nwhere here\n\n\\(A = K_{**}^+\\)\n\\(U = -K_*^{+T}\\)\n\\(W = (K^+ + \\sigma^2 I_{n+1})^{-1}\\)\n\\(V = K_*^+\\)\n\nThus, we wish to compute\n\\[\nEIG(x) = -\\log \\left[ \\det(K_{**}^+) \\cdot 1/ \\det(K^+ + \\sigma^2 I_{n+1}) \\cdot \\det \\big(K^+ + \\sigma^2 I_{n+1}- K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big) \\right]\n\\]\nSince \\(K_{**}\\) is positive semi-definite, its determinant is always non-negative so we can ignore it in comparisons.\n\n\n\n\n\n\n\n\nOrder\nExpression\nCost\n\n\n\n\n1\n\\(K_{**}^+ + \\sigma^2 I_{n+1}\\)\n\\(O((n+1)^2)\\)\n\n\n2\n\\(\\det(K^+ + \\sigma^2 I_{n+1})\\)\n\\(O((n+1)^3)\\)\n\n\n3\n\\((K_{**}^+)^{-1}\\)\n\\(O(m^3)\\), reusable\n\n\n4\n\\((K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O(m^2(n+1))\\)\n\n\n5\n\\(K_*^+ (K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O(m(n+1)^2)\\)\n\n\n6\n\\((K^+ + \\sigma^2 I_{n+1}) - K_*^+ (K_{**}^+)^{-1}K_*^{+T}\\)\n\\(O((n+1)^2)\\)\n\n\n7\n\\(\\det\\big( (K^+ + \\sigma^2 I_{n+1}) - K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big)\\)\n\\(O((n+1)^3)\\)\n\n\n8\n\\(\\log \\left[ 1/ \\det(K^+ + \\sigma^2 I_{n+1}) \\cdot \\det \\big(K^+ + \\sigma^2 I_{n+1}- K_*^+ (K_{**}^+)^{-1}K_*^{+T} \\big) \\right]\\)\n\\(O(1)\\)\n\n\n\nSo the cost is \\[\nO((n+1)^2 + (n+1)^3 + {\\color{blue}m^3} + m^2(n+1) + m(n+1)^2).\n\\]\nWe will need to compute the above quantity \\(m\\) times for comparison \\(\\arg\\max_x\\), thus the full costs is\n\\[\nO((n+1)^2m + (n+1)^3m + {\\color{blue}m^3} + m^3(n+1) + m^2(n+1)^2).\n\\]\n\n\n\nEIG Reformualtion - a second attempt\n\nEIG over \\(n\\) Observations\nInstead of the one-step EIG update (the difference in entropies between the posteriors with and without an additional observation), below we derive the EIG of the entirety of \\(n\\) observations. This quantity could be used as the objective for non-myopic policies, such as the case of deep adaptive designs (Foster et al. 2021).\nConsider we have the prior (a GP with kernel \\(k\\)) \\(p(\\cdot)\\) and we have \\(n\\) observations \\(\\mathcal{D} = \\{(x_i, y_i)\\}_{i=1}^n =: \\{ (\\boldsymbol{x}, \\boldsymbol{y})\\}\\) which yields the posterior \\(p(\\cdot | \\mathcal{D})\\), the information gain quantity of interest would be\n\\[\nIG(\\boldsymbol{x}) = H(p(\\cdot)) - H(p(\\cdot | \\mathcal{D})) = MI(p(\\cdot); \\boldsymbol{y})\n\\] where the last equality follows from the definition of mutual information.\nAgain, if we consider those GPs on a fixed, finite set of test points \\(x_*\\) like before, we would be able to show the following:\n\\[\n\\begin{split}\nIG(\\boldsymbol{x}) &= H(p(\\cdot)) - H(p(\\cdot | \\mathcal{D})) \\\\\n&= \\frac{1}{2} \\log \\det K_{**} - \\frac{1}{2} \\log \\det \\left[ K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*\\right] \\\\\n&= - \\frac{1}{2} \\log \\det \\left[K_{**}^{-1}( K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1} K_*)\\right] \\\\\n&= - \\frac{1}{2} \\log \\det \\left[I_m - K_{**}^{-1} K_*^T (K + \\sigma^2 I_n)^{-1} K_*)\\right] \\\\\n\\end{split}\n\\] where, as before, we use the shorthand notations\n\n\\(K = k(X,X)\\), size \\(n \\times n\\).\n\\(K_* = k(X, x_*)\\), size \\(n \\times m\\).\n\\(K_{**} = k(x_*, x_*)\\), size \\(m \\times m\\).\n\n\n\nLow Cost EIG Formulation\nThe above computation is at least cubic in \\(m\\) due to the determinant operation. In fact, using the symmetric property of the mutual information, we could obtain a much better expression.\nNote that \\[MI(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X).\\] We first denote the prior as \\(f\\), the observations \\(\\boldsymbol{y_A}\\) at locations \\(\\boldsymbol{x_A}\\) with observational noise \\(\\boldsymbol{\\varepsilon}\\) so \\(\\boldsymbol{y_A} = f(\\boldsymbol{x_A}) + \\boldsymbol{\\varepsilon}\\).\nThe information gain from prior to posterior after observing \\(\\boldsymbol{y_A}\\) can be written as the mutual information\n\\[\nIG(\\boldsymbol{x_A}) = H(f) - H(f |\\boldsymbol{y_A}) = MI(f; \\boldsymbol{y_A}) = H(\\boldsymbol{y_A}) - H(\\boldsymbol{y_A} | f).\n\\]\nNotice that since \\(y_A = f(\\boldsymbol{x_A}) + \\boldsymbol{\\varepsilon}\\), it is a multivariate with covariance \\(K(\\boldsymbol{x_A}, \\boldsymbol{x_A}) + \\sigma^2 I\\). In addition, \\(\\boldsymbol{y_A} | f\\) has covariance being just \\(\\sigma^2 I\\). Therefore, we have\n\\[\n\\begin{split}\nIG(\\boldsymbol{x_A}) &= H(\\boldsymbol{y_A}) - H(\\boldsymbol{y_A} | f) \\\\\n&= \\log \\det (K(\\boldsymbol{x_A}, \\boldsymbol{x_A}) + \\sigma^2 I) - \\log \\det (\\sigma^2 I) \\\\\n&= \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_A}, \\boldsymbol{x_A}))\n\\end{split}\n\\]\nwhich is the expression used in Section 2.2 of Srinivas et al. (2010), and is computationally cheap.\nUsing the same concept, we can rewrite the EIG of posteriors between \\(\\mathcal{D} = \\{ (\\boldsymbol{x_A}, \\boldsymbol{y_A}) \\}\\) and \\(\\mathcal{D}^+ = \\{ (\\boldsymbol{x_B}, \\boldsymbol{y_B}) \\}\\) (i.e. subject to one more observation). We have the information gain\n\\[\n\\begin{split}\nIG(x) &= H(f | \\boldsymbol{y_A}) - H(f | \\boldsymbol{y_B}) \\\\\n&= - H(f) + H(f | \\boldsymbol{y_A}) + H(f) - H(f | \\boldsymbol{y_B}) \\\\\n&= - [H(f) - H(f | \\boldsymbol{y_A})] + [H(f) - H(f | \\boldsymbol{y_B})] \\\\\n&= - [IG(\\boldsymbol{x_A}) ] + [IG(\\boldsymbol{x_B})] \\\\\n&= - \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_A}, \\boldsymbol{x_A})) + \\log \\det (I + \\sigma^{-2} K(\\boldsymbol{x_B}, \\boldsymbol{x_B})).\n\\end{split}\n\\]\nNotice that the first term is the same when comparing across different \\(x\\), thus can be omitted. This formulation’s cost is therefore\n\\[\nO((n+1)^2 + (n+1)^3)\n\\]\nfor one-time computation and the overall cost for comparison \\(\\arg\\max_x\\) is\n\\[\nO(m(n+1)^2 + m(n+1)^3).\n\\]\nOne should note that similar rewriting of entropy-related objectives using the symmetry of mutual information also exist in the Bayesian optimization literature with the entropy search and the predictive entropy search (e.g. Hernández-Lobato, Hoffman, and Ghahramani (2014)).\n\n\n\n\n\n\nReferences\n\nFoster, Adam, Desi R Ivanova, Ilyas Malik, and Tom Rainforth. 2021. “Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design.” In International Conference on Machine Learning, 3384–95. PMLR.\n\n\nHernández-Lobato, José Miguel, Matthew W Hoffman, and Zoubin Ghahramani. 2014. “Predictive Entropy Search for Efficient Global Optimization of Black-Box Functions.” Advances in Neural Information Processing Systems 27.\n\n\nLindley, Dennis V. 1956. “On a Measure of the Information Provided by an Experiment.” The Annals of Mathematical Statistics 27 (4): 986–1005.\n\n\nRainforth, Tom, Adam Foster, Desi R Ivanova, and Freddie Bickford Smith. 2024. “Modern Bayesian Experimental Design.” Statistical Science 39 (1): 100–114.\n\n\nSrinivas, Niranjan, Andreas Krause, Sham Kakade, and Matthias Seeger. 2010. “Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design.” In Proceedings of the 27th International Conference on Machine Learning, 1015–22. Omnipress."
  },
  {
    "objectID": "posts/2025-06-18-gp-identifiability/index.html",
    "href": "posts/2025-06-18-gp-identifiability/index.html",
    "title": "Identifiability Issues of Gaussian Processes",
    "section": "",
    "text": "Gaussian Processes (GPs) are central to spatial statistics and nonparametric modeling, offering a principled way to model spatial dependence. The Matérn kernel, in particular, provides a flexible class of covariance functions that control both the range and smoothness of spatial correlation.\nHowever, the identifiability of GP parameters is an important issue sometimes overlooked in practice. Here, we will explore the existing literature on the consistency and identifiability of Matérn GP parameters."
  },
  {
    "objectID": "posts/2025-06-18-gp-identifiability/index.html#introduction",
    "href": "posts/2025-06-18-gp-identifiability/index.html#introduction",
    "title": "Identifiability Issues of Gaussian Processes",
    "section": "",
    "text": "Gaussian Processes (GPs) are central to spatial statistics and nonparametric modeling, offering a principled way to model spatial dependence. The Matérn kernel, in particular, provides a flexible class of covariance functions that control both the range and smoothness of spatial correlation.\nHowever, the identifiability of GP parameters is an important issue sometimes overlooked in practice. Here, we will explore the existing literature on the consistency and identifiability of Matérn GP parameters."
  },
  {
    "objectID": "posts/2025-06-18-gp-identifiability/index.html#the-matérn-kernel-and-parameters",
    "href": "posts/2025-06-18-gp-identifiability/index.html#the-matérn-kernel-and-parameters",
    "title": "Identifiability Issues of Gaussian Processes",
    "section": "1. The Matérn Kernel and Parameters",
    "text": "1. The Matérn Kernel and Parameters\nThe Matérn covariance function is defined as:\n\\[\nk_\\nu(h) = \\sigma^2 \\cdot \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)} \\left( \\frac{h}{l} \\right)^\\nu K_\\nu\\left( \\frac{h}{l} \\right)\n\\]\nwhere:\n\n\\(\\sigma^2\\): kernel variance\n\\(l\\):lengthscale\n\\(\\nu\\): smoothness (higher \\(\\nu\\) implies smoother sample paths)\n\\(h\\): distance\n\\(K_\\nu\\): modified Bessel function of the second kind\n\nFurthermore, we can consider the Fourier transform of the above covariance function and obtain the Matérn spectral density function, given by\n\\[\nS(u) = C \\frac{\\sigma^2 l^{-2\\nu}}{(l^{-2} + u^2)^{\\nu + d/2}}\n\\]\nfor some constant \\(C &gt; 0\\) where \\(d\\) is the dimension of the stochastic process."
  },
  {
    "objectID": "posts/2025-06-18-gp-identifiability/index.html#asymptotic-frameworks",
    "href": "posts/2025-06-18-gp-identifiability/index.html#asymptotic-frameworks",
    "title": "Identifiability Issues of Gaussian Processes",
    "section": "2. Asymptotic Frameworks",
    "text": "2. Asymptotic Frameworks\nWhen considering the asymptotic parameter estiamtions, we often let the number of observations go to infinity. In the context of spatial statistics, there are two cases of this limiting behavior: fixed-domain and increasing-domain. Futhermore, it is standard in the literature to assume that the underlying spatial field is fixed throughout the sampling process (i.e. we can always making observations about the same spatial field sample).\n\nFixed-Domain Asymptotics\nAlso called infill asymptotics. The domain of interest remains fixed (e.g., \\([0,1]^d\\)), while the number of observations increases within that domain.\n\nCommon in geostatistics when the spatial extent is constrained (e.g., environmental sampling).\nThe GP becomes more densely sampled, but still only a single realization is observed.\n\n\n\nIncreasing-Domain Asymptotics\nThe spatial domain grows (e.g., from \\([0,1]^2\\) to \\([0, L]^2\\)), while maintaining fixed sampling density.\n\nAppropriate in large-scale spatial surveys (e.g., national or continental datasets).\nProvides more information about the long-range behavior of the process."
  },
  {
    "objectID": "posts/2025-06-18-gp-identifiability/index.html#identifiability-and-equivalence-of-gaussian-measures",
    "href": "posts/2025-06-18-gp-identifiability/index.html#identifiability-and-equivalence-of-gaussian-measures",
    "title": "Identifiability Issues of Gaussian Processes",
    "section": "3. Identifiability and Equivalence of Gaussian Measures",
    "text": "3. Identifiability and Equivalence of Gaussian Measures\nWe will restrict ourselves to the fixed-domain asymptotic setting and examine the identifiability issue under this regime. The key theoretical tool we use is the equivalence between probability measures.\nConsider two probability measures \\(P_1, P_2\\) defined on the same probability space \\((\\Omega, \\mathcal{F})\\). We say the measure \\(P_1\\) is absolutely continuous w.r.t. \\(P_2\\) if \\(P_2(A) = 0 \\implies P_1(A) = 0\\) for all \\(A \\in \\mathcal{F}\\), denoted by \\(P_1 \\ll P_2\\). We also say \\(P_1\\) is equivalent to \\(P_2\\) if we have \\(P_1 \\ll P_2\\) and \\(P_1 \\gg P_2\\).\nThe statistical implications of equivalent measures are: (1) we cannot claim with probability one samples from any of the equivalent measure are from which of the measures, (2) if the equivalent measures is a family parameterised by \\(\\theta \\in \\Theta\\), we cannot consistently estimate all \\(\\theta\\), (3) for equivalent measures, the prediction of a new random variable condition on the same list of random variables agree as the list increases to infinity. Thus, roughly speaking, if the measures are equivalent, we cannot estimate parameters consistently, yet they should yield the same predictions.\nWe denote \\(P_{\\sigma, l}\\) to be the Gaussian measure for a Matérn GP of smoothness parameter \\(\\nu\\) with variance \\(\\sigma^2\\) and lengthscale \\(l\\). It turns out that, two such measures \\(P_{\\sigma_1, l_1}, P_{\\sigma_2, l_2}\\) are equivalent if and only if \\(\\sigma_1^2 / l_1^{2\\nu} = \\sigma_2^2 / l_2^{2\\nu}\\) (Zhang (2004), Stein (2004)).\nThis implies that we cannot consistently estimate \\(\\sigma\\) or \\(l\\), yet we can consistently estimate the microergodic parameter \\(\\sigma_2^2 / l^{2\\nu}\\). Additionally, despite the lack of identifiabilities for some parameters, interpolation and predictions remains feasible.\nThe result above is obtained by a sufficient condition of Gaussian measure equivalence due to Stein (1999), which poses a condition based on the spectral densities of the two measures. The result of Zhang (2004) was established by checking this condition. It should not be too surprising then to accept the critical importance of \\(\\sigma_2^2 / l^{2\\nu}\\) by looking at its role in the spectral density of a Matérn kernel.\n\n\n\nSimulation results on the estimation of parameters of a Matern kernel with increasing number of observations."
  },
  {
    "objectID": "posts/2025-06-18-gp-identifiability/index.html#nugget",
    "href": "posts/2025-06-18-gp-identifiability/index.html#nugget",
    "title": "Identifiability Issues of Gaussian Processes",
    "section": "4. Nugget",
    "text": "4. Nugget\nThe observation noise of a spatial model is often known as the nugget in the literature. The result above does not assume the observations are made with noise. The recent work of Tang, Zhang, and Banerjee (2021) extends many of the previous consistency and identifiability results to the case where nuggets occur. The qualitative behaviour remains under this more general setting, yet the asymptotic normality of the maximum likelihood estimator of the microergodic parameter has a different convergence rate: for observation number \\(n\\), without the nugget the rate is \\(n^{1/2}\\) and with the nugget the rate is \\(n^{1/(2+4\\nu / d)}\\)."
  },
  {
    "objectID": "posts/2025-06-18-gp-identifiability/index.html#visualizing-the-likelihood-surface",
    "href": "posts/2025-06-18-gp-identifiability/index.html#visualizing-the-likelihood-surface",
    "title": "Identifiability Issues of Gaussian Processes",
    "section": "5. Visualizing the Likelihood Surface",
    "text": "5. Visualizing the Likelihood Surface\nWe now visualize likelihoods under noiseless and noisy observations. Both surfaces indicates the non-identifiability of \\(\\sigma\\) and \\(l\\)."
  },
  {
    "objectID": "posts/2025-06-18-gp-identifiability/index.html#summary",
    "href": "posts/2025-06-18-gp-identifiability/index.html#summary",
    "title": "Identifiability Issues of Gaussian Processes",
    "section": "6. Summary",
    "text": "6. Summary\n\n\n\n\n\n\n\n\nScenario\nIdentifiable Parameters\nKey Results\n\n\n\n\nNoiseless, fixed-domain\nOnly \\(\\sigma^2/l^{2\\nu}\\)\nZhang (2004)\n\n\nNoisy, fixed-domain\n\\(\\sigma^2/l^{2\\nu}\\), \\(\\tau^2\\)\nTang, Zhang, and Banerjee (2021)\n\n\nIncreasing-domain\nAll parameters\nStandard asymptotics apply\n\n\n\nAs a side remark, we are always working in the setting where the spatial field is fixed throughout the sampling. The following simulation result indicates that we may have identifiable parameters for different spatial field samples.\n\n\n\nSimulation results on the estimation of parameters of a Matern kernel with increasing number of observations with different spatial field each run."
  },
  {
    "objectID": "posts/2025-07-11-deep-learning-generalisation/index.html",
    "href": "posts/2025-07-11-deep-learning-generalisation/index.html",
    "title": "[Reading Notes] Deep Learning is Not So Mysterious or Different",
    "section": "",
    "text": "The ICML 2025 paper Deep Learning is Not So Mysterious or Different by Wilson (Wilson 2025) reviews literature on the generalization properties of overparameterized neural networks and argues that soft inductive bias is a key concept for understanding their generalization behavior.\n\n\nDouble descent and benign overfitting are phenomena observed in neural networks, where generalization (i.e. prediction performance on the test set) improves as the number of model parameters increases—even after the model perfectly fits the training data. The classical double descent curve, illustrated below, shows test error decreasing, then increasing, and then decreasing again as model capacity grows.\n\n\n\nDouble Descent. Extracted from Figure 1 of Schaeffer et al. (2024).\n\n\nThe left side of the curve aligns with classical statistical learning theory: increasing model complexity (e.g. through more parameters) initially reduces bias but eventually leads to overfitting and increased test error. However, contrary to this traditional view, further increasing model capacity can improve generalization, a phenomenon that challenges the old bias–variance tradeoff.\n\n\n\nInductive bias refers to assumptions or constraints that restrict the model class within the broader universe of possible functions. For example, linear regression imposes a hard inductive bias by limiting the hypothesis space to linear functions, thereby excluding polynomial or other nonlinear models. Penalized regression methods, such as LASSO, further modify the model class by emphasizing sparsity, imposing what can be viewed as a soft inductive bias—they prefer simpler models but do not outright exclude complex ones.\n\n\n\nPictorial comparison of hard and soft inductive biases. Extracted from Figure 3 of Wilson (2025).\n\n\n\n\n\nA central point in Wilson (2025) is that model complexity is not necessarily tied to the number of parameters. An overparameterized model can still be “simple” in an information-theoretic or geometric sense. One useful metric is the effective dimension of a matrix, defined as:\n\\[\nN_\\text{eff}(A) = \\sum_i \\frac{\\lambda_i}{\\lambda_i + \\alpha},\n\\] where \\(\\lambda_i\\) are the eigenvalues of matrix \\(A\\), and \\(\\alpha\\) is a regularization parameter. Intuitively, a full matrix may still have low effective dimension (e.g. if most eigenvalues are small), while a sparse matrix could have higher effective dimension. Thus, counting parameters alone does not reliably reflect model complexity.\n\n\n\nThe central claims of Wilson (2025) can be summarized as follows:\n\nA model with more parameters may be simpler under appropriate complexity measures (e.g. effective dimension or compressibility).\nSoft inductive bias steers the model toward simpler solutions within a rich hypothesis space.\nNeural networks naturally impose soft inductive biases, increasingly so as their size grows.\n\nMoreover, generalization can be rigorously captured by PAC-Bayes and countable hypothesis bounds, which upper bound the expected risk as the sum of empirical risk and a complexity penalty—often expressed as model compressibility or description length. This theoretical framing accommodates models with millions or billions of parameters."
  },
  {
    "objectID": "posts/2025-07-11-deep-learning-generalisation/index.html#summary",
    "href": "posts/2025-07-11-deep-learning-generalisation/index.html#summary",
    "title": "[Reading Notes] Deep Learning is Not So Mysterious or Different",
    "section": "",
    "text": "The ICML 2025 paper Deep Learning is Not So Mysterious or Different by Wilson (Wilson 2025) reviews literature on the generalization properties of overparameterized neural networks and argues that soft inductive bias is a key concept for understanding their generalization behavior.\n\n\nDouble descent and benign overfitting are phenomena observed in neural networks, where generalization (i.e. prediction performance on the test set) improves as the number of model parameters increases—even after the model perfectly fits the training data. The classical double descent curve, illustrated below, shows test error decreasing, then increasing, and then decreasing again as model capacity grows.\n\n\n\nDouble Descent. Extracted from Figure 1 of Schaeffer et al. (2024).\n\n\nThe left side of the curve aligns with classical statistical learning theory: increasing model complexity (e.g. through more parameters) initially reduces bias but eventually leads to overfitting and increased test error. However, contrary to this traditional view, further increasing model capacity can improve generalization, a phenomenon that challenges the old bias–variance tradeoff.\n\n\n\nInductive bias refers to assumptions or constraints that restrict the model class within the broader universe of possible functions. For example, linear regression imposes a hard inductive bias by limiting the hypothesis space to linear functions, thereby excluding polynomial or other nonlinear models. Penalized regression methods, such as LASSO, further modify the model class by emphasizing sparsity, imposing what can be viewed as a soft inductive bias—they prefer simpler models but do not outright exclude complex ones.\n\n\n\nPictorial comparison of hard and soft inductive biases. Extracted from Figure 3 of Wilson (2025).\n\n\n\n\n\nA central point in Wilson (2025) is that model complexity is not necessarily tied to the number of parameters. An overparameterized model can still be “simple” in an information-theoretic or geometric sense. One useful metric is the effective dimension of a matrix, defined as:\n\\[\nN_\\text{eff}(A) = \\sum_i \\frac{\\lambda_i}{\\lambda_i + \\alpha},\n\\] where \\(\\lambda_i\\) are the eigenvalues of matrix \\(A\\), and \\(\\alpha\\) is a regularization parameter. Intuitively, a full matrix may still have low effective dimension (e.g. if most eigenvalues are small), while a sparse matrix could have higher effective dimension. Thus, counting parameters alone does not reliably reflect model complexity.\n\n\n\nThe central claims of Wilson (2025) can be summarized as follows:\n\nA model with more parameters may be simpler under appropriate complexity measures (e.g. effective dimension or compressibility).\nSoft inductive bias steers the model toward simpler solutions within a rich hypothesis space.\nNeural networks naturally impose soft inductive biases, increasingly so as their size grows.\n\nMoreover, generalization can be rigorously captured by PAC-Bayes and countable hypothesis bounds, which upper bound the expected risk as the sum of empirical risk and a complexity penalty—often expressed as model compressibility or description length. This theoretical framing accommodates models with millions or billions of parameters."
  },
  {
    "objectID": "posts/2025-07-11-deep-learning-generalisation/index.html#loose-thoughts",
    "href": "posts/2025-07-11-deep-learning-generalisation/index.html#loose-thoughts",
    "title": "[Reading Notes] Deep Learning is Not So Mysterious or Different",
    "section": "Loose Thoughts",
    "text": "Loose Thoughts\n\nRegression Penalty\nIn Section 2 of Wilson (2025), a polynomial regression model \\(f(x, w) = \\sum_j w_j x^j\\) is trained with a loss function:\n\\[\nL(w) = - \\log p(y|f(x,w)) + \\sum_j \\gamma^j w_j^2, \\qquad \\gamma &gt; 1.\n\\] This introduces a regularization term that penalizes higher-order terms exponentially more, encouraging simpler functions even within a flexible function space. This differs from standard L1 (LASSO) or L2 (Ridge) regularization, which treat all coefficients uniformly. Conceptually, this approach resembles kernel ridge regression, where the RKHS norm acts as a complexity penalty. The analogy to penalizing higher-order terms can likely be formalized via the spectral interpretation of the RKHS norm.\n\n\nSoft vs. Hard Inductive Bias\nA recurring theme of the paper is that soft inductive bias is often preferable to hard inductive bias, particularly in the context of neural networks. This raises interesting questions in the domain of physics-informed machine learning (PIML). For instance, Physics-Informed Neural Networks (PINNs) apply soft physical constraints via collocation points, while operator learning methods (e.g. DeepONets or FNOs) often encode more rigid physical assumptions—effectively imposing harder inductive biases. There are also approaches that encode PDE structure directly into GP kernels.\nIt would be fruitful to explore how varying degrees of inductive bias softness influence generalization and extrapolation in PIML. For example, do soft constraints help in the presence of approximate symmetries, while hard constraints work better in strictly governed physical regimes?"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "I am broadly interested in the decision aspects of Bayesian computational statistics and machine learning. In particular, my research focuses on developing sequential Bayesian designs – such as active learning and Bayesian optimization – for a wide range of application areas including probabilistic numerics and ocean engineering. I am also exploring what post-Bayesian ideas could offer for more robust, efficient decision-making."
  },
  {
    "objectID": "research.html#publication",
    "href": "research.html#publication",
    "title": "Research",
    "section": "Publication",
    "text": "Publication\nGoogle Scholar\n\nPreprints\n\nZhang, R. Y., Moss, H. B., Astfalck, L. C., Cripps, E. J., and Leslie, D. S. (2025). BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories under Spatio-Temporal Vector Fields. [arxiv] [code] [talk] (submitted)\n\n\n\nPublications and Workshop Papers\n\nIguchi, Y., Livingstone, S., Nüsken, N., Vasdekis, G., & Zhang, R. Y. (2025+). Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift: an unadjusted Barker algorithm. IMA Journal of Numeical Analysis. [arxiv] [code] (forthcoming)\nZhang, R. Y., Moss, H. B., Astfalck, L. C., Cripps, E. J., and Leslie, D. S. (2024). BALLAST: Bayesian Active Learning with Look-ahead Amendment for Sea-drifter Trajectories. NeurIPS 2024 Workshop on Bayesian Decision-making and Uncertainty. [OpenReview] [slides] [talk]"
  },
  {
    "objectID": "posts/2025-08-06-mc-qmc-rqmc/index.html",
    "href": "posts/2025-08-06-mc-qmc-rqmc/index.html",
    "title": "Monte Carlo, Antithetic, and Quasi Monte Carlo",
    "section": "",
    "text": "Consider the standard Monte Carlo integration problem of\n\\[\nI = \\int f(x) dq(x) = \\int f(x) q(x) dx = \\mathbb{E}_{X \\sim q}[f(X)]\n\\]\nwhere \\(q\\) is the density of the random variable \\(X\\), \\(f\\) is a function with bounded variance (i.e. \\(\\int (f-I)^2 q(x) dx = \\sigma^2 &lt; \\infty\\)), and we try to estimate \\(I\\) using samples of \\(X\\).\nLet \\(D\\) denote the dimension of the random variable \\(X\\). This would also be the dimension of the intergral \\(I\\).\nFor any estimate \\(\\hat{I}\\) of \\(I\\), we will consider the mean squared error (MSE) of the estimate, i.e. \\(\\mathbb{E}[(\\hat{I} - I)^2]\\), for its capture of both bias and variance of the estimate as it decomposes into the sum of the variance and the squared bias of the estimator."
  },
  {
    "objectID": "posts/2025-08-06-mc-qmc-rqmc/index.html#monte-carlo",
    "href": "posts/2025-08-06-mc-qmc-rqmc/index.html#monte-carlo",
    "title": "Monte Carlo, Antithetic, and Quasi Monte Carlo",
    "section": "Monte Carlo",
    "text": "Monte Carlo\nThe Monte Carlo method draws \\(N\\) i.i.d. samples \\(X_1, X_2, \\ldots, X_N\\) from \\(X\\) and compute the Monte Carlo approximate of \\(I\\) as follows,\n\\[\n\\hat{I}_N = \\frac{1}{N}\\sum_{i=1}^N f(X_i).\n\\]\nFirst, it is immediate from the law of large numbers that \\(\\hat{I}_N\\) is unbiased, i.e. \\(\\mathbb{E}[\\hat{I}_N - I] = 0\\).\nWe can compute the MSE of the Monte Carlo estimate \\(\\hat{I}_N\\) with \\(N\\) i.i.d. samples following the derivations below.\n\\[\n\\begin{split}\n\\text{MSE}(\\hat{I}_N) &= \\text{Var} [\\hat{I}_N] + \\mathbb{E} [\\hat{I}_N - I]^2 \\\\\n&= \\text{Var} \\left[ \\frac{1}{N} \\sum_{i=1}^N f(X_i) \\right] \\\\\n&= \\frac{1}{N^2} \\cdot N \\cdot \\text{Var} [f(X_i)] \\\\\n&= \\frac{\\sigma^2}{N}\n\\end{split}\n\\] where we used the fact that the sample are i.i.d. at the third equal sign.\nTherefore, the Monte Carlo estimate is consistent, as setting \\(N \\to \\infty\\) drives the MSE to zero and the estimate is unbiased. Note that there is no dependency of MSE on the dimension \\(D\\)."
  },
  {
    "objectID": "posts/2025-08-06-mc-qmc-rqmc/index.html#antithetic",
    "href": "posts/2025-08-06-mc-qmc-rqmc/index.html#antithetic",
    "title": "Monte Carlo, Antithetic, and Quasi Monte Carlo",
    "section": "Antithetic",
    "text": "Antithetic\nConsider two random variables \\(X_1, X_2\\) that are identically distribution with unknown (for now) correlation. Let \\(\\mu, h^2\\) be the mean and variance of \\(X_1, X_2\\) and WLOG we let \\(\\mu = 0\\).\nThe average \\((X_1 + X_2)/2\\) is certainly \\(\\mu\\), and the variance of the average is given by\n\\[\n\\begin{split}\n\\text{Var} \\left[\\frac{X_1 + X_2}{2}  \\right] &= \\frac{1}{4} \\left(  \\text{Var}[X_1] + \\text{Var}[X_2] + 2\\text{Cov}(X_1, X_2) \\right) \\\\\n&= \\frac{1}{4} \\left(  2h^2 + 2\\sqrt{\\text{Var}[X_1] \\text{Var}[X_2]}\\text{Corr}(X_1, X_2) \\right) \\\\\n&=\\frac{1}{4} \\left(  2h^2 + 2h^2 \\text{Corr}(X_1, X_2)  \\right) \\\\\n&= \\frac{h^2}{2} (1 + \\text{Corr}(X_1, X_2) )\n\\end{split}\n\\]\nwhich means the average’s variance is minimised when \\(X_1, X_2\\) are perfectly negatively correlated (e.g. \\(X_1 = -X_2\\)). Such pair of random variables is known as the antithetic pair.\nWhen we then use the random variables (or samples) for Monte Carlo estimation, assuming we have samples \\(X_1, X_2\\) and the objective quantity \\(\\mathbb{E}[f(X)]\\), we have\n\\[\n\\mathbb{E}[f(X)] \\approx \\frac{1}{2}\\left[ f(X_1) + f(X_2)\\right] .\n\\]\nFor linear \\(f\\), it should be obvious that doing antithetic would improve the quality of the estimate. However, the benefit could be unclear for non-linear \\(f\\). For example, if \\(f(x) = x^2\\), and assuming \\(X\\) is zero-mean so \\((X_1, X_2 = -X_1)\\) forms an antithetic pair, we get\n\\[\n\\mathbb{E}[X^2] \\approx \\frac{1}{2}\\left[ X_1^2 + X_2^2\\right]  = X_1^2\n\\]\nwhich is worse than drawing two independent samples."
  },
  {
    "objectID": "posts/2025-08-06-mc-qmc-rqmc/index.html#quasi-monte-carlo",
    "href": "posts/2025-08-06-mc-qmc-rqmc/index.html#quasi-monte-carlo",
    "title": "Monte Carlo, Antithetic, and Quasi Monte Carlo",
    "section": "Quasi-Monte Carlo",
    "text": "Quasi-Monte Carlo\nWe return to Monte Carlo and consider the special case where our random variable \\(X\\) is the uniform distribution on the \\(D\\)-dimensional unit hypercube \\([0,1]^D\\). The standard Monte Carlo would randomly take points in the hypercube to sample, which does not feel very optimal. For \\(N\\) samples, it may be more efficient to evenly spread them across the full domain \\([0,1]^D\\) - this is the idea of Quasi-Monte Carlo (QMC).\nWe let \\(X_i^Q\\) for \\(i = 1, 2, \\ldots, D\\) denote the QMC samples and the QMC estimate of the integral is thus\n\\[\nI \\approx \\hat{I}_N^Q = \\frac{1}{N} \\sum_{i =1}^N f(X_i^Q).\n\\]\nNote that the assumption of uniform random variables is not overlly restrictive, as we can apply standard sampling techiques (e.g. inverse CDF) to convert an Unif[0,1] random variable to other commonly used random variables. See Devroye’s classic Non-Uniform Random Variate Generation for more information.\n\nStratification\nThe first thing one may propose to spread points more evenly would be stratification. If we wish to put \\(N\\) points in the hypercube (for simplicity we assume \\(N = n^D\\) for some integer \\(n\\)), we could divide each side of the hypercube into \\(n\\) even parts and construct \\(n^D\\) evenly sized hypercube blocks such that we can place a sample point in the center for each of the blocks. This ensures the points to be of the same distance from its neighbours.\nIf the function of interest \\(f\\) is sufficiently regular with periodicity, or have certain portions with larger than average variation, one would imagine this stratification produces a better integral approximation.\nVery quickly, we have figured out the two key factors of the QMC estimate quality: the sampling sequence’s discrepancy and the variation of the function. This intuition is solidified below.\n\n\nKoksma-Hlawka Inequality\nThe key result of QMC is the Koksma-Hlawka inequality which provides an upper bound of the estimation error of QMC for uniform random variables on \\([0,1]^D\\). This is given by\n\\[\n\\left|  \\int_{[0,1]^D} f(x) dx - \\frac{1}{N} \\sum_{i=1}^N f(X_i^Q)\\right| = | I - \\hat{I}_N^Q| \\le D^*_N \\cdot V_{HK}(f)\n\\]\nwhere \\(D_N^*\\) is the star-discrepancy of the sequence \\(\\{X_i^Q\\}_{i=1}^N\\) and \\(V_{HK}(f)\\) is the variation of function \\(f\\) in the Hardy-Krause sense. The star-discrepancy of a set of points \\(P_N = \\{ X_i^Q \\}_{i=1}^N\\) is defined as\n\\[\nD_N^*(P_N) := \\sup_{t \\in [0,1]^D} \\left| \\frac{1}{N}\\sum_{i=1}^N 1_{[0,t)}(X_i^Q) - \\prod_{d=1}^D t_d\\right|\n\\]\nwhich is the maximum difference between the size of rectangle from with a corner fixed at zero and the number of points from \\(P_N\\) it contains. The variation of Hardy-Krause sense for function \\(f\\) is given by\n\\[\nV_\\text{HK} (f) := \\sum_{\\emptyset \\neq u \\subseteq [D]} \\int_{[0,1]^D} \\left| \\frac{\\partial^{|u|}}{\\partial x_u}f(x_u, 1_{-u}) \\right| dx_u.\n\\]\nAlthough the above definition is a bit complicated looking, it is quite simple. Consider \\(f(x, y) = xy\\) so \\(D = 2\\), we have \\(\\partial_{xy} f = 0\\), \\(\\partial_x f = y\\), and \\(\\partial_y f = x\\) and thus\n\\[\n\\begin{split}\nV_\\text{HK}(f) &= \\int |\\partial_xf(x,1)|dx + \\int |\\partial_y f(1,y)|dy + \\iint |\\partial_{xy} f(x,y)|dx dy \\\\\n&= \\int 1 dx + \\int 1 dy + \\iint 0 dx dy = 2.\n\\end{split}\n\\]\nA proof sketch of the Koksma-Hlawka inequality is provided below.\n\n\nLow-Discrepancy Sequences\nSuggested by the Koksma-Hlawka inequality, we can control the quality of QMC estimate by constructing sequences with low star-discrepancy. Multiple such sequences (called low-discrepancy sequences, for obvious reasons) exist in the literature, and below we will briefly outline a few commonly considered ones. Note that both Halton and Sobol sequences require the number of samples to be pre-determined, while Latin hypercube does not.\n\nHalton sequence\n\nPick a set of coprime bases \\(b_1, b_2, \\dots, b_s\\) (usually the first \\(s\\) primes: 2, 3, 5, …).\nIn dimension \\(j\\), take the integer index \\(n\\) in base \\(b_j\\), reverse its digits after the decimal point, and interpret as a fraction — this is the radical-inverse.\nCombine coordinates to get the \\(n\\)-th point.\n\n\n\nSobol sequence\n\nUses base-2 arithmetic and direction numbers derived from primitive polynomials over GF(2).\nEach coordinate is a digital expansion using these direction numbers, ensuring good uniformity across projections.\n\n\n\nLatin hypercube sampling\n\nIn each dimension, divide \\([0,1]\\) into \\(N\\) equal intervals.\nSample exactly one point from each interval in each dimension.\nRandomly permute the points along each axis so that every projection on any single coordinate axis is uniform.\n\n\n\n\nError Rate\nRecall the Koksma-Hlawka inequality which provides the error bound for QMC, we wish to consider its scaling with the number of samples \\(N\\) and dimension \\(D\\). The existing results on the scaling of star-discrepancy for low-discrepancy sequences \\(P_N\\) are often of the following scale\n\\[\nD^*(P_N) = O\\left( \\frac{(\\log N)^D}{N}\\right)\n\\]\nwhereas the variation \\(V_\\text{HK}\\) of function \\(f\\) scales, in the worst case, exponentially in \\(D\\) based on its definition (as it is summing over exponentially increasing terms). Therefore, it is not hard to imagine that QMC outperforms standard Monte Carlo for sufficiently small dimension \\(D\\) for large enough sample \\(N\\) where the \\(1/N\\) scaling takes over; while standard Monte Carlo takes over QMC for large dimensions.\n\n\nRandomised QMC\nQMC is deterministic by definition. It is sometimes preferred to have stochasticities. Two commonly used tricks to randomise QMC are random shift and scrambing.\nRandom shift is trivial. Given an existing deterministic sequence \\(\\{X_i^Q\\}_i\\), we add a uniformly drawn value to each of them then modulo by one for each dimension to make sure the jittered points are within the considered domain \\([0,1]^D\\).\nScrambling works for digital sequences like Sobol or Halton, where the digits are permuted randomly in such a way to preserve uniformity and low discrepancy. See Owen scrambling."
  },
  {
    "objectID": "posts/2025-08-06-mc-qmc-rqmc/index.html#appendix",
    "href": "posts/2025-08-06-mc-qmc-rqmc/index.html#appendix",
    "title": "Monte Carlo, Antithetic, and Quasi Monte Carlo",
    "section": "Appendix",
    "text": "Appendix\n\nProof Sketch of Koksma-Hlawka\nThe original proof of the Koksma-Hlawka inequality is given using integration by parts and some decomposotion. Below, we roughly outline a simple proof using the powerful tool of reproducing kernel Hilbert space (RKHS).\nWe consider an RKHS \\(\\mathcal{H}\\) with reproducing kernel \\(K\\). For any function \\(f \\in \\mathcal{H}\\), the reproducing property of RKHS states that\n\\[\nf(x) = \\langle f, K(\\cdot, x)\\rangle_\\mathcal{H}, \\qquad \\int f(x) dx = \\int \\langle f, K(\\cdot, x)\\rangle_\\mathcal{H} dx =  \\langle f, \\int K(\\cdot, x) dx\\rangle_\\mathcal{H}\n\\]\nwhere \\(\\langle \\cdot, \\cdot \\rangle_\\mathcal{H}\\) is the inner product equipped with the RKHS. For a probability measure \\(\\mu\\), we have the kernel mean embedding \\(m_\\mu\\) of the measure defined as\n\\[\nm_\\mu := \\int K(\\cdot, x) d\\mu(x).\n\\]\nNotice that the full integral \\(I\\) is the expectation over the uniform measure \\(\\mu\\) while the QMC estimate is the expectation over the discrete measure \\(\\mu_N\\) as average Dirac mass. Thus, we have\n\\[\n\\begin{split}\n|I - \\hat{I}_N^Q| &= \\left| \\int f d\\mu - \\int fd\\mu_N \\right| \\\\\n&= \\left| \\int \\langle f, K(\\cdot, x)\\rangle_\\mathcal{H} d\\mu(x) - \\int \\langle f, K(\\cdot, x)\\rangle_\\mathcal{H} d\\mu_N(x)  \\right| \\\\\n&= \\left\\langle f, \\int K(\\cdot, x)d\\mu(x) \\right\\rangle_\\mathcal{H} - \\left\\langle f, \\int K(\\cdot, x)d\\mu_N(x) \\right\\rangle_\\mathcal{H} \\\\\n&= \\langle f, m_\\mu \\rangle_\\mathcal{H} - \\langle f, m_{\\mu_N}\\rangle_\\mathcal{H} = \\langle f, m_\\mu - m_{\\mu_N}\\rangle_\\mathcal{H} \\\\\n&\\le \\|f\\|_\\mathcal{H} \\cdot \\|m_\\mu - m_{\\mu_N}\\|_\\mathcal{H}.\n\\end{split}\n\\]\nFinally, it can be established that if we set the reproducing kernel as \\(K_D: [0,1]^D \\times [0,1]^D \\to \\mathbb{R}\\) given by\n\\[\nK_D(x, y) = \\prod_{j = 1}^D \\left( 1 + \\min\\{1-x_j, 1-y_j \\}\\right)\n\\]\nand work with its corresponding RKHS \\(\\mathcal{H}_D\\) we could recover the desired Koksma-Hlawka inequality frm the above derivation."
  },
  {
    "objectID": "posts/2025-08-06-mc-qmc-rqmc/index.html#experiments",
    "href": "posts/2025-08-06-mc-qmc-rqmc/index.html#experiments",
    "title": "Monte Carlo, Antithetic, and Quasi Monte Carlo",
    "section": "Experiments",
    "text": "Experiments\nBelow shows some numerical experiments comparing the relative performance of the Monte Carlo techniques in MSE for the estimation task of \\(\\mathbb{E}[f(X)]\\) where \\(f(a) = \\|a\\|_2^2\\) and \\(X\\) is either 1D uniform, multivariate Gaussian with diagonal covariance matrix, and multivariate Gaussian with non-diagonal covariance matrix. We compare standard Monte Carlo, antithetic, Sobol sequence, and randomised Sobol at varying number of sample sizes. 100 runs are conducted for the estimation of MSE. The code can be found here\n\nUniform\n\n\n\nUniform. Antithetic gives perfect estimation for smaller sample sizes thus points not shown on the log-scale plot.\n\n\n\n\nMultivariate Gaussian (Diagonal Covariance)\n\n\n\nDimension 4\n\n\n\n\n\nDimension 40\n\n\n\n\n\nDimension 400\n\n\n\n\nMultivariate Gaussian (Non-Diagonal Covariance)\n\n\n\nDimension 4\n\n\n\n\n\nDimension 40\n\n\n\n\n\nDimension 400"
  },
  {
    "objectID": "posts/2025-10-08-matheron/index.html",
    "href": "posts/2025-10-08-matheron/index.html",
    "title": "Approximate GP Posterior Sampling via the Matheron Rule",
    "section": "",
    "text": "This blog post is about J. Wilson et al. (2020) and J. T. Wilson et al. (2021)."
  },
  {
    "objectID": "posts/2025-10-08-matheron/index.html#matheron-rule",
    "href": "posts/2025-10-08-matheron/index.html#matheron-rule",
    "title": "Approximate GP Posterior Sampling via the Matheron Rule",
    "section": "Matheron Rule",
    "text": "Matheron Rule\nThe Matheron rule states that, for random variables \\(a\\), \\(b\\) that are jointly Gaussian, the conditional distribution \\(a | b = \\beta\\) is given by\n\\[\n(a| b = \\beta) \\stackrel{d}{=} a + \\Sigma_{a,b} \\Sigma_{b,b}^{-1} (\\beta - b)\n\\]\nwhere \\(\\stackrel{d}{=}\\) means distributional equivalence, \\(\\Sigma_{a,b} := \\text{Cov}(a, b)\\) and \\(\\Sigma_{b,b} := \\text{Cov}(b, b)\\). This can be verified easily by matching the mean and covariance of the two sides of the equation.\nUsing this formulation of conditional Gaussian, we can extrapolate it to a Gaussian process (GP) and obtain the following result. For a GP \\(f \\sim GP(\\mu, k)\\) with marginal \\(f_n = f(X_n)\\) at observation locations \\(X_n\\) that gives noisy observations \\(y = f(X_n) + \\varepsilon\\) for \\(\\varepsilon \\sim N(0, \\sigma^2 I_n)\\), we have the Matheron rule for posterior GP \\(f|y\\)\n\\[\n(f|y) (\\cdot) \\stackrel{d}{=} f(\\cdot) + k(\\cdot, X_n) (K_{n, n} + \\sigma^2I_n)^{-1} (y - f_n - \\varepsilon)\n\\]\nwhere \\(K_{n,n} = k(X_n, X_n)\\) is the Gram matrix of \\(X_n\\) with GP kernel \\(k\\).\nAt this point, we recall that the standard formulation posterior GP at test points \\(X_* \\in \\mathbb{R}^m\\) is given by\n\\[\n\\begin{split}\nf(X_*) |y &\\sim N_{n}(\\mu_{* | y}, K_{* | y}), \\\\\n\\mu_{* | y} &= \\mu(X) + K_{*,n}^T (K_{n,n} + \\sigma^2 I_n)^{-1} y,\\\\\nK_{* | y} &= K_{*,*} - K_{*,n} (K_{n,n} + \\sigma^2 I_n)^{-1}K_{*,n}^T.\n\\end{split}\n\\]\nand to draw a sample \\(f_*^{(1)}\\) from such a posterior \\(f(X_*) |y\\), we have\n\\[\nf_*^{(1)} = \\mu_{* | y} + \\sqrt{K_{* | y}} ~\\xi, \\qquad \\xi \\sim N(0, I)\n\\]\nwhere the matrix ‘square root’ \\(\\sqrt{A} := L\\) where \\(LL^T = A\\). Such matrix square roots are usually obtained using Cholesky decomposition, which has \\(O(m^3)\\) time complexity for \\(A \\in \\mathbb{R}^{m \\times m}\\). Thus, assuming the posterior mean and covariance are already obtained, the cost of drawing a posterior sample is \\(O(m^3)\\) for \\(m\\) test points.\nNow, to samples directly using the Matheron rule formulation, we have\n\\[\n(f|y) (X_*) \\stackrel{d}{=} f(X_*) + k(X_*, X_n) (K_{n, n} + \\sigma^2I_n)^{-1} (y - f(X_n) - \\varepsilon)\n\\]\nand thus after obtaining a sample \\(f^{(1),prior} = f_*^{(1),prior} \\cup f_n^{(1),prior}\\) from the prior \\(f(X)\\) with \\(X = X_* \\cup X_n\\), we can obtain a posterior sample \\(f_*^{(1)}\\) as\n\\[\nf_*^{(1)} = f_*^{(1),prior} + k(X_*, X_n) (K_{n, n} + \\sigma^2I_n)^{-1} (y - f_n^{(1),prior} - \\varepsilon^{(1)}), \\quad \\varepsilon^{(1)} \\sim N(0, \\sigma^2 I_n)\n\\]\nwhich cost \\(O((m+n)^3)\\) assuming all the mean and covariance are pre-computed as its takes \\(O((m+n)^3)\\) to compute the matrix square root \\(k(X, X)\\) to sample \\(f^{(1),prior}\\), although the cost could be reduced to \\(O(m^3+n^3)\\) using Schur complement. Thus, as it stands currently, there is no computational benefit of posterior sampling using the Matheron rule – even a loss when \\(X_n \\not \\subset X_*\\)! However, when we start to draw samples approximately, the Materon rule offers more flexibility.\n\n\n\nJ. T. Wilson et al. (2021)"
  },
  {
    "objectID": "posts/2025-10-08-matheron/index.html#approximations",
    "href": "posts/2025-10-08-matheron/index.html#approximations",
    "title": "Pathwise Conditioning of Gaussian Processes via the Matheron Rule",
    "section": "Approximations",
    "text": "Approximations"
  },
  {
    "objectID": "posts/2025-10-08-matheron/index.html#approximate-sampling",
    "href": "posts/2025-10-08-matheron/index.html#approximate-sampling",
    "title": "Approximate GP Posterior Sampling via the Matheron Rule",
    "section": "Approximate Sampling",
    "text": "Approximate Sampling\nRecall the general Matheron rule for GP posterior yields\n\\[\n(f|y) (\\cdot) \\stackrel{d}{=} \\underbrace{f(\\cdot)}_\\text{prior} + \\underbrace{k(\\cdot, X_n) (K_{n, n} + \\sigma^2I_n)^{-1} (y - f_n - \\varepsilon)}_\\text{update}\n\\] where the first term on the right is a prior term, and the second term is the update term. The two computational bottlenecks for posterior sampling in this fashion are (a) the sampling from prior, and (b) the computation of matrix inverse and multiple for the update. When we turn to approximate sampling, the decoupling of terms due to the Matheron rule enables us to use different approximations to each of the prior and update terms, as opposed to applying one approximation throughout in standard posterior sampling.\nMany approximation methods for GP exist:\n\n[Low Rank] Random Fourier feature (Rahimi and Recht 2007)\n[Sparse] Inducing points (Titsias 2009; Leibfried et al. 2020)\n[Iterative] Conjugate gradient (Pleiss 2020), SGD (Lin et al. 2023)\n\nAll of the above can be mixed-and-matched to approximate the prior and update terms of the Matheron update to utilise their respective pros. For example, the results in J. T. Wilson et al. (2021) suggest that random Fourier feature is good for approximating the prior term, while conjugate gradient is suitable for approximating the update.\n\n\n\nJ. T. Wilson et al. (2021)\n\n\n\n\n\nJ. T. Wilson et al. (2021)"
  },
  {
    "objectID": "posts/2025-10-08-matheron/index.html#the-kalman-link",
    "href": "posts/2025-10-08-matheron/index.html#the-kalman-link",
    "title": "Approximate GP Posterior Sampling via the Matheron Rule",
    "section": "The Kalman Link",
    "text": "The Kalman Link"
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html",
    "href": "posts/2025-10-12-fourier-transform/index.html",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "",
    "text": "The Fourier transform provides a way to represent signals, functions, or processes in terms of their frequency components, revealing the oscillatory structure underlying time-domain phenomena. Formally, the Fourier framework rests on the idea that complex exponentials \\(e^{2\\pi i f t}\\) form an orthogonal basis for many function spaces. By expanding signals in this basis, we can analyse, filter, and reconstruct them in the frequency domain."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#continuous-fourier-transform-cft",
    "href": "posts/2025-10-12-fourier-transform/index.html#continuous-fourier-transform-cft",
    "title": "From Fourier Transform to Fast Fourier Transform",
    "section": "2.1 Continuous Fourier Transform (CFT)",
    "text": "2.1 Continuous Fourier Transform (CFT)\nFor a continuous-time signal ( x(t) ), the Fourier Transform (FT) and its inverse are defined as\n[ X(f) = {-}^{} x(t) e^{-2i f t} , dt, x(t) = {-}^{} X(f) e^{2i f t} , df. ]\n\nExistence Conditions\nThe transform exists if one of the following holds:\n\n( x(t) L^1() ): absolutely integrable; ( X(f) ) is continuous and bounded.\n( x(t) L^2() ): square-integrable; ( X(f) ) exists in the mean-square sense, with Parseval’s identity\n[ |x(t)|^2 dt = |X(f)|^2 df. ]\n( x(t) ) is a tempered distribution (e.g. periodic, impulses), defined via generalized functions.\n\n\n\nKey Properties\n\n\n\nProperty\nTime Domain\nFrequency Domain\n\n\n\n\nLinearity\n( a x_1 + b x_2 )\n( a X_1 + b X_2 )\n\n\nTime shift\n( x(t-t_0) )\n( e^{-2i f t_0} X(f) )\n\n\nFrequency shift\n( e^{2i f_0 t} x(t) )\n( X(f - f_0) )\n\n\nConvolution\n( (x*y)(t) )\n( X(f) Y(f) )\n\n\n\nThese properties underpin filtering and spectral analysis."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#the-four-core-scenarios",
    "href": "posts/2025-10-12-fourier-transform/index.html#the-four-core-scenarios",
    "title": "From Fourier Transform to Fast Fourier Transform",
    "section": "2.2 The Four Core Scenarios",
    "text": "2.2 The Four Core Scenarios\nThe Fourier framework applies to deterministic and stochastic, continuous and discrete systems.\nBelow are the four canonical cases and their mathematical conditions.\n\n\n\n\n\n\n\n\n\n\nCase\nDomain\nType\nExistence Condition\nExample\n\n\n\n\n(i)\nContinuous\nDeterministic\n( x(t) L^1 ) or ( L^2 )\nAcoustic pulse, decaying wave\n\n\n(ii)\nContinuous\nStochastic\nWSS + ( R_X() L^1 )\nThermal noise, turbulent flow\n\n\n(iii)\nDiscrete\nDeterministic\n( x[n] l^1 ) or ( l^2 )\nDigital audio signal\n\n\n(iv)\nDiscrete\nStochastic\nWSS + ( R_X[k] l^1 )\nStock returns, discrete-time noise\n\n\n\n\nDeterministic Signals\n\nContinuous Deterministic:\nThe standard Fourier transform above applies directly.\nDiscrete Deterministic (DTFT):\nFor a discrete-time sequence ( x[n] ), [ X() = _{n=-}^{} x[n] e^{-in}, [-, ). ] This transform is periodic in frequency with period ( 2).\n\n\n\nStochastic Processes\nIf ( X(t) ) (or ( X[n] )) is a wide-sense stationary (WSS) process, its autocorrelation function ( R_X() = [X(t) X(t+)] ) defines a power spectral density (PSD) via the Wiener–Khinchin theorem: [ S_X(f) = _{-}^{} R_X() e^{-2i f } , d. ] The PSD gives the expected power per unit frequency."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#the-shannonnyquist-sampling-theorem",
    "href": "posts/2025-10-12-fourier-transform/index.html#the-shannonnyquist-sampling-theorem",
    "title": "From Fourier Transform to Fast Fourier Transform",
    "section": "3.1 The Shannon–Nyquist Sampling Theorem",
    "text": "3.1 The Shannon–Nyquist Sampling Theorem\nIf a continuous-time signal ( x(t) ) is bandlimited to frequencies ( |f| &lt; f_{} ), then it can be perfectly reconstructed from samples ( x[n] = x(nT) ) if the sampling frequency satisfies\n[ f_s = &gt; 2 f_{}. ]\nThe reconstruction formula is\n[ x(t) = _{n=-}^{} x[n] , !(), (x) = . ]\n\nInterpretation\n\nSampling in time ⇒ periodic replication in frequency.\n\nIf ( f_s &gt; 2 f_{} ), the replicas do not overlap and perfect recovery is possible.\n\nIf ( f_s &lt; 2 f_{} ), overlap occurs, producing aliasing.\n\nThis theorem links continuous and discrete analysis rigorously."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#derivation-cooleytukey-1965",
    "href": "posts/2025-10-12-fourier-transform/index.html#derivation-cooleytukey-1965",
    "title": "From Fourier Transform to Fast Fourier Transform",
    "section": "5.1 Derivation (Cooley–Tukey, 1965)",
    "text": "5.1 Derivation (Cooley–Tukey, 1965)\nLet ( N = 2M ) (even). Split ( x[n] ) into even and odd samples: [ x_{}[n] = x[2n], x_{}[n] = x[2n+1]. ]\nThen [ X[k] = {n=0}^{M-1} x[2n] e^{-2i k(2n)/N} + e^{-2i k / N} {n=0}^{M-1} x[2n+1] e^{-2i k(2n)/N}. ]\nDefine [ E[k] = x_{}, O[k] = x_{}. ] Then [ X[k] = E[k] + e^{-2i k / N} O[k], X[k+M] = E[k] - e^{-2i k / N} O[k]. ]\nEach DFT of size ( N ) becomes two DFTs of size ( N/2 ) plus ( N ) multiplications.\nThe recursion depth is ( _2 N ), giving [ T(N) = O(N N). ]"
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#fft-pseudocode",
    "href": "posts/2025-10-12-fourier-transform/index.html#fft-pseudocode",
    "title": "From Fourier Transform to Fast Fourier Transform",
    "section": "5.2 FFT Pseudocode",
    "text": "5.2 FFT Pseudocode\n\nFFT &lt;- function(x) {\n  N &lt;- length(x)\n  if (N == 1) return(x)\n  X_even &lt;- FFT(x[seq(1, N, by = 2)])\n  X_odd  &lt;- FFT(x[seq(2, N, by = 2)])\n  k &lt;- 0:(N/2 - 1)\n  twiddle &lt;- exp(-2i * pi * k / N) * X_odd\n  c(X_even + twiddle, X_even - twiddle)\n}\n\n\nComplexity Comparison\n\n\n\nMethod\nOperations\nMemory\n\n\n\n\nDFT (naïve)\n( O(N^2) )\n( O(N) )\n\n\nFFT\n( O(N N) )\n( O(N) ) (in-place)\n\n\n\nThis efficiency revolutionized digital signal processing and numerical computation."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#introduction-and-motivation",
    "href": "posts/2025-10-12-fourier-transform/index.html#introduction-and-motivation",
    "title": "From Fourier Transform to Fast Fourier Transform",
    "section": "",
    "text": "The Fourier transform is one of the most profound mathematical tools in applied science. It provides a way to represent signals, functions, or processes in terms of their frequency components, revealing the oscillatory structure underlying time-domain phenomena. Formally, the Fourier framework rests on the idea that complex exponentials \\(e^{2\\pi i f t}\\) form an orthogonal basis for many function spaces. By expanding signals in this basis, we can analyze, filter, and reconstruct them in the frequency domain — a cornerstone of modern computation, physics, and data science."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#mathematical-foundations-of-the-fourier-transform",
    "href": "posts/2025-10-12-fourier-transform/index.html#mathematical-foundations-of-the-fourier-transform",
    "title": "From Fourier Transform to Fast Fourier Transform",
    "section": "Mathematical Foundations of the Fourier Transform",
    "text": "Mathematical Foundations of the Fourier Transform\n\nContinuous Fourier Transform (CFT)\nFor a continuous-time signal \\(x(t)\\), the Fourier Transform (FT) and its inverse are defined as\n\\[\nX(f) = \\int_{-\\infty}^{\\infty} x(t) e^{-2\\pi i f t} \\, dt,\n\\qquad\nx(t) = \\int_{-\\infty}^{\\infty} X(f) e^{2\\pi i f t} \\, df.\n\\]\n\nExistence Conditions\nThe transform exists if one of the following holds:\n\n\\(x(t) \\in L^1(\\mathbb{R})\\): absolutely integrable; \\(X(f)\\) is continuous and bounded.\n\\(x(t) \\in L^2(\\mathbb{R})\\): square-integrable; \\(X(f)\\) exists in the mean-square sense, with Parseval’s identity\n\\[\n\\int |x(t)|^2 dt = \\int |X(f)|^2 df.\n\\]\n\\(x(t)\\) is a tempered distribution (e.g. periodic, impulses), defined via generalized functions.\n\n\n\nKey Properties\n\n\n\nProperty\nTime Domain\nFrequency Domain\n\n\n\n\nLinearity\n\\(a x_1 + b x_2\\)\n\\(a X_1 + b X_2\\)\n\n\nTime shift\n\\(x(t-t_0)\\)\n\\(e^{-2\\pi i f t_0} X(f)\\)\n\n\nFrequency shift\n\\(e^{2\\pi i f_0 t} x(t)\\)\n\\(X(f - f_0)\\)\n\n\nConvolution\n\\((x*y)(t)\\)\n\\(X(f) Y(f)\\)\n\n\n\nThese properties underpin filtering and spectral analysis.\n\n\n\nThe Four Core Scenarios\nThe Fourier framework applies to deterministic and stochastic, continuous and discrete systems.\nBelow are the four canonical cases and their mathematical conditions.\n\n\n\n\n\n\n\n\n\n\nCase\nDomain\nType\nExistence Condition\nExample\n\n\n\n\n(i)\nContinuous\nDeterministic\n\\(x(t) \\in L^1\\) or \\(L^2\\)\nAcoustic pulse, decaying wave\n\n\n(ii)\nContinuous\nStochastic\nWSS + \\(R_X(\\tau) \\in L^1\\)\nThermal noise, turbulent flow\n\n\n(iii)\nDiscrete\nDeterministic\n\\(x[n] \\in l^1\\) or \\(l^2\\)\nDigital audio signal\n\n\n(iv)\nDiscrete\nStochastic\nWSS + \\(R_X[k] \\in l^1\\)\nStock returns, discrete-time noise\n\n\n\n\nDeterministic Signals\n\nContinuous Deterministic:\nThe standard Fourier transform above applies directly.\nDiscrete Deterministic (DTFT):\nFor a discrete-time sequence \\(x[n]\\), \\[\nX(\\omega) = \\sum_{n=-\\infty}^{\\infty} x[n] e^{-i\\omega n}, \\qquad \\omega \\in [-\\pi, \\pi).\n\\] This transform is periodic in frequency with period \\(2\\pi\\).\n\n\n\n\nStochastic Processes\nIf \\(X(t)\\) (or \\(X[n]\\)) is a wide-sense stationary (WSS) process, its autocorrelation function \\(R_X(\\tau) = \\mathbb{E}[X(t) X(t+\\tau)]\\) defines a power spectral density (PSD) via the Wiener–Khinchin theorem: \\[\nS_X(f) = \\int_{-\\infty}^{\\infty} R_X(\\tau) e^{-2\\pi i f \\tau} \\, d\\tau.\n\\] The PSD gives the expected power per unit frequency."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#sampling-and-discretization",
    "href": "posts/2025-10-12-fourier-transform/index.html#sampling-and-discretization",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "Sampling and Discretization",
    "text": "Sampling and Discretization\n\nThe Shannon–Nyquist Sampling Theorem\nIf a continuous-time signal \\(x(t)\\) is bandlimited to frequencies \\(|f| &lt; f_{\\max}\\), then it can be perfectly reconstructed from samples \\(x[n] = x(nT)\\) if the sampling frequency satisfies\n\\[\nf_s = \\frac{1}{T} &gt; 2 f_{\\max}.\n\\]\nThe reconstruction formula is then\n\\[\nx(t) = \\sum_{n=-\\infty}^{\\infty} x[n] \\mathrm{sinc}\\left(\\frac{t-nT}{T}\\right),\n\\quad \\text{where } \\mathrm{sinc}(x) = \\frac{\\sin(\\pi x)}{\\pi x}.\n\\] To interpret the above result, we notice:\n\nSampling in time ⇒ periodic replication in frequency.\n\nIf \\(f_s &gt; 2 f_{\\max}\\), the replicas do not overlap and perfect recovery is possible.\n\nIf \\(f_s &lt; 2 f_{\\max}\\), overlap occurs, producing aliasing.\n\nThis theorem links continuous and discrete analysis rigorously."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#from-dtft-to-discrete-fourier-transform-dft",
    "href": "posts/2025-10-12-fourier-transform/index.html#from-dtft-to-discrete-fourier-transform-dft",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "From DTFT to Discrete Fourier Transform (DFT)",
    "text": "From DTFT to Discrete Fourier Transform (DFT)\nThe Discrete Fourier Transform (DFT) converts a finite-length sequence of \\(N\\) uniformly sampled data points into \\(N\\) equally spaced frequency components:\n\\[\nX[k] = \\sum_{n=0}^{N-1} x[n] e^{-2\\pi i kn/N}, \\quad\nx[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X[k] e^{2\\pi i kn/N}.\n\\]\nThe DFT assumes: - The sequence is periodic of period \\(N\\).\n- The data are uniformly sampled.\n\nMatrix Form\n\\[\nX = F_N x, \\quad (F_N)_{kn} = e^{-2\\pi i kn / N}.\n\\]\nThis direct computation requires \\(N^2\\) operations."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#the-fast-fourier-transform-fft",
    "href": "posts/2025-10-12-fourier-transform/index.html#the-fast-fourier-transform-fft",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "The Fast Fourier Transform (FFT)",
    "text": "The Fast Fourier Transform (FFT)\nThe FFT is an algorithm for computing the DFT efficiently.\n\nDerivation (Cooley–Tukey, 1965)\nLet \\(N = 2M\\) (even). Split \\(x[n]\\) into even and odd samples: \\[\nx_{\\text{even}}[n] = x[2n], \\qquad x_{\\text{odd}}[n] = x[2n+1].\n\\]\nThen \\[\nX[k] = \\sum_{n=0}^{M-1} x[2n] e^{-2\\pi i k(2n)/N}\n      + e^{-2\\pi i k / N} \\sum_{n=0}^{M-1} x[2n+1] e^{-2\\pi i k(2n)/N}.\n\\]\nDefine \\[\nE[k] = \\text{DFT of } x_{\\text{even}}, \\quad\nO[k] = \\text{DFT of } x_{\\text{odd}}.\n\\] Then \\[\nX[k] = E[k] + e^{-2\\pi i k / N} O[k],\n\\qquad\nX[k+M] = E[k] - e^{-2\\pi i k / N} O[k].\n\\]\nEach DFT of size \\(N\\) becomes two DFTs of size \\(N/2\\) plus \\(N\\) multiplications.\nThe recursion depth is \\(\\log_2 N\\), giving \\[\nT(N) = O(N \\log N).\n\\]\n\n\nFFT Pseudocode\n\nFFT &lt;- function(x) {\n  N &lt;- length(x)\n  if (N == 1) return(x)\n  X_even &lt;- FFT(x[seq(1, N, by = 2)])\n  X_odd  &lt;- FFT(x[seq(2, N, by = 2)])\n  k &lt;- 0:(N/2 - 1)\n  twiddle &lt;- exp(-2i * pi * k / N) * X_odd\n  c(X_even + twiddle, X_even - twiddle)\n}\n\n\nComplexity Comparison\n\n\n\nMethod\nOperations\nMemory\n\n\n\n\nDFT (naïve)\n( O(N^2) )\n( O(N) )\n\n\nFFT\n( O(N N) )\n( O(N) ) (in-place)\n\n\n\nThis efficiency revolutionized digital signal processing and numerical computation."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#practical-considerations-and-limitations",
    "href": "posts/2025-10-12-fourier-transform/index.html#practical-considerations-and-limitations",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "Practical Considerations and Limitations",
    "text": "Practical Considerations and Limitations\n\nUniform sampling: Required for standard FFT.\nPeriodicity assumption: The DFT assumes the signal repeats every ( N ). Nonperiodic data cause spectral leakage, mitigated by windowing (Hann, Hamming).\nNonuniform sampling: Requires Nonuniform FFT (NUFFT) or interpolation.\nNon-power-of-two lengths: Use mixed-radix or Bluestein algorithms.\nPrecision: Finite floating-point accuracy limits numerical dynamic range.\nTime–frequency tradeoff: FFT is global; for nonstationary signals use STFT or wavelet transforms."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#broader-perspective",
    "href": "posts/2025-10-12-fourier-transform/index.html#broader-perspective",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "Broader Perspective",
    "text": "Broader Perspective\nFourier analysis unifies many branches of applied mathematics:\n\nIn PDEs, it converts differential operators into algebraic multipliers.\nIn probability, it relates to characteristic functions and power spectra.\nIn numerical computation, FFTs enable fast convolution and correlation.\nIn modern data science, it underlies spectral clustering, signal compression, and filtering.\n\nThe Fast Fourier Transform stands as one of the most impactful algorithms in history — turning an ( O(N^2) ) operation into ( O(N N) ) and making spectral computation routine in real time."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#fourier-transform",
    "href": "posts/2025-10-12-fourier-transform/index.html#fourier-transform",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "Fourier Transform",
    "text": "Fourier Transform\n\nContinuous-Time Fourier Transform\nFor a continuous-time signal \\(x(t)\\), the Fourier Transform (FT) \\(X(\\omega)\\) and its inverse are defined as\n\\[\nX(\\omega) = \\int_{-\\infty}^{\\infty} x(t) e^{-2\\pi i f t} dt,\n\\qquad\nx(t) = \\int_{-\\infty}^{\\infty} X(\\omega) e^{2\\pi i f t} d\\omega.\n\\]\nThe transform exists if one of the following holds:\n\n\\(x(t) \\in L^1(\\mathbb{R})\\): absolutely integrable; \\(X(\\omega)\\) is continuous and bounded.\n\\(x(t) \\in L^2(\\mathbb{R})\\): square-integrable; \\(X(\\omega)\\) exists in the mean-square sense, with Parseval’s identity\n\\[\n\\int |x(t)|^2 dt = \\int |X(\\omega)|^2 d\\omega.\n\\]\n\\(x(t)\\) is a tempered distribution (e.g. periodic, impulses), defined via generalized functions.\n\n\n\nDiscrete-Time Fourier Transform\nFor a discrete-time sequence \\(x[n]\\), \\[\nX(\\omega) = \\sum_{n=-\\infty}^{\\infty} x[n] e^{-i\\omega n}, \\qquad \\omega \\in [-\\pi, \\pi).\n\\] This transform is periodic in frequency with period \\(2\\pi\\).\n\n\nStochastic Processes\nIf \\(X(t)\\) (or \\(X[n]\\)) is a wide-sense stationary (WSS) process, its autocorrelation function\n\\[\nR_X(\\tau) = \\mathbb{E}[X(t) X(t+\\tau)]\n\\]\ndefines a power spectral density (PSD) via the Wiener–Khinchin theorem:\n\\[\nS_X(f) = \\int_{-\\infty}^{\\infty} R_X(\\tau) e^{-2\\pi i f \\tau} \\, d\\tau.\n\\]\nThe PSD gives the expected power per unit frequency.\n\n\nThe Four Scenarios\nThe Fourier framework applies to deterministic and stochastic, continuous and discrete systems. Below summarises the four canonical cases and their mathematical conditions.\n\n\n\n\n\n\n\n\n\nDomain\nType\nExistence Condition\nExample\n\n\n\n\nContinuous\nDeterministic\n\\(x(t) \\in L^1\\) or \\(L^2\\)\nAcoustic pulse, decaying wave\n\n\nContinuous\nStochastic\nWSS + \\(R_X(\\tau) \\in L^1\\)\nThermal noise, turbulent flow\n\n\nDiscrete\nDeterministic\n\\(x[n] \\in l^1\\) or \\(l^2\\)\nDigital audio signal\n\n\nDiscrete\nStochastic\nWSS + \\(R_X[k] \\in l^1\\)\nStock returns, discrete-time noise\n\n\n\n\n\nKey Properties\nSome of the key properties of the Fourier transform are listed below.\n\n\n\n\n\n\n\n\nProperty\nTime Domain\nFrequency Domain\n\n\n\n\nLinearity\n\\(a x_1 + b x_2\\)\n\\(a X_1 + b X_2\\)\n\n\nTime shift\n\\(x(t-t_0)\\)\n\\(e^{-2\\pi i \\omega t_0} X(\\omega)\\)\n\n\nFrequency shift\n\\(e^{2\\pi i \\omega_0 t} x(t)\\)\n\\(X(\\omega - \\omega_0)\\)\n\n\nConvolution\n\\((x*y)(t)\\)\n\\(X(\\omega) Y(\\omega)\\)"
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#the-fast-fourier-transform",
    "href": "posts/2025-10-12-fourier-transform/index.html#the-fast-fourier-transform",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "The Fast Fourier Transform",
    "text": "The Fast Fourier Transform\n\nDiscrete Fourier Transform\nThe discrete Fourier transform (DFT) converts a finite-length sequence (as opposed to the infinite-length sequence in the discrete-time Fourier transform above) of \\(N\\) uniformly sampled data points into \\(N\\) equally spaced frequency components:\n\\[\nX[k] = \\sum_{n=0}^{N-1} x[n] e^{-2\\pi i kn/N}, \\quad\nx[n] = \\frac{1}{N} \\sum_{k=0}^{N-1} X[k] e^{2\\pi i kn/N}.\n\\]\nDFT is one of the most consequential constructs in all of applied mathematics as it provides the mathematical and computational bridge between continuous Fourier analysis and the discrete, finite data we actually have in the real world. However, the direct application of DFT has \\(O(N^2)\\) time-complexity as each of the \\(N\\) transformed terms of \\(X\\) sums over all \\(N\\) terms of \\(x\\).\n\n\nCooley-Tukey’s FFT\nThe fast Fourier transform (FFT), proposed by Cooley and Tukey in their 1965 paper “An algorithm for the machine calculation of complex Fourier series”, is an efficient way of computing the DFT with a complexity of merely \\(O(N \\log_2 N)\\) via recursion.\nFor simplicity, we assume our sequence length is even, i.e. \\(N = 2M\\) for some integer \\(M\\). Splitting the sequence \\(x[n]\\) into even and odd samples gives \\[\nx_{\\text{even}}[n] = x[2n], \\qquad x_{\\text{odd}}[n] = x[2n+1].\n\\]\nThen, we notice the discrete Fourier transform \\(X[k]\\) can be formulated as\n\\[\nX[k] = \\sum_{n=0}^{M-1} x[2n] e^{-2\\pi i k(2n)/N}\n      + e^{-2\\pi i k / N} \\sum_{n=0}^{M-1} x[2n+1] e^{-2\\pi i k(2n)/N}.\n\\]\nWe define the two terms\n\\[\nE[k] = \\text{DFT of } x_{\\text{even}}, \\quad\nO[k] = \\text{DFT of } x_{\\text{odd}}.\n\\]\nSo\n\\[\nX[k] = E[k] + e^{-2\\pi i k / N} O[k],\n\\qquad\nX[k+M] = E[k] - e^{-2\\pi i k / N} O[k].\n\\]\nThis divides each DFT computation of size \\(N\\) into two DFTs of size \\(N/2\\) plus \\(N\\) multiplications. This recursive division has depth \\(\\log_2 N\\), which means the total time complexity of FFT is \\(O(N \\log N)\\). The pseudocode of FFT is presented below.\n\nFFT &lt;- function(x) {\n  N &lt;- length(x)\n  if (N == 1) return(x)\n  X_even &lt;- FFT(x[seq(1, N, by = 2)])\n  X_odd  &lt;- FFT(x[seq(2, N, by = 2)])\n  k &lt;- 0:(N/2 - 1)\n  twiddle &lt;- exp(-2i * pi * k / N) * X_odd\n  c(X_even + twiddle, X_even - twiddle)\n}\n\n\n\nPractical Considerations and Limitations\n\n(Uniform Sampling) Required for standard FFT.\n(Periodicity) The DFT assumes the signal repeats every ( N ). Nonperiodic data cause spectral leakage, mitigated by windowing (Hann, Hamming).\n(Nonuniform Sampling) Requires Nonuniform FFT (NUFFT) or interpolation.\n(Non-power-of-two Lengths) Use mixed-radix or Bluestein algorithms.\n(Precision) Finite floating-point accuracy limits numerical dynamic range.\n(Time–frequency Tradeoff) FFT is global; for nonstationary signals use STFT or wavelet transforms."
  },
  {
    "objectID": "posts/2025-10-12-fourier-transform/index.html#example-signal-sampling-and-reconstruction",
    "href": "posts/2025-10-12-fourier-transform/index.html#example-signal-sampling-and-reconstruction",
    "title": "[Derivation Scribbles] Fourier Transform and FFT",
    "section": "Example: Signal Sampling and Reconstruction",
    "text": "Example: Signal Sampling and Reconstruction\nThis experiment demonstrates how a discrete signal can be analysed and perfectly reconstructed using the DFT and its inverse. A simple two-tone waveform is constructed by summing two sinusoids: one at \\(f_1=3 \\text{Hz}\\) and another at \\(f_2=0.8 \\text{Hz}\\) with three times the amplitude, i.e. \n\\[\n  x(t) = \\sin(2\\pi f_1 t) + 3\\sin(2\\pi f_2 t).\n\\]\nThe signal is sampled at 10 Hz over a 2-second window, giving 20 discrete samples that represent the time-domain signal. A finely sampled version of the same signal is also generated to serve as a “ground truth” continuous reference.\nThe first plot shows how the two sinusoidal components combine to form the total signal and how this composite waveform is captured at discrete time points. The discrete samples contain all the information necessary to recover the original continuous-time signal, provided that the sampling frequency is more than twice the highest signal frequency (the Nyquist condition), which is satisfied here since \\(f_s = 10 \\text{Hz} &gt; 2 × 3 \\text{Hz}\\).\nThe second plot displays the two-sided amplitude spectrum obtained from the DFT of the sampled signal (computed using FFT). It reveals distinct peaks at ±3 Hz and ±0.8 Hz, corresponding to the frequencies of the two underlying sinusoids. The symmetric appearance of the spectrum reflects the fact that the original signal is purely real, so its Fourier representation contains conjugate frequency pairs. This plot demonstrates how the DFT decomposes a discrete-time signal into its constituent frequency components.\nFinally, the third plot compares the original “ground truth” continuous signal with a reconstructed version obtained by the inverse FFT. The two curves coincide almost perfectly, confirming that the discrete samples fully preserve the information of the band-limited signal. Together, these results illustrate the core principles of Fourier analysis: decomposition of time-domain signals into frequencies and accurate reconstruction under proper sampling.\n\n\n\nSampling Frequency 10Hz\n\n\nWhen we reduce the sampling frequencies below the Nyquist limit, the reconstructed signal starts to deviate from the ground truth as important portions of the spectrum are not captured.\n\n\n\nSampling Frequency 6Hz\n\n\n\n\n\nSampling Frequency 2Hz\n\n\nThe R code used to generate the above plot can be found here."
  },
  {
    "objectID": "index.html#news",
    "href": "index.html#news",
    "title": "Rui-Yang Zhang",
    "section": "News",
    "text": "News\n\n[Jan 26] Presented BALLAST at the STOR-i Annual Conference in Lancaster and the Irish CRT Winter Symposium in Dublin.\n[Dec 25] Our paper Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift has been accepted in the IMA Journal of Numerical Analysis! This is joint work with Yuga Iguchi, Sam Livingstone, Nik Nüsken, and Giorgos Vasdekis.\n\n\n\n\nAdapted from Adrien’s codes."
  }
]