<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rui-Yang Zhang">
<meta name="dcterms.date" content="2025-08-11">
<meta name="description" content="Basics of Monte Carlo, Antithetic, Quasi Monte Carlo, Randomised Quasi Monte Carlo.">

<title>Rui-Yang Zhang - Monte Carlo, Antithetic, and Quasi Monte Carlo</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../rui.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../rui.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Rui-Yang Zhang</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html" rel="" target="">
 <span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html" rel="" target="">
 <span class="menu-text">Notes &amp; Slides</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resources.html" rel="" target="">
 <span class="menu-text">Resources</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Monte Carlo, Antithetic, and Quasi Monte Carlo</h1>
                  <div>
        <div class="description">
          Basics of Monte Carlo, Antithetic, Quasi Monte Carlo, Randomised Quasi Monte Carlo.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Sampling</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Rui-Yang Zhang </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 11, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#monte-carlo" id="toc-monte-carlo" class="nav-link active" data-scroll-target="#monte-carlo">Monte Carlo</a></li>
  <li><a href="#antithetic" id="toc-antithetic" class="nav-link" data-scroll-target="#antithetic">Antithetic</a></li>
  <li><a href="#quasi-monte-carlo" id="toc-quasi-monte-carlo" class="nav-link" data-scroll-target="#quasi-monte-carlo">Quasi-Monte Carlo</a>
  <ul class="collapse">
  <li><a href="#stratification" id="toc-stratification" class="nav-link" data-scroll-target="#stratification">Stratification</a></li>
  <li><a href="#koksma-hlawka-inequality" id="toc-koksma-hlawka-inequality" class="nav-link" data-scroll-target="#koksma-hlawka-inequality">Koksma-Hlawka Inequality</a></li>
  <li><a href="#low-discrepancy-sequences" id="toc-low-discrepancy-sequences" class="nav-link" data-scroll-target="#low-discrepancy-sequences">Low-Discrepancy Sequences</a></li>
  <li><a href="#error-rate" id="toc-error-rate" class="nav-link" data-scroll-target="#error-rate">Error Rate</a></li>
  <li><a href="#randomised-qmc" id="toc-randomised-qmc" class="nav-link" data-scroll-target="#randomised-qmc">Randomised QMC</a></li>
  </ul></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments">Experiments</a>
  <ul class="collapse">
  <li><a href="#uniform" id="toc-uniform" class="nav-link" data-scroll-target="#uniform">Uniform</a></li>
  <li><a href="#multivariate-gaussian-diagonal-covariance" id="toc-multivariate-gaussian-diagonal-covariance" class="nav-link" data-scroll-target="#multivariate-gaussian-diagonal-covariance">Multivariate Gaussian (Diagonal Covariance)</a></li>
  <li><a href="#multivariate-gaussian-non-diagonal-covariance" id="toc-multivariate-gaussian-non-diagonal-covariance" class="nav-link" data-scroll-target="#multivariate-gaussian-non-diagonal-covariance">Multivariate Gaussian (Non-Diagonal Covariance)</a></li>
  </ul></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#proof_sketch" id="toc-proof_sketch" class="nav-link" data-scroll-target="#proof_sketch">Proof Sketch of Koksma-Hlawka</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Consider the standard Monte Carlo integration problem of</p>
<p><span class="math display">\[
I = \int f(x) dq(x) = \int f(x) q(x) dx = \mathbb{E}_{X \sim q}[f(X)]
\]</span></p>
<p>where <span class="math inline">\(q\)</span> is the density of the random variable <span class="math inline">\(X\)</span>, <span class="math inline">\(f\)</span> is a function with bounded variance (i.e.&nbsp;<span class="math inline">\(\int (f-I)^2 q(x) dx = \sigma^2 &lt; \infty\)</span>), and we try to estimate <span class="math inline">\(I\)</span> using samples of <span class="math inline">\(X\)</span>.</p>
<p>Let <span class="math inline">\(D\)</span> denote the dimension of the random variable <span class="math inline">\(X\)</span>. This would also be the dimension of the intergral <span class="math inline">\(I\)</span>.</p>
<p>For any estimate <span class="math inline">\(\hat{I}\)</span> of <span class="math inline">\(I\)</span>, we will consider the mean squared error (MSE) of the estimate, i.e.&nbsp;<span class="math inline">\(\mathbb{E}[(\hat{I} - I)^2]\)</span>, for its capture of both bias and variance of the estimate as it decomposes into the sum of the variance and the squared bias of the estimator.</p>
<section id="monte-carlo" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo">Monte Carlo</h2>
<p>The Monte Carlo method draws <span class="math inline">\(N\)</span> i.i.d. samples <span class="math inline">\(X_1, X_2, \ldots, X_N\)</span> from <span class="math inline">\(X\)</span> and compute the <strong>Monte Carlo approximate</strong> of <span class="math inline">\(I\)</span> as follows,</p>
<p><span class="math display">\[
\hat{I}_N = \frac{1}{N}\sum_{i=1}^N f(X_i).
\]</span></p>
<p>First, it is immediate from the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers">law of large numbers</a> that <span class="math inline">\(\hat{I}_N\)</span> is unbiased, i.e.&nbsp;<span class="math inline">\(\mathbb{E}[\hat{I}_N - I] = 0\)</span>.</p>
<p>We can compute the MSE of the Monte Carlo estimate <span class="math inline">\(\hat{I}_N\)</span> with <span class="math inline">\(N\)</span> i.i.d. samples following the derivations below.</p>
<p><span class="math display">\[
\begin{split}
\text{MSE}(\hat{I}_N) &amp;= \text{Var} [\hat{I}_N] + \mathbb{E} [\hat{I}_N - I]^2 \\
&amp;= \text{Var} \left[ \frac{1}{N} \sum_{i=1}^N f(X_i) \right] \\
&amp;= \frac{1}{N^2} \cdot N \cdot \text{Var} [f(X_i)] \\
&amp;= \frac{\sigma^2}{N}
\end{split}
\]</span> where we used the fact that the sample are i.i.d. at the third equal sign.</p>
<p>Therefore, the Monte Carlo estimate is consistent, as setting <span class="math inline">\(N \to \infty\)</span> drives the MSE to zero and the estimate is unbiased. Note that there is no dependency of MSE on the dimension <span class="math inline">\(D\)</span>.</p>
</section>
<section id="antithetic" class="level2">
<h2 class="anchored" data-anchor-id="antithetic">Antithetic</h2>
<p>Consider two random variables <span class="math inline">\(X_1, X_2\)</span> that are identically distribution with unknown (for now) correlation. Let <span class="math inline">\(\mu, h^2\)</span> be the mean and variance of <span class="math inline">\(X_1, X_2\)</span> and WLOG we let <span class="math inline">\(\mu = 0\)</span>.</p>
<p>The average <span class="math inline">\((X_1 + X_2)/2\)</span> is certainly <span class="math inline">\(\mu\)</span>, and the variance of the average is given by</p>
<p><span class="math display">\[
\begin{split}
\text{Var} \left[\frac{X_1 + X_2}{2}  \right] &amp;= \frac{1}{4} \left(  \text{Var}[X_1] + \text{Var}[X_2] + 2\text{Cov}(X_1, X_2) \right) \\
&amp;= \frac{1}{4} \left(  2h^2 + 2\sqrt{\text{Var}[X_1] \text{Var}[X_2]}\text{Corr}(X_1, X_2) \right) \\
&amp;=\frac{1}{4} \left(  2h^2 + 2h^2 \text{Corr}(X_1, X_2)  \right) \\
&amp;= \frac{h^2}{2} (1 + \text{Corr}(X_1, X_2) )
\end{split}
\]</span></p>
<p>which means the average’s variance is minimised when <span class="math inline">\(X_1, X_2\)</span> are perfectly negatively correlated (e.g.&nbsp;<span class="math inline">\(X_1 = -X_2\)</span>). Such pair of random variables is known as the <strong>antithetic</strong> pair.</p>
<p>When we then use the random variables (or samples) for Monte Carlo estimation, assuming we have samples <span class="math inline">\(X_1, X_2\)</span> and the objective quantity <span class="math inline">\(\mathbb{E}[f(X)]\)</span>, we have</p>
<p><span class="math display">\[
\mathbb{E}[f(X)] \approx \frac{1}{2}\left[ f(X_1) + f(X_2)\right] .
\]</span></p>
<p>For linear <span class="math inline">\(f\)</span>, it should be obvious that doing antithetic would improve the quality of the estimate. However, the benefit could be unclear for non-linear <span class="math inline">\(f\)</span>. For example, if <span class="math inline">\(f(x) = x^2\)</span>, and assuming <span class="math inline">\(X\)</span> is zero-mean so <span class="math inline">\((X_1, X_2 = -X_1)\)</span> forms an antithetic pair, we get</p>
<p><span class="math display">\[
\mathbb{E}[X^2] \approx \frac{1}{2}\left[ X_1^2 + X_2^2\right]  = X_1^2
\]</span></p>
<p>which is worse than drawing two independent samples.</p>
</section>
<section id="quasi-monte-carlo" class="level2">
<h2 class="anchored" data-anchor-id="quasi-monte-carlo">Quasi-Monte Carlo</h2>
<p>We return to Monte Carlo and consider the special case where our random variable <span class="math inline">\(X\)</span> is the uniform distribution on the <span class="math inline">\(D\)</span>-dimensional unit hypercube <span class="math inline">\([0,1]^D\)</span>. The standard Monte Carlo would randomly take points in the hypercube to sample, which does not feel very optimal. For <span class="math inline">\(N\)</span> samples, it may be more efficient to evenly spread them across the full domain <span class="math inline">\([0,1]^D\)</span> - this is the idea of Quasi-Monte Carlo (QMC).</p>
<p>We let <span class="math inline">\(X_i^Q\)</span> for <span class="math inline">\(i = 1, 2, \ldots, D\)</span> denote the QMC samples and the QMC estimate of the integral is thus</p>
<p><span class="math display">\[
I \approx \hat{I}_N^Q = \frac{1}{N} \sum_{i =1}^N f(X_i^Q).
\]</span></p>
<p>Note that the assumption of uniform random variables is not overlly restrictive, as we can apply standard sampling techiques (e.g.&nbsp;inverse CDF) to convert an Unif[0,1] random variable to other commonly used random variables. See Devroye’s classic <a href="https://luc.devroye.org/rnbookindex.html">Non-Uniform Random Variate Generation</a> for more information.</p>
<section id="stratification" class="level3">
<h3 class="anchored" data-anchor-id="stratification">Stratification</h3>
<p>The first thing one may propose to spread points more evenly would be <strong>stratification</strong>. If we wish to put <span class="math inline">\(N\)</span> points in the hypercube (for simplicity we assume <span class="math inline">\(N = n^D\)</span> for some integer <span class="math inline">\(n\)</span>), we could divide each side of the hypercube into <span class="math inline">\(n\)</span> even parts and construct <span class="math inline">\(n^D\)</span> evenly sized hypercube blocks such that we can place a sample point in the center for each of the blocks. This ensures the points to be of the same distance from its neighbours.</p>
<p>If the function of interest <span class="math inline">\(f\)</span> is sufficiently regular with periodicity, or have certain portions with larger than average variation, one would imagine this stratification produces a better integral approximation.</p>
<p>Very quickly, we have figured out the two key factors of the QMC estimate quality: the sampling sequence’s discrepancy and the variation of the function. This intuition is solidified below.</p>
</section>
<section id="koksma-hlawka-inequality" class="level3">
<h3 class="anchored" data-anchor-id="koksma-hlawka-inequality">Koksma-Hlawka Inequality</h3>
<p>The key result of QMC is the Koksma-Hlawka inequality which provides an upper bound of the estimation error of QMC for uniform random variables on <span class="math inline">\([0,1]^D\)</span>. This is given by</p>
<p><span class="math display">\[
\left|  \int_{[0,1]^D} f(x) dx - \frac{1}{N} \sum_{i=1}^N f(X_i^Q)\right| = | I - \hat{I}_N^Q| \le D^*_N \cdot V_{HK}(f)
\]</span></p>
<p>where <span class="math inline">\(D_N^*\)</span> is the <a href="https://en.wikipedia.org/wiki/Low-discrepancy_sequence#Definition_of_discrepancy">star-discrepancy</a> of the sequence <span class="math inline">\(\{X_i^Q\}_{i=1}^N\)</span> and <span class="math inline">\(V_{HK}(f)\)</span> is the variation of function <span class="math inline">\(f\)</span> in the <a href="https://arxiv.org/abs/1510.04522">Hardy-Krause sense</a>. The star-discrepancy of a set of points <span class="math inline">\(P_N = \{ X_i^Q \}_{i=1}^N\)</span> is defined as</p>
<p><span class="math display">\[
D_N^*(P_N) := \sup_{t \in [0,1]^D} \left| \frac{1}{N}\sum_{i=1}^N 1_{[0,t)}(X_i^Q) - \prod_{d=1}^D t_d\right|
\]</span></p>
<p>which is the maximum difference between the size of rectangle from with a corner fixed at zero and the number of points from <span class="math inline">\(P_N\)</span> it contains. The variation of Hardy-Krause sense for function <span class="math inline">\(f\)</span> is given by</p>
<p><span class="math display">\[
V_\text{HK} (f) := \sum_{\emptyset \neq u \subseteq [D]} \int_{[0,1]^D} \left| \frac{\partial^{|u|}}{\partial x_u}f(x_u, 1_{-u}) \right| dx_u.
\]</span></p>
<p>Although the above definition is a bit complicated looking, it is quite simple. Consider <span class="math inline">\(f(x, y) = xy\)</span> so <span class="math inline">\(D = 2\)</span>, we have <span class="math inline">\(\partial_{xy} f = 0\)</span>, <span class="math inline">\(\partial_x f = y\)</span>, and <span class="math inline">\(\partial_y f = x\)</span> and thus</p>
<p><span class="math display">\[
\begin{split}
V_\text{HK}(f) &amp;= \int |\partial_xf(x,1)|dx + \int |\partial_y f(1,y)|dy + \iint |\partial_{xy} f(x,y)|dx dy \\
&amp;= \int 1 dx + \int 1 dy + \iint 0 dx dy = 2.
\end{split}
\]</span></p>
<p>A proof sketch of the Koksma-Hlawka inequality is provided <a href="#proof_sketch">below</a>.</p>
</section>
<section id="low-discrepancy-sequences" class="level3">
<h3 class="anchored" data-anchor-id="low-discrepancy-sequences">Low-Discrepancy Sequences</h3>
<p>Suggested by the Koksma-Hlawka inequality, we can control the quality of QMC estimate by constructing sequences with low star-discrepancy. Multiple such sequences (called low-discrepancy sequences, for obvious reasons) exist in the literature, and below we will briefly outline a few commonly considered ones. Note that both Halton and Sobol sequences require the number of samples to be pre-determined, while Latin hypercube does not.</p>
<section id="halton-sequence" class="level4">
<h4 class="anchored" data-anchor-id="halton-sequence">Halton sequence</h4>
<ul>
<li>Pick a set of coprime bases <span class="math inline">\(b_1, b_2, \dots, b_s\)</span> (usually the first <span class="math inline">\(s\)</span> primes: 2, 3, 5, …).</li>
<li>In dimension <span class="math inline">\(j\)</span>, take the integer index <span class="math inline">\(n\)</span> in base <span class="math inline">\(b_j\)</span>, reverse its digits after the decimal point, and interpret as a fraction — this is the radical-inverse.</li>
<li>Combine coordinates to get the <span class="math inline">\(n\)</span>-th point.</li>
</ul>
</section>
<section id="sobol-sequence" class="level4">
<h4 class="anchored" data-anchor-id="sobol-sequence">Sobol sequence</h4>
<ul>
<li>Uses base-2 arithmetic and direction numbers derived from primitive polynomials over GF(2).</li>
<li>Each coordinate is a digital expansion using these direction numbers, ensuring good uniformity across projections.</li>
</ul>
</section>
<section id="latin-hypercube-sampling" class="level4">
<h4 class="anchored" data-anchor-id="latin-hypercube-sampling">Latin hypercube sampling</h4>
<ul>
<li>In each dimension, divide <span class="math inline">\([0,1]\)</span> into <span class="math inline">\(N\)</span> equal intervals.</li>
<li>Sample exactly one point from each interval in each dimension.</li>
<li>Randomly permute the points along each axis so that every projection on any single coordinate axis is uniform.</li>
</ul>
</section>
</section>
<section id="error-rate" class="level3">
<h3 class="anchored" data-anchor-id="error-rate">Error Rate</h3>
<p>Recall the Koksma-Hlawka inequality which provides the error bound for QMC, we wish to consider its scaling with the number of samples <span class="math inline">\(N\)</span> and dimension <span class="math inline">\(D\)</span>. The existing results on the scaling of star-discrepancy for low-discrepancy sequences <span class="math inline">\(P_N\)</span> are often of the following scale</p>
<p><span class="math display">\[
D^*(P_N) = O\left( \frac{(\log N)^D}{N}\right)
\]</span></p>
<p>whereas the variation <span class="math inline">\(V_\text{HK}\)</span> of function <span class="math inline">\(f\)</span> scales, in the worst case, exponentially in <span class="math inline">\(D\)</span> based on its definition (as it is summing over exponentially increasing terms). Therefore, it is not hard to imagine that QMC outperforms standard Monte Carlo for sufficiently small dimension <span class="math inline">\(D\)</span> for large enough sample <span class="math inline">\(N\)</span> where the <span class="math inline">\(1/N\)</span> scaling takes over; while standard Monte Carlo takes over QMC for large dimensions.</p>
</section>
<section id="randomised-qmc" class="level3">
<h3 class="anchored" data-anchor-id="randomised-qmc">Randomised QMC</h3>
<p>QMC is deterministic by definition. It is sometimes preferred to have stochasticities. Two commonly used tricks to randomise QMC are random shift and scrambing.</p>
<p>Random shift is trivial. Given an existing deterministic sequence <span class="math inline">\(\{X_i^Q\}_i\)</span>, we add a uniformly drawn value to each of them then modulo by one for each dimension to make sure the jittered points are within the considered domain <span class="math inline">\([0,1]^D\)</span>.</p>
<p>Scrambling works for digital sequences like Sobol or Halton, where the digits are permuted randomly in such a way to preserve uniformity and low discrepancy. See <a href="https://artowen.su.domains/pubtalks/qmctutorial-annotated.pdf">Owen scrambling</a>.</p>
</section>
</section>
<section id="experiments" class="level2">
<h2 class="anchored" data-anchor-id="experiments">Experiments</h2>
<p>Below shows some numerical experiments comparing the relative performance of the Monte Carlo techniques in MSE for the estimation task of <span class="math inline">\(\mathbb{E}[f(X)]\)</span> where <span class="math inline">\(f(a) = \|a\|_2^2\)</span> and <span class="math inline">\(X\)</span> is either 1D uniform, multivariate Gaussian with diagonal covariance matrix, and multivariate Gaussian with non-diagonal covariance matrix. We compare standard Monte Carlo, antithetic, Sobol sequence, and randomised Sobol at varying number of sample sizes. 100 runs are conducted for the estimation of MSE. The code can be found <a href="./mvtnorm.py">here</a></p>
<section id="uniform" class="level3">
<h3 class="anchored" data-anchor-id="uniform">Uniform</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./unif_mse.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Uniform. Antithetic gives perfect estimation for smaller sample sizes thus points not shown on the log-scale plot.</figcaption>
</figure>
</div>
</section>
<section id="multivariate-gaussian-diagonal-covariance" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-gaussian-diagonal-covariance">Multivariate Gaussian (Diagonal Covariance)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./mvtnorm_diag_mse_d4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Dimension 4</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./mvtnorm_diag_mse_d40.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Dimension 40</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./mvtnorm_diag_mse_d400.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Dimension 400</figcaption>
</figure>
</div>
</section>
<section id="multivariate-gaussian-non-diagonal-covariance" class="level3">
<h3 class="anchored" data-anchor-id="multivariate-gaussian-non-diagonal-covariance">Multivariate Gaussian (Non-Diagonal Covariance)</h3>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./mvtnorm_mse_d4.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Dimension 4</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./mvtnorm_mse_d40.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Dimension 40</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="./mvtnorm_mse_d400.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Dimension 400</figcaption>
</figure>
</div>
</section>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<section id="proof_sketch" class="level3">
<h3 class="anchored" data-anchor-id="proof_sketch">Proof Sketch of Koksma-Hlawka</h3>
<p>The original proof of the Koksma-Hlawka inequality is given using integration by parts and some decomposotion. Below, we roughly outline a simple proof using the powerful tool of <a href="https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space">reproducing kernel Hilbert space</a> (RKHS).</p>
<p>We consider an RKHS <span class="math inline">\(\mathcal{H}\)</span> with reproducing kernel <span class="math inline">\(K\)</span>. For any function <span class="math inline">\(f \in \mathcal{H}\)</span>, the reproducing property of RKHS states that</p>
<p><span class="math display">\[
f(x) = \langle f, K(\cdot, x)\rangle_\mathcal{H}, \qquad \int f(x) dx = \int \langle f, K(\cdot, x)\rangle_\mathcal{H} dx =  \langle f, \int K(\cdot, x) dx\rangle_\mathcal{H}
\]</span></p>
<p>where <span class="math inline">\(\langle \cdot, \cdot \rangle_\mathcal{H}\)</span> is the inner product equipped with the RKHS. For a probability measure <span class="math inline">\(\mu\)</span>, we have the kernel mean embedding <span class="math inline">\(m_\mu\)</span> of the measure defined as</p>
<p><span class="math display">\[
m_\mu := \int K(\cdot, x) d\mu(x).
\]</span></p>
<p>Notice that the full integral <span class="math inline">\(I\)</span> is the expectation over the uniform measure <span class="math inline">\(\mu\)</span> while the QMC estimate is the expectation over the discrete measure <span class="math inline">\(\mu_N\)</span> as average Dirac mass. Thus, we have</p>
<p><span class="math display">\[
\begin{split}
|I - \hat{I}_N^Q| &amp;= \left| \int f d\mu - \int fd\mu_N \right| \\
&amp;= \left| \int \langle f, K(\cdot, x)\rangle_\mathcal{H} d\mu(x) - \int \langle f, K(\cdot, x)\rangle_\mathcal{H} d\mu_N(x)  \right| \\
&amp;= \left\langle f, \int K(\cdot, x)d\mu(x) \right\rangle_\mathcal{H} - \left\langle f, \int K(\cdot, x)d\mu_N(x) \right\rangle_\mathcal{H} \\
&amp;= \langle f, m_\mu \rangle_\mathcal{H} - \langle f, m_{\mu_N}\rangle_\mathcal{H} = \langle f, m_\mu - m_{\mu_N}\rangle_\mathcal{H} \\
&amp;\le \|f\|_\mathcal{H} \cdot \|m_\mu - m_{\mu_N}\|_\mathcal{H}.
\end{split}
\]</span></p>
<p>Finally, it can be established that if we set the reproducing kernel as <span class="math inline">\(K_D: [0,1]^D \times [0,1]^D \to \mathbb{R}\)</span> given by</p>
<p><span class="math display">\[
K_D(x, y) = \prod_{j = 1}^D \left( 1 + \min\{1-x_j, 1-y_j \}\right)
\]</span></p>
<p>and work with its corresponding RKHS <span class="math inline">\(\mathcal{H}_D\)</span> we could recover the desired Koksma-Hlawka inequality frm the above derivation.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>