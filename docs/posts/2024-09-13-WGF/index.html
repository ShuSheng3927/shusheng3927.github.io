<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rui-Yang Zhang">
<meta name="author" content="Christopher Nemeth">
<meta name="dcterms.date" content="2024-09-13">
<meta name="description" content="Blog post on gradient flows in Euclidean and Wasserstein spaces.">

<title>Rui-Yang Zhang - Why Should We Care About Gradient Flows?</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../rui.svg" rel="icon" type="image/svg+xml">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../rui.svg" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Rui-Yang Zhang</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../research.html" rel="" target="">
 <span class="menu-text">Research</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../notes.html" rel="" target="">
 <span class="menu-text">Notes</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resources.html" rel="" target="">
 <span class="menu-text">Resources</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Why Should We Care About Gradient Flows?</h1>
                  <div>
        <div class="description">
          Blog post on gradient flows in Euclidean and Wasserstein spaces.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">Gradient Flow</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Authors</div>
      <div class="quarto-title-meta-contents">
               <p>Rui-Yang Zhang </p>
               <p>Christopher Nemeth </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 13, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#gradient-flows-in-the-euclidean-space" id="toc-gradient-flows-in-the-euclidean-space" class="nav-link active" data-scroll-target="#gradient-flows-in-the-euclidean-space">Gradient Flows in the Euclidean Space</a></li>
  <li><a href="#the-langevin-diffusion" id="toc-the-langevin-diffusion" class="nav-link" data-scroll-target="#the-langevin-diffusion">The Langevin Diffusion</a></li>
  <li><a href="#monte-carlo-sampling" id="toc-monte-carlo-sampling" class="nav-link" data-scroll-target="#monte-carlo-sampling">Monte Carlo Sampling</a></li>
  <li><a href="#wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation" id="toc-wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation" class="nav-link" data-scroll-target="#wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation">Wasserstein Gradient Flow - a Bridge between Sampling and Optimisation</a></li>
  <li><a href="#what-else-can-we-do" id="toc-what-else-can-we-do" class="nav-link" data-scroll-target="#what-else-can-we-do">What Else Can We Do?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Optimisation is a fundamental task in modern-day statistics and machine learning. A large set of problems in machine learning and statistics can be easily phrased as an optimisation problem - given some objective function <span class="math inline">\(f\)</span> defined on a domain <span class="math inline">\(\mathcal{X}\)</span>, we wish to find a point <span class="math inline">\(x \in \mathcal{X}\)</span> that minimises <span class="math inline">\(f\)</span> (or maximises <span class="math inline">\(-f\)</span>). Sometimes, we do not even need to find the global minimum of <span class="math inline">\(f\)</span>, and a sufficiently close local minimum would be good too.</p>
<section id="gradient-flows-in-the-euclidean-space" class="level2">
<h2 class="anchored" data-anchor-id="gradient-flows-in-the-euclidean-space">Gradient Flows in the Euclidean Space</h2>
<p>A common optimisation algorithm is the <strong>gradient descent</strong>. If our objective function <span class="math inline">\(f\)</span> defined on the Euclidean space <span class="math inline">\(\mathbb{R}^d\)</span> is continuous and we can compute its gradient <span class="math inline">\(\nabla f\)</span>, then, the gradient descent algorithm will iteratively apply the following update</p>
<p><span class="math display">\[
x_{n+1} = x_n - h \nabla f(x_n)
\]</span></p>
<p>until we converge or reach a termination point. The parameter <span class="math inline">\(h&gt;0\)</span> above is the step size of our algorithm, often referred to as a <em>learning rate</em> and it is a tuning parameter of the gradient descent algorithm. When we set <span class="math inline">\(h\)</span> to be very small, and let it tend to zero, we would convert the above discrete-in-time algorithm into a continuous-in-time algorithm, described as</p>
<p><span class="math display">\[
\frac{\mathrm{d}}{\mathrm{d}t} x_t = -\nabla f(x_t)
\]</span></p>
<p>where we use <span class="math inline">\(t\)</span> instead of <span class="math inline">\(n\)</span> to denote the time index as we are in continuous time rather than discrete time. Notice that for the above ordinary differential equation (ODE), after an Euler discretisation (of time), will become the gradient descent algorithm. The ODE is known as the <strong>gradient flow</strong> (in Euclidean space), and we can show that various frequently used algorithms can be interpreted as different discretisations of the gradient flow. For example, an implicit Euler discretisation of the gradient flow gives us the <strong>proximal point</strong> <strong>algorithm.</strong></p>
<p>One can certainly see the conceptual benefit of considering gradient flow for understanding discrete-in-time optimisation algorithms - we suddenly have a simple, elegant mental picture of the limiting case of these procedures. However, rather unfortunately, the gradient flow in Euclidean space could not help us that much more than that. Often in theoretical analysis of iterative algorithms, we are interested in the convergence rate of these algorithms to some target value, and in the cases where approximations happen in the algorithms, we are interested in capturing the errors induced. Because of the discretisation in time, we could not translate many of the theories about gradient flow in Euclidean space into their discrete-in-time counterparts. This is the main reason why although gradient flows are extremely natural and tempting to investigate, they have not been considered as much, until very recently.</p>
</section>
<section id="the-langevin-diffusion" class="level2">
<h2 class="anchored" data-anchor-id="the-langevin-diffusion">The Langevin Diffusion</h2>
<p>A major breakthrough, at least from a theoretical perspective, happened with Jordan, Kinderlehrer &amp; Otto’s 1998 paper <a href="https://doi.org/10.1137/S0036141096303359">The Variational Formulation of the Fokker-Planck Equation</a>. In there, the authors made an explicit connection between the <strong>Langevin diffusion</strong>, a particular type of Stochastic Differential Equation (SDE) with very nice equilibrium properties, and a gradient flow in the space of probability distributions. The Langevin diffusion can be characterised by the SDE</p>
<p><span class="math display">\[
\mathrm{d}X_t = \nabla \log \pi(X_t) \mathrm{d}t + \sqrt{2}\mathrm{d}B_t
\]</span></p>
<p>where <span class="math inline">\(\{B_t\}\)</span> is a Brownian motion and <span class="math inline">\(\pi\)</span> is the equilibrium distribution of the process, and it could also be characterised by the <strong>Fokker-Planck equation</strong></p>
<p><span class="math display">\[
\partial_t p_t(x) = \text{div} \left( p_t(x) \nabla \log \frac{p_t(x)}{\pi(x)} \right)
\]</span></p>
<p>where <span class="math inline">\(p_t(x)\)</span> is the probability distribution of <span class="math inline">\(X_t\)</span>. Naively, one can think about the two characterisations of the Langevin diffusion as a state space version and a distribution space version of the same motion.</p>
<p>So, the paper of JKO1998 established that the Fokker-Planck equation of the Langevin diffusion is equivalent to a gradient flow in the <strong>Wasserstein space</strong> with the objective function being the KL divergence <span class="math inline">\(f(\cdot) = \text{KL}(\cdot \| \pi)\)</span> where</p>
<p><span class="math display">\[
\text{KL}(p\| q) := \int p(x) \log[p(x) / q(x)] dx = \mathbb{E}_{X \sim p} [\log ( p(X)/q(X)) ].
\]</span></p>
<p>Intuitively, what this connection tells us is that the particles following a Langevin diffusion are moving - in the steepest direction - towards their equilibrium distribution.</p>
<p>As an example, let’s assume that our target distribution of interest <span class="math inline">\(p\)</span> is a Gaussian <span class="math inline">\(\mathcal{N}(0,1)\)</span> and particles are represented by the distribution <span class="math inline">\(q\)</span>. As seen in the following movie, we can use the Wasserstein gradient flow of KL divergence to sequentially evolve <span class="math inline">\(q\)</span> and minimise the KL divergence.</p>
<p><img src="WGF.gif" class="img-fluid"> (Thanks to <a href="https://louissharrock.github.io">Louis Sharrock</a> for creating this movie)</p>
<p>This result seems neat, but what is so special about this Langevin diffusion? It turns out that the Langevin diffusion is rather fundamental in sampling algorithms for computational statistics.</p>
</section>
<section id="monte-carlo-sampling" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-sampling">Monte Carlo Sampling</h2>
<p>In statistics, especially in Bayesian statistics, we would often run into the problem of having a complicated probability distribution that we wish to compute expectations of, such as in the case of computing the posterior mean of a parameter of interest. If the distribution is complex and we cannot analytically evaluate our expectations of interest, then we often rely on using (independent) samples from the distribution to form an empirical approximation of the distribution. To be more precise, if we have a target probability distribution <span class="math inline">\(\pi\)</span>, we will get a sequence of independent samples <span class="math inline">\(X_1, X_2, \ldots, X_n \sim \pi\)</span> and we have</p>
<p><span class="math display">\[
\pi(x) \approx \frac{1}{n} \sum_{k=1}^n 1_{X_k}(x)
\]</span></p>
<p>where <span class="math inline">\(1_{X_k}(x)\)</span> is the indicator function that takes the value 1 when <span class="math inline">\(x = X_k\)</span> and zero otherwise. This is the <strong>Monte Carlo method</strong>, and it can be shown that under weak conditions of the target distribution <span class="math inline">\(\pi\)</span>, the empirical distribution converges to <span class="math inline">\(\pi\)</span> at a rate of <span class="math inline">\(O(1/\sqrt{n})\)</span> for <span class="math inline">\(n\)</span> Monte Carlo samples. The only problem with the Monte Carlo method is, how do we get those samples? As alluded slightly from the Langevin diffusion, since we can set the equilibrium distribution of a Langevin diffusion to (almost) any target distribution and the process will converge to it after running for a while, we can just start the SDE at some point and run it for long enough so it hits the equilibrium, and use the trajectories afterwards as samples from the target distribution.</p>
<p>Immediately, we would ask - how exactly do we simulate a continuous-in-time SDE? The simplest solution is to use the Euler-Maruyama scheme and obtain discretisations using the following iterative procedure</p>
<p><span class="math display">\[
X_{(n+1)h} = X_{nh} + h \nabla \log \pi(X_{nh})+\sqrt{2h} \xi
\]</span></p>
<p>where <span class="math inline">\(\xi \sim N(0,1)\)</span>. This gives us the <strong>unadjusted Langevin algorithm</strong> (ULA), also known as the <strong>Langevin Monte Carlo</strong> (LMC) algorithm in the machine learning literature.</p>
<p>Since this is a discretisation, it introduces some numerical errors (the precise reason for the errors will be explained in a bit) and by using ULA we will not obtain exact samples from the target distribution <span class="math inline">\(\pi\)</span>. For sufficiently small <span class="math inline">\(h\)</span>, the error would be tolerable. We could also do smart things such as <strong>Metropolis adjustments</strong> to remove the error, and we would recover the <strong>Metropolis Adjusted Langevin Algorithm</strong> (MALA) which is a staple of the <strong>Markov chain Monte Carlo</strong> (MCMC) algorithms for computational statistics. More thorough discussions on MCMC algorithms can be found in textbooks such as <a href="https://link.springer.com/book/10.1007/978-1-4757-4145-2">Monte Carlo Statistical Methods</a> by Robert &amp; Casella, or the recent <a href="https://arxiv.org/abs/2407.12751">Scalable Monte Carlo for Bayesian Learning</a> by Fearnhead, Nemeth, Oates &amp; Sherlock. One could also find a more detailed theoretical study of ULA in Roberts &amp; Tweedie’s 1996 paper <a href="https://projecteuclid.org/journals/bernoulli/volume-2/issue-4/Exponential-convergence-of-Langevin-distributions-and-their-discrete-approximations/bj/1178291835.full">Exponential Convergence of Langevin Distributions and their Discrete Approximations</a>.</p>
</section>
<section id="wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation" class="level2">
<h2 class="anchored" data-anchor-id="wasserstein-gradient-flow---a-bridge-between-sampling-and-optimisation">Wasserstein Gradient Flow - a Bridge between Sampling and Optimisation</h2>
<p>So far, we have learnt that the Langevin diffusion can be viewed as a gradient flow, and the discrete-in-time version of the Langevin diffusion allows us to draw samples from a target distribution. It turns out that we can also interpret the discrete Langevin diffusion of the LMC as a discrete-in-time approximation of the corresponding gradient flow in the space of probability distributions (to be more precise, the Wasserstein space, so we would often call this type of gradient flow a <em>Wasserstein gradient flow</em>).</p>
<p>In the 2018 paper <a href="https://arxiv.org/abs/1802.08089">Sampling as Optimization in the Space of Measures</a> by Wibisono, the author pointed out that the LMC as an Euler-Maruyama discretisation of the Langevin diffusion can be viewed as a forward-flow splitting discretisation of the Wasserstein gradient flow with the objective function being the KL divergence. The forward-flow splitting scheme is a way to discretise time by doing half a step of forward discretisation, and half a step of flow discretisation, for each full step of the iteration. The expression of the two discretisations is slightly involved to describe in the space of probability distributions, but if we translate them into the state space, it is simply</p>
<p><span class="math display">\[
\text{(forward)} \ X_{(n+1/2)h} = X_{nh} + h \nabla \log \pi(X_{nh}),
\]</span> <span class="math display">\[
\text{(flow)} \ X_{(n+1)h} = X_{(n+1/2)h} + \sqrt{2h} \xi
\]</span> with <span class="math inline">\(\xi \sim N(0,1)\)</span>, which combines to give us the full LMC update. Another observation in Wibisono (2018) is that, if we swap the flow step with a backward discretisation step, we would be able to cancel the error of discretising the Langevin diffusion. Unfortunately, the backward step is not implementable in general. Nevertheless, this paper provides us with the very important information that there exists a hidden connection between sampling (using LMC) and optimisation (using gradient flows). A bridge between the two areas has been formally built at this point.</p>
<p>To further utilise the power of this connection, Durmus, Majewski &amp; Miasojedow in their 2019 paper <a href="https://arxiv.org/abs/1802.09188">Analysis of Langevin Monte Carlo via Convex Optimization</a> provided us with a more explicit characterisation of the error of LMC using convergence analysis of the Wasserstein gradient flow. Unlike in the case of gradient flows in Euclidean space, the theoretical studies of Wasserstein gradient flows can actually be used in the analysis of their discrete-in-time counterparts.</p>
</section>
<section id="what-else-can-we-do" class="level2">
<h2 class="anchored" data-anchor-id="what-else-can-we-do">What Else Can We Do?</h2>
<p>At this point, it should be clear that the connection between sampling and optimisation established using Wasserstein gradient flows is promising and potentially very useful.</p>
<p>One immediate area of work is to interpret existing sampling algorithms as gradient flows, and use these realisations to help us better understand the properties of these algorithms. There are already some successful fruits from this branch:</p>
<ul>
<li>Liu’s 2017 paper <a href="https://arxiv.org/abs/1704.07520">Stein Variational Gradient Descent as Gradient Flow</a> interpreted the Stein Variational Gradient Descent algorithm, a powerful sampling algorithm, as a type of gradient flow.</li>
<li>Duncan, Nüsken &amp; Szpruch’s 2023 paper <a href="https://arxiv.org/abs/1912.00894">On the Geometry of Stein Variational Gradient Descent</a> built on the above realisation and showed several convergence results about the algorithm, as well as certain improvements based on such gradient flow analysis.</li>
<li>Nüsken’s 2024 paper <a href="https://arxiv.org/pdf/2409.01464">Stein Transport for Bayesian Learning</a> proposed a promising new algorithm Stein Transport that extends the Stein Variational Gradient Descent by tweaking the geometry of the Wassterstein gradient flow.</li>
<li>Chopin, Crucinio &amp; Korba’s 2024 paper <a href="https://arxiv.org/abs/2310.11914">A connection between Tempering and Entropic Mirror Descent</a> has established that tempering sequential Monte Carlo algorithms can be viewed as a type of discretisation of the gradient flow in the Fisher-Rao geometry.</li>
</ul>
<p>In addition, a class of work that could be made possible with this new connection is those that translate algorithmic tricks from one field (say optimisation) to another (say sampling). A very nice example of this thinking is the line of work by Sharrock, Nemeth and coauthors over recent years. A lot of optimisation algorithms involve tuning parameters (also known as learning rates) that have to be manually adjusted, and different specifications of them will sometimes yield very different performance of the algorithms. To tackle the difficulties of tuning such parameters, there is a class of <strong>learning-rate-free</strong> algorithms that replace the tuning of learning rates with an automatic mechanism. With the help of the connection between sampling and optimisation made by gradient flows, recent work has managed to replace the manually-tuned learning rates of sampling algorithms with automatic, learning-rate-free ones, as shown in the papers such as <a href="https://proceedings.mlr.press/v202/sharrock23a/sharrock23a.pdf">Coin Sampling: Gradient-Based Bayesian Inference without Learning Rates</a> and <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/cdee6c3eaa2adc285f11da7711a75c12-Paper-Conference.pdf">Learning Rate Free Sampling in Constrained Domains</a>.</p>
<p>Overall, gradient flows and related ideas have become a promising tool for investigating theoretical properties of sampling algorithms, and have shown a considerable amount of potential to inspire designs of new sampling algorithms. There remains a vast pool of unanswered questions and possible extensions in this area. More breakthroughs are to be expected from this line of work.</p>
<p><strong>P.S.</strong> A book-length, formal introduction to the material covered above and more can be found in <a href="https://arxiv.org/abs/2407.18163">Statistical Optimal Transport</a> by Chewi, Niles-Weed &amp; Rigollet.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>