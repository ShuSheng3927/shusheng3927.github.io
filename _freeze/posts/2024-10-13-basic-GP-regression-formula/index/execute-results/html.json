{
  "hash": "ebb9711f2a212f7f4158933a8bce4b27",
  "result": {
    "markdown": "---\ntitle: \"[Derivation Scribbles] Basic GP Regression Formula\"\ndescription: \"Derivations for the Gaussian process predictive distribution. Single-output GP, observe with additive Gaussian noise.\"\ndate: \"13 October 2024\"\ncategories:\n  - Gaussian Process\n  - Derivation Scribbles\n---\n\n\n![Gaussian Process Regression, adapted from <https://docs.jaxgaussianprocesses.com/>.](gp.svg)\n\n### Block Matrix Inversion\n\nThe first thing we need to establish is the block matrix inversion identity. Consider an invertible matrix $\\Sigma$ that can be written as\n\n$$\n\\Sigma = \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}\n$$\n\nwhere $\\Sigma_{AA}, \\Sigma_{AB}, \\Sigma_{BA}, \\Sigma_{BB}$ are matrices of the right dimension and sufficiently non-singular. Next, we have the block matrix inversion identity stated below.\n\n$$\n\\begin{split}\n\\Sigma^{-1} &= \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}^{-1} \\\\\n&= \\begin{bmatrix} (\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1}  & -(\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1} \\Sigma_{AB} \\Sigma_{BB}^{-1}\\\\ -\\Sigma_{BB}^{-1} \\Sigma_{BA}(\\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA})^{-1}  & (\\Sigma_{BB} - \\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB})^{-1} \\end{bmatrix}.\n\\end{split}\n$$\n\n### Marginal and Conditional Gaussians\n\nConsider a multivariate Gaussian distribution $x = (x_A, x_B)^T$ where $x_A$ is $d_A$ dimensional, $x_B$ is $d_B$ dimensional, and $x$ is $d = d_A + d_B$ dimensional. The mean vector and covariance matrix of the multivariate Gaussian is set to be as follows:\n\n$$\nx = \\begin{bmatrix} x_A \\\\ x_B \\end{bmatrix} \\sim N_d \\left( \\mu, \\Sigma\\right) = N_d \\left( \\begin{bmatrix} \\mu_A \\\\ \\mu_B \\end{bmatrix}, \\begin{bmatrix}\\Sigma_{AA} & \\Sigma_{AB} \\\\\\Sigma_{BA} & \\Sigma_{BB} \\\\\\end{bmatrix}\\right).\n$$\n\nIt is easy to notice that the **marginal distributions** $x_A$ and $x_B$ can be obtained by selecting the needed entries of the above equation, i.e.\n\n$$\n\\begin{split}\nx_A &\\sim N_{d_A}(\\mu_A, \\Sigma_{AA}), \\\\\nx_B &\\sim N_{d_B}(\\mu_B, \\Sigma_{BB}). \n\\end{split}\n$$\n\nThe conditional distributions are a bit tricky, which we will derive below. Due to symmetry, we will derive the conditional distribution $x_A | x_B$ and just state $x_B | x_A$. Using $p(\\cdot)$ to denote the density of a random variable, we have\n\n$$\n\\begin{split}\np(x_A | x_B) &= \\frac{p(x_A, x_B)}{p(x_B)} \\\\ &\\propto \\exp\\left\\{  -\\frac{1}{2} (x - \\mu)^T\\Sigma^{-1}(x - \\mu) \\right\\}.\n\\end{split}\n$$\n\nFocusing on the terms inside the second exponential, we first denote\n\n$$\n\\Sigma^{-1} = \\begin{bmatrix} V_{AA} & V_{AB} \\\\ V_{BA} & V_{BB} \\end{bmatrix}\n$$\n\nwhich then yield\n\n$$\n\\begin{split} \n&\\quad  (x - \\mu)^T\\Sigma^{-1}(x - \\mu) \\\\\n&=  \\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix}^T \\begin{bmatrix} V_{AA} & V_{AB} \\\\ V_{BA} & V_{BB} \\end{bmatrix}\\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix}  \\\\\n&= \\begin{bmatrix} (x_A - \\mu_A)^T V_{AA} + (x_B - \\mu_B)^T V_{BA} \\\\ (x_A - \\mu_A)^T V_{AB} + (x_B - \\mu_B)^T V_{BB} \\end{bmatrix}^T\\begin{bmatrix} x_A - \\mu_A \\\\ x_B - \\mu_b \\end{bmatrix} \\\\\n&= (x_A - \\mu_A)^T V_{AA} (x_A - \\mu_A) + (x_A - \\mu_A)^T V_{AB} (x_B - \\mu_B) \\\\\n&\\quad +  (x_B - \\mu_B)^T V_{BA} (x_A - \\mu_A) + (x_B - \\mu_B)^T V_{BB} (x_B - \\mu_B).\n\\end{split}\n$$\n\nWe can keep terms with $x_A$ and put the rest into the normalising constant. As $V_{AA}$ is square and $V_{AB}= V_{BA}^T$, we can simplify our above equation into\n\n$$\n\\begin{split} \n&\\quad x_A^T V_{AA} x_A - 2 x_A^T V_{AA} \\mu_A + 2x_A^T V_{AB} (x_B - \\mu_B) \\\\\n&= x_A^T V_{AA} x_A - 2 x_A^T [ V_{AA} \\mu_A +V_{AB} (x_B - \\mu_B)] \\\\\n&= (x_A - \\mu')^T V_{AA}(x_A - \\mu')+ C\n\\end{split}\n$$\n\nfor some constant $C$ independent of $x_A$ and the newly defined\n\n$$\n\\mu' = \\mu_A - V_{AA}^{-1}V_{AB} (x_B - \\mu_B).\n$$\n\nTherefore, using the values of $V_{AA}, V_{AB}$ from the block matrix inversion formula earlier, we have\n\n$$\n\\begin{split}\n\\mu' &= \\mu_A - V_{AA}^{-1}V_{AB} (x_B - \\mu_B) \\\\\n&= \\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}(x_B - \\mu_B) \\\\\nV_{AA}^{-1} &= \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA} \\\\\n\\end{split}\n$$\n\nand via symmetry, we have the **conditional distributions**\n\n$$\n\\begin{split}\nx_A | x_B &\\sim N_{d_A}(\\mu_A + \\Sigma_{AB}\\Sigma_{BB}^{-1}(x_B - \\mu_B), \\Sigma_{AA} - \\Sigma_{AB}\\Sigma_{BB}^{-1}\\Sigma_{BA}), \\\\\nx_B | x_A &\\sim N_{d_B}(\\mu_B + \\Sigma_{BA}\\Sigma_{AA}^{-1}(x_A - \\mu_A), \\Sigma_{BB} - \\Sigma_{BA}\\Sigma_{AA}^{-1}\\Sigma_{AB}). \n\\end{split}\n$$\n\n### Gaussian Process Regression\n\nConsider we have a single-output Gaussian process $f \\sim \\mathcal{GP}(\\mu, k)$ where $\\mu$ is the mean function and $k$ is the kernel function. The support of this GP is assumed to be $\\mathbb{R}^d$. Consider we have made $n$ observations of this GP $f$ where the observations are made at locations $X \\in \\mathbb{R}^n$ with values $y \\in \\mathbb{R}^n$ and the observations are noisy with independent additive Gaussian noise of variance $\\sigma^2$, i.e. $y = f(X) + \\xi$ with $\\xi_i \\sim N(0, \\sigma^2) ~\\forall i = 1, 2, \\ldots, n$. Denote the existing observations as $\\mathcal{D} = \\{ X, y \\}$.\n\nUnder our modelling assumptions, we could write down the (log) likelihood of the $m$ observations $y$ under our GP prior $f \\sim \\mathcal{GP}(\\mu, k)$. Since $y = f(X) + \\xi$, we have\n\n$$\ny | X \\sim N_n \\left( \\mu(X), k(X, X) + \\sigma^2 I_n \\right) \n$$ \n\nparamerised by $\\theta$ (e.g. observation noise $\\sigma$, lengthscale and variance of the kernel $k$) which gives us the following log likelihood\n\n$$\n\\log p(y|X) = - \\frac{n}{2}\\log(2\\pi) - \\log | k(X, X) + \\sigma^2 I_n | - \\frac{1}{2} \\left( y - \\mu(X) \\right)^T ( k(X, X) + \\sigma^2 I_n)^{-1}\\left( y - \\mu(X) \\right)\n$$ \n\nthat we maximise w.r.t. $\\theta$ to obtain the maximum likelihood estimators of the (hyper)parameters.\n\nNext, conditional on these observations, we wish to know the distributions of the GP at test points $X_* \\in \\mathbb{R}^m$, i.e. the conditional distribution $y_* = f(X_*) ~| \\mathcal{D}$. This can be achieved by first model $y_*$ and $y$ jointly, then condition on $y$. Using the conditional distribution formula above, we denote for simplicity the Gram matrices\n\n$$\nK = k(X, X) \\in \\mathbb{R}^{n \\times n}, \\qquad K_* = k(X, X_*) \\in \\mathbb{R}^{n \\times m}, \\qquad K_{**}=k(X_*, X_*) \\in \\mathbb{R}^{m \\times m},\n$$\n\nwhich gives us\n\n$$\n\\begin{split} \ny_* ~|X_*, \\mathcal{D}, \\sigma^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= \\mu(X_*) + K_*^T (K + \\sigma^2 I_n)^{-1} (y - \\mu(X)),\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1}K_*.\n\\end{split}\n$$\n\nIn the common scenario where we assume $\\mu = 0$, we further have the following **GP predictive distribution**\n\n$$\n\\begin{split} \ny_* ~|X_*, \\mathcal{D}, \\sigma^2 &\\sim N_{n}(\\mu_{y_* | \\mathcal{D}}, K_{y_* | \\mathcal{D}}), \\\\\n\\mu_{y_* | \\mathcal{D}} &= K_*^T (K + \\sigma^2 I_n)^{-1} y,\\\\\nK_{y_* | \\mathcal{D}} &= K_{**} - K_*^T (K + \\sigma^2 I_n)^{-1}K_*.\n\\end{split}\n$$\n\n\n### Computational Costs\n\nWe first state the basic computational costs of matrix operations. See [here](https://en.wikipedia.org/wiki/Computational_complexity_of_mathematical_operations#Matrix_algebra) and the links within for more details. \n\n\n::: {.cell warnings='false'}\n\n| Operations        | Dimensions   | Cost  | Remark  |\n|---------------|:-------------:|------:|------:|\n| col 3 is      | right-aligned | $1600 |af|\n| col 2 is      | centered      |   $12 |adf|\n| zebra stripes | are neat      |    $1 |asdf|\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}